@comment{This file has been generated by Pybliographer}


@Article{4600618,
  Author         = {Noikaew, N. and Chitsobhuk, O. },
  Title          = {Performance improvement for arithmetic coder using
                   parallel symbol encoding in JPEG2000},
  Journal        = {Electrical Engineering/Electronics, Computer,
                   Telecommunications and Information Technology, 2008.
                   ECTI-CON 2008. 5th International Conference on},
  Volume         = {2},
  Pages          = {1073--1076},
  abstract       = {JPEG2000 image compression standard is designed to
                   support a wide rage of applications. However, its use
                   is restricted since its implementation requires high
                   hardware cost. Bit plane coder (BPC) and arithmetic
                   coder are the main resource intensive components, which
                   indicate the overall throughput of JPEG2000 encoder.
                   Usually, the regular operation of the arithmetic coder
                   is sequential, which can process only one symbol at a
                   time. However, the bit-plane coding can generate more
                   than one symbol per clock cycle. Consequently, the
                   coding speed will be limited and bottlenecked at the
                   interface between the output of the bit-plane coding
                   and the input of the arithmetic coder. Therefore, in
                   this paper, the parallel symbol encoding is proposed to
                   improve the performance of the JPEG2000 arithmetic
                   coder. The encoding performance is enhanced through the
                   use of the proposed prediction process of the upper
                   bound value, the lower bound and the index value. Since
                   the prediction processes of the upper bound and lower
                   bound values are quite similar, they can be done
                   simultaneously with the similar prediction condition.
                   As a result, the proposed arithmetic coder architecture
                   can process dual symbols per clock cycle at the speed
                   greater than 100 MHz with the throughput greater than
                   60 Msymbols/sec.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/ECTICON.2008.4600618},
  citeulike-article-id = {4030241},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/ECTICON.2008.4600618},
  keywords       = {-source-ieee-hw-comp-2007-},
  month          = {May},
  posted-at      = {2009-02-10 15:43:40},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/ECTICON.2008.4600618},
  year           = 2008
}

@Article{4418700,
  Author         = {Lei, Sheng and Taoye, Zheng and Jin, Zhang X. },
  Title          = {Design and implementation of SAR raw data BAQ based on
                   FPGA},
  Journal        = {Synthetic Aperture Radar, 2007. APSAR 2007. 1st Asian
                   and Pacific Conference on},
  Pages          = {664--666},
  abstract       = {SAR raw data rate is one of most important limiting
                   factor of spaceborne SAR system, in this paper, a
                   design method of SAR raw data real-time compression
                   system based on FPGA is discussed in detail, which
                   adopts Block Adaptive Quantization (BAQ) algorithm. By
                   using less hardware logic resource, SAR raw data rate
                   is reduced, and performance of this real-time
                   compression system is enhanced greatly. In BAQ module,
                   four-channel parallel processing logic architecture is
                   designed to circulate the statistic of data block; the
                   logic architecture of multipliers and comparers is
                   optimized, and when time-division multiple BAQ modules
                   mode is applied in both I and Q channels in multi-FPGA,
                   the real time processing ability is enhanced
                   geminately. Experiment indicates the highest clock is
                   149MHz, data throughput of single module is great than
                   lGbps, and when compression ratio is 8/3, the signal
                   distortion noise ratio (SDNR) is great than 14dB.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/APSAR.2007.4418700},
  citeulike-article-id = {4030212},
  date-modified  = {2009-02-12 00:40:43 +0100},
  doi            = {http://dx.doi.org/10.1109/APSAR.2007.4418700},
  keywords       = {-source-ieee-hw-comp-2007-, fpga; ian_todo},
  month          = {Nov.},
  posted-at      = {2009-02-10 15:43:39},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/APSAR.2007.4418700},
  year           = 2007
}

@Article{4264152,
  Author         = {Nakaska, J. K. and Haslett, J. W. },
  Title          = {2 GHz Automatically Tuned Q-Enhanced CMOS Bandpass
                   Filter},
  Journal        = {Microwave Symposium, 2007. IEEE/MTT-S International},
  Pages          = {1599--1602},
  abstract       = {An automatically tuned 2 GHz 0.18 mum CMOS 3-stage RF
                   filter is presented. The bandpass filter achieves
                   center frequency tuning while maintaining a relatively
                   flat passband. Q-enhanced resonators allow for more
                   than 28 dB of insertion loss compensation in the Alter
                   response. Measured results show that the filter is
                   tunable in frequency by 32.5\%, has a 14.7 dB noise
                   figure, a -3.9 dBm 1dB input compression point, a +5.1
                   dBm IIP3 intercept, and consumes 21 mA from a 1.8 V
                   supply. Automatic tuning of the multistage filter is
                   performed by over-enhancing each stage of the filter
                   until oscillation occurs to set resonant frequency and
                   then backing off the enhancement using a passive Q
                   tuning method, which does not effect the resonant
                   frequency. Automatic frequency and quality factor
                   tuning control were performed using digital logic
                   synthesized in an FPGA. The hardware in-situ automatic
                   tuning of the multi-pole integrated filter eliminates
                   the need for a replica filter.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/MWSYM.2007.379991},
  citeulike-article-id = {4030227},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/MWSYM.2007.379991},
  keywords       = {-source-ieee-hw-comp-2007-, fpga},
  month          = {June},
  posted-at      = {2009-02-10 15:43:39},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/MWSYM.2007.379991},
  year           = 2007
}

@Article{1041776,
  Author         = {Wolff, F. G. and Papachristou, C. },
  Title          = {Multiscan-based test compression and hardware
                   decompression using LZ77},
  Journal        = {Test Conference, 2002. Proceedings. International},
  Pages          = {331--339},
  abstract       = {In this paper we present a new test data compression
                   technique and an associated decompression scheme for
                   testing VLSI chips. Our method is based on our novel
                   use of the much utilized, in software, LZW,
                   particularly LZ77 algorithm. We adapt LZ77 to
                   accommodate bit strings rather than character sets.
                   Moreover, we exploit the large presence of don't cares
                   in the uncompressed test sets that we generated using
                   commercial ATPG tools. Our decompression scheme makes
                   effective use of the on chip boundary scan during
                   decompression and then feeding the internal multiple
                   scan chains for testing. The hardware overhead cost for
                   this scheme is minimal. Experimental results are
                   provided.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/TEST.2002.1041776},
  citeulike-article-id = {4030265},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/TEST.2002.1041776},
  keywords       = {-source-ieee-hw-comp, ian_todo},
  posted-at      = {2009-02-10 15:43:42},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/TEST.2002.1041776},
  year           = 2002
}

@Article{4647069,
  Author         = {Mirkovi\'{c}, M. D. and Stojanovi\'{c}, I. S. },
  Title          = {Algorithm for obtaining a self-synchronising M-ary
                   code enabling data compression},
  Journal        = {Computers and Digital Techniques, IEE Proceedings E},
  Volume         = {134},
  Number         = {2},
  Pages          = {112--118},
  abstract       = {An algorithm for obtaining a self-synchronising M-ary
                   code (M {\^{A}}Â¿ 2) enabling the compression of data
                   from a stationary discrete memoryless source is
                   proposed. After presenting the code algorithm, its
                   properties are analysed and the implementation of the
                   code is described. The code proposed is compared to the
                   Huffman code with regard to the average code-word
                   length, the possibility of self synchronisation and the
                   complexity of hardware realisation. Although for
                   certain sources the code proposed is equal or nearly
                   equal to the Huffman code regarding data compression,
                   in general it is less efficient. However, its property
                   of being self synchronising, and its relatively simple
                   hardware realisation, make this code valuable for
                   practical applications.},
  annote         = {variable length code selbstsynchronisierend(=fehler
                   bleiben auf 1 zeichen beschr{\"a}nkt!) sehr einfach in
                   hw zu implementieren effizienz w{\"a}re zu kl{\"a}ren
                   alt, klassiker in biblio},
  bdsk-url-1     = {http://dx.doi.org/10.1049/ip-e:19870020},
  citeulike-article-id = {4030263},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1049/ip-e:19870020},
  keywords       = {-source-ieee-hw-comp; ian_read; ian_note},
  month          = {March},
  posted-at      = {2009-02-10 15:43:42},
  priority       = {2},
  url            = {http://dx.doi.org/10.1049/ip-e:19870020},
  year           = 1987
}

@Article{4639630,
  Author         = {Lu, Yufeng and Oruklu, E. and Saniie, J. },
  Title          = {Fast Chirplet Transform With FPGA-Based Implementation},
  Journal        = {Signal Processing Letters, IEEE},
  Volume         = {15},
  Pages          = {577--580},
  abstract       = {This letter presents a fast chirplet transform (FCT)
                   algorithm, a computationally efficient method, for
                   decomposing highly convoluted signals into a linear
                   expansion of chirplets. The FCT algorithm successively
                   estimates the chirplet parameters in order to represent
                   a broad range of chirplet shapes, including the
                   broadband, narrowband, symmetric, skewed,
                   nondispersive, or dispersive. These parameters have
                   significant physical interpretations for radar, sonar,
                   seismic, and ultrasonic applications. For the real-time
                   application and embedded implementation of the FCT
                   algorithm, an FPGA-based hardware/software co-design is
                   developed on Xilinx Virtex-II Pro FPGA development
                   platform. Based on the balance among the system
                   constraints, cost, and the efficiency of estimations,
                   the performance of different algorithm implementation
                   schemes have been explored. The developed
                   system-on-chip successfully exhibits robustness in the
                   chirplet transform of experimental signals. The FCT
                   algorithm addresses a broad range of applications
                   including velocity measurement, target detection,
                   deconvolution, object classification, data compression,
                   and pattern recognition.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/LSP.2008.2001816},
  citeulike-article-id = {4030232},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/LSP.2008.2001816},
  keywords       = {-source-ieee-hw-comp-2007-, fpga},
  posted-at      = {2009-02-10 15:43:40},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/LSP.2008.2001816},
  year           = 2008
}

@Article{4427906,
  Author         = {Jeang, Yuan L. and Wey, Tzuu S. and Wang, Hung Y. and
                   Tai, Chih C. and Chu, Ching H. },
  Title          = {A Cycle-Stealing Technique for Pipelined Instruction
                   Decompression System for Embedded Microprocessors},
  Journal        = {Innovative Computing, Information and Control, 2007.
                   ICICIC '07. Second International Conference on},
  Pages          = {261},
  abstract       = {For instruction decompression, many techniques have
                   been developed. However, when branching and cache
                   missing occur, they may either incur delays to refill
                   buffers or sacrifice the compression ratio or slow down
                   the clock rate. This paper presents a new technique
                   based on cycle-stealing technique to eliminate all
                   these defects. The simulation results for several
                   benchmarks show that the average compression ratio, the
                   hardware cost and the speed are all better than other
                   techniques.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/ICICIC.2007.12},
  citeulike-article-id = {4030210},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/ICICIC.2007.12},
  keywords       = {-source-ieee-hw-comp-2007-, embedded-systems; ian_todo},
  month          = {Sept.},
  posted-at      = {2009-02-10 15:43:38},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/ICICIC.2007.12},
  year           = 2007
}

@Article{4167987,
  Author         = {Kavousianos, X. and Kalligeros, E. and Nikolos, D. },
  Title          = {Multilevel Huffman Coding: An Efficient Test-Data
                   Compression Method for IP Cores},
  Journal        = {Computer-Aided Design of Integrated Circuits and
                   Systems, IEEE Transactions on},
  Volume         = {26},
  Number         = {6},
  Pages          = {1070--1083},
  abstract       = {A new test-data compression method suitable for cores
                   of unknown structure is introduced in this paper. The
                   proposed method encodes the test data provided by the
                   core vendor using a new, very effective compression
                   scheme based on multilevel Huffman coding. Each Huffman
                   codeword corresponds to three different kinds of
                   information, and thus, significant compression
                   improvements compared to the already known techniques
                   are achieved. A simple architecture is proposed for
                   decoding the compressed data on chip. Its hardware
                   overhead is very low and comparable to that of the most
                   efficient methods in the literature. Moreover, the
                   major part of the decompressor can be shared among
                   different cores, which reduces the hardware overhead of
                   the proposed architecture considerably. Additionally,
                   the proposed technique offers increased probability of
                   detection of unmodeled faults since the majority of the
                   unknown values of the test sets are replaced by
                   pseudorandom data generated by a linear feedback shift
                   register},
  bdsk-url-1     = {http://dx.doi.org/10.1109/TCAD.2006.885830},
  citeulike-article-id = {4030205},
  date-modified  = {2009-02-10 22:30:30 +0100},
  doi            = {http://dx.doi.org/10.1109/TCAD.2006.885830},
  file           = {file:///home/roman/workspace/zipchip/papers/Kavousianos07.pdf},
  keywords       = {-source-ieee-hw-comp-2007-; ian\_todo; roman\_todo},
  month          = {June},
  posted-at      = {2009-02-10 15:43:38},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/TCAD.2006.885830},
  year           = 2007
}

@Article{Papaefstathiou04a,
  Author         = {Papaefstathiou, I. },
  Title          = {Titan II: an IPcomp processor for 10-Gbps networks},
  Journal        = {Design \& Test of Computers, IEEE},
  Volume         = {21},
  Number         = {6},
  Pages          = {514--523},
  abstract       = {Data compression techniques can alleviate
                   low-bandwidth problems in multigigabit networks, and
                   are especially useful when combined with encryption.
                   The Titan II is a hardware compressor/decompressor core
                   capable of speeds of up to 10 gigabits per second. Its
                   compression algorithm is a variation of the Lempel-Ziv
                   (LZ) algorithm that uses part of the previous input
                   stream as the dictionary.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/MDT.2004.100},
  citeulike-article-id = {4030301},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/MDT.2004.100},
  keywords       = {-source-ieee-hw-lz, hw, lz; ian_todo},
  month          = {Nov.-Dec.},
  posted-at      = {2009-02-10 15:43:45},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/MDT.2004.100},
  year           = 2004
}

@Article{4127443,
  Author         = {Somasundaram, K. and Domnic, S. },
  Title          = {Adaptive Index Coding Scheme for VQ Indices
                   Compression},
  Journal        = {Computing: Theory and Applications, 2007. ICCTA '07.
                   International Conference on},
  Pages          = {639--644},
  abstract       = {An efficient lossless coding scheme to encode vector
                   quantization (VQ) indices is presented in this paper.
                   In our scheme, we have designed a new coding model
                   based on the schemes proposed in the previous works.
                   The computational complexity of the method is quite low
                   and its memory requirement is small. So that it can be
                   implemented in hardware. The experimental results show
                   that the proposed scheme gives better compression than
                   existing lossless index coding schemes},
  bdsk-url-1     = {http://dx.doi.org/10.1109/ICCTA.2007.21},
  citeulike-article-id = {4030181},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/ICCTA.2007.21},
  keywords       = {-source-ieee-hw-comp-2007-, image-coding; ian_todo},
  month          = {March},
  posted-at      = {2009-02-10 15:43:37},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/ICCTA.2007.21},
  year           = 2007
}

@Article{1268945,
  Author         = {Wolff, F. G. and Papachristou, C. and Mcintyre, D. R. },
  Title          = {Test compression and hardware decompression for
                   scan-based SoCs},
  Journal        = {Design, Automation and Test in Europe Conference and
                   Exhibition, 2004. Proceedings},
  Volume         = {1},
  abstract       = {We present a new decompression architecture suitable
                   for embedded cores in SoCs which focuses on improving
                   the download time by avoiding higher internal-to-ATE
                   clock ratios and by exploiting hardware parallelism.
                   The bounded Huffman compression facilitates
                   decompression hardware tradeoffs. Our technique is
                   scalable in that the downloadable RAM-based decode
                   table and accommodates for different SoC cores with
                   different characteristics such as the number of scan
                   chains and test set data distributions.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/DATE.2004.1268945},
  citeulike-article-id = {4030254},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/DATE.2004.1268945},
  keywords       = {-source-ieee-hw-comp},
  month          = {Feb.},
  posted-at      = {2009-02-10 15:43:41},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/DATE.2004.1268945},
  year           = 2004
}

@Article{4584272,
  Author         = {Yu, Guoxia and Vladimirova, T. and Wu, Xiaofeng and
                   Sweeting, M. N. },
  Title          = {A New High-Level Reconfigurable Lossless Image
                   Compression System for Space Applications},
  Journal        = {Adaptive Hardware and Systems, 2008. AHS '08. NASA/ESA
                   Conference on},
  Pages          = {183--190},
  abstract       = {On board image data compression is an important
                   feature of satellite remote sensing payloads.
                   Reconfigurable intellectual property (IP) cores can
                   enable change of functionality or modifications. A new
                   and efficient lossless image compression scheme for
                   space applications is proposed. In this paper, we
                   present a lossless image compression IP core designed
                   using AccelDSP, which gives users high level of
                   flexibility. One typical configuration is implemented
                   and tested on an FPGA prototyping board. Finally, it is
                   integrated successfully into a system-on-chip platform
                   for payload data processing and control.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/AHS.2008.56},
  citeulike-article-id = {4030238},
  date-modified  = {2009-02-12 00:39:36 +0100},
  doi            = {http://dx.doi.org/10.1109/AHS.2008.56},
  file           = {file:///home/roman/workspace/zipchip/papers/Yu08.pdf},
  keywords       = {-source-ieee-hw-comp-2007-, fpga, image-coding;
                   ian\_todo; roman\_todo},
  month          = {June},
  posted-at      = {2009-02-10 15:43:40},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/AHS.2008.56},
  year           = 2008
}

@Article{4556794,
  Author         = {Arava, Prasad V. K. and Jo, Manhwee and Lee,
                   Hyoukjoong and Choi, Kiyoung },
  Title          = {A Generic Design for Encoding and Decoding Variable
                   Length Codes in Multi-codec Video Processing Engines},
  Journal        = {Symposium on VLSI, 2008. ISVLSI '08. IEEE Computer
                   Society Annual},
  Pages          = {197--202},
  abstract       = {The growth of multimedia communication and the mobile
                   convergence lead to a rapid increase in the market of
                   portable multimedia applications. Many of these
                   multimedia applications require supporting multiple
                   codec standards. Although these codec standards are
                   different, they share some common features which can be
                   exploited for hardware implementations of an integrated
                   multi-codec engine. Variable length coding is a
                   lossless data compression technique adopted by most of
                   the codecs. The existing implementations of variable
                   length coding are specific to a particular codec and
                   are not suitable for a multi-codec environment. We
                   propose a methodology for efficiently implementing
                   variable length coding in the multi-codec environment.
                   We demonstrate the effectiveness of our methodology by
                   comparing it with the existing implementations for
                   MPEG4 and H.264.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/ISVLSI.2008.49},
  citeulike-article-id = {4030199},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/ISVLSI.2008.49},
  keywords       = {-source-ieee-hw-comp-2007-},
  month          = {April},
  posted-at      = {2009-02-10 15:43:38},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/ISVLSI.2008.49},
  year           = 2008
}

@Article{4148787,
  Author         = {Kato, M. and Chia},
  Title          = {Compression for Low Power Consumption in
                   Battery-powered Handsets},
  Journal        = {Data Compression Conference, 2007. DCC '07},
  Pages          = {386},
  abstract       = {Only summary form given.Java has been introduced to
                   the mobile/wireless handsets and Java enabled handsets
                   are now prevalent in the market, showing their
                   successful launch. A variety of attractive services
                   have been developed and promise a lot of fun to our
                   daily lives, which are however targeted to the high end
                   products that are very expensive. We focus on economic
                   types with similar performance to the high end
                   products. Memory compression is a key technique to
                   achieve the goal and is integrated into a Java runtime
                   environment. We save memory with runtime compression
                   and reduce power by a memory bank partitioning
                   technique (powering off unused memory banks). In memory
                   compression algorithms and the Java runtime environment
                   are studied. The Wilson and Kaplan (WK) algorithm is
                   discussed as in-memory compression and introduced to
                   the Java heap as an alternative of the garbage
                   collection (gc). Experiments show around 10\% of
                   speedup and up to 45\% of memory saving in the
                   application core part (20.0 of memory overhead for a
                   user interface). The Java system using the memory
                   compression outperforms its base configuration and will
                   give further chance to reduce memory demand in
                   combination with the garbage collection designed for
                   the Java compressed heap. Experiments show that the
                   in-heap memory compression technique can save more than
                   50 \% (up to 60 \%) of the heap demand and that half of
                   the memory banks for the Java heap may never be turned
                   on. 50 \% of the power consumption (memory leakage
                   current) can be saved. The hardware compressor and
                   decompressor are currently ten thousand times faster
                   than the software counterparts. The time overhead could
                   be negligible by introducing the hardware compressor
                   and decompressor. Hardware/software codesign is an
                   additional key to achieve our goal. The contribution of
                   the runtime memory and heap compression is threefold:
                   small memory demand, lower power consumption, and
                   negligible time overhead in battery-powered handsets.
                   Future work i- - ncludes several issues unsolved in the
                   Java memory compression: studies of integration of CLDC
                   Hot Spot implementation and extension of the Java
                   virtual machine to the traditional virtual machine.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/DCC.2007.24},
  citeulike-article-id = {4030176},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/DCC.2007.24},
  keywords       = {-source-ieee-hw-comp-2007-},
  month          = {March},
  posted-at      = {2009-02-10 15:43:37},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/DCC.2007.24},
  year           = 2007
}

@Article{4289488,
  Author         = {Jung, Sangkil and Hong, Sangjin and Kim, Kyungtae and
                   Jee, Junghoon and Kim, Eunah },
  Title          = {Voice Transmission Enhancing Model on Wireless Mesh
                   Networks},
  Journal        = {Communications, 2007. ICC '07. IEEE International
                   Conference on},
  Pages          = {4949--4954},
  abstract       = {This paper initially shows ROHC and packet aggregation
                   significantly improve the number of successful voice
                   calls. However, the improvement does not include
                   processor's processing overhead, which is identified by
                   measuring ROHC processing time from Intel Pentium 4 and
                   RouterBOARD, and applying the results into NS-2
                   simulations. Simulation results indicate the number of
                   successful voice calls is seriously affected by the
                   processing overhead. For the solution of the processing
                   limitation, we propose a hardware model of the two
                   algorithms and numerically analyze the model.
                   Simulation results show the hardware model makes the
                   number of successful voice calls approach the ideal
                   (no-delay) case, and the numerical model exactly
                   characterizes the hardware behaviors.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/ICC.2007.817},
  citeulike-article-id = {4030217},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/ICC.2007.817},
  keywords       = {-source-ieee-hw-comp-2007-},
  month          = {June},
  posted-at      = {2009-02-10 15:43:39},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/ICC.2007.817},
  year           = 2007
}

@Article{4348179,
  Author         = {Long, Yan X. and Jun, Yu },
  Title          = {Improvement on Realization Structure of Block Matching
                   Algorithm and its Performance Verification},
  Journal        = {Communications, Circuits and Systems, 2007. ICCCAS
                   2007. International Conference on},
  Pages          = {834--836},
  abstract       = {Full search block matching algorithm (FSBMA) is a full
                   search method which has the highest search precision
                   among block matching algorithms. However, it has large
                   numbers of calculations, so it doesn't match to
                   real-time application. This paper brings out an
                   improved realization structure to realize motion
                   estimate (ME) of FSBMA. The hardware structure takes
                   module design method, fully uses hardware resources and
                   adapts pipeline technology, the verification of its
                   performance also presented.},
  citeulike-article-id = {4030219},
  date-modified  = {2009-02-12 01:23:39 +0100},
  file           = {file:///home/roman/workspace/zipchip/papers/Long07.pdf},
  keywords       = {-source-ieee-hw-comp-2007-; roman\_todo},
  month          = {July},
  posted-at      = {2009-02-10 15:43:39},
  priority       = {2},
  year           = 2007
}

@Article{755668,
  Author         = {Balkenhol, B. and Kurtz, S. and Shtarkov, Y.M.},
  Title          = {Modifications of the burrows and wheeler data
                   compression algorithm},
  Journal        = {Data Compression Conference, 1999. Proceedings. DCC
                   '99},
  Volume         = {},
  Number         = {},
  Pages          = {188-197},
  abstract       = {We improve upon previous results on the Burrows and
                   Wheeler (BW)-algorithm. Based on the context tree
                   model, we consider the specific statistical properties
                   of the data at the output of the BWT. We describe six
                   important properties, three of which have not been
                   described elsewhere. These considerations lead to
                   modifications of the coding method, which in turn
                   improve the coding efficiency. We briefly describe how
                   to compute the BWT with low complexity in time and
                   space, using suffix trees in two different
                   representations. Finally, we present experimental
                   results about the compression rate and running time of
                   our method, and compare these results to previous
                   achievements},
  doi            = {10.1109/DCC.1999.755668},
  file           = {file:///home/roman/workspace/zipchip/papers/Balkenhol99.pdf},
  issn           = {},
  keywords       = {roman\_todo},
  month          = {Mar},
  year           = 1999
}

@Article{Ranganathan91a,
  Author         = {Ranganathan, N. and Henriques, S. },
  Title          = {A high speed VLSI chip for data compression},
  Journal        = {University/Government/Industry Microelectronics
                   Symposium, 1991. Proceedings., Ninth Biennial},
  Pages          = {190--194},
  abstract       = {The authors describe a high-speed VLSI chip that
                   implements the LZ technique for data compression. The
                   LZ technique for data compression involves two basic
                   steps, parsing and coding. The LZ-based compression
                   method is a powerful technique and gives high
                   compression efficiency for text and image data. The
                   architecture is systolic and uses the principles of
                   pipelining and parallelism in order to obtain high
                   speed and throughput. Hardware schemes are proposed for
                   decompressing data that has been compressed using the
                   LZ method. The data compression hardware can be
                   integrated into real-time systems so that data can be
                   compressed and decompressed on-the-fly. A prototype
                   CMOS VLSI chip has been designed and fabricated using
                   CMOS 2-micron technology implementing a systolic array
                   of nine processors. The proposed hardware can yield
                   compression rates of about 20 million characters per
                   second},
  bdsk-url-1     = {http://dx.doi.org/10.1109/UGIM.1991.148148},
  citeulike-article-id = {4030294},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/UGIM.1991.148148},
  keywords       = {-source-ieee-hw-lz, hw, lz, ian_todo},
  month          = {Jun},
  posted-at      = {2009-02-10 15:43:44},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/UGIM.1991.148148},
  year           = 1991
}

@Article{1188694,
  Author         = {Nunez, J. L. and Jones, S. },
  Title          = {Lossless data compression programmable hardware for
                   high-speed data networks},
  Journal        = {Field-Programmable Technology, 2002. (FPT).
                   Proceedings. 2002 IEEE International Conference on},
  Pages          = {290--293},
  abstract       = {This paper presents a high-performance application
                   specific architecture for real time lossless data
                   compression, which enables data throughputs over 1.5
                   Gbits/s compression and decompression using
                   contemporary low-cost re-programmable FPGA technology.
                   The implementation is embedded into a PCI-based system
                   and tested at speed using a PC as the host computer.. A
                   single FPGA is used to map all the functions in the
                   system including the compression and decompressor
                   cores, DMA logic, control logic and Master/Target PCI
                   core. The independent compression and decompression
                   channels enable a combined compression and
                   decompression performance over 3 Gbits/s and robust
                   self-checking hardware where each compress block can be
                   automatically decompressed to defect hardware failures
                   or errors introduced by the communication channel.},
  citeulike-article-id = {4030249},
  date-modified  = {2009-02-12 00:49:20 +0100},
  keywords       = {-source-ieee-hw-comp, fpga; ian_todo},
  month          = {Dec.},
  posted-at      = {2009-02-10 15:43:41},
  priority       = {2},
  year           = 2002
}

@Article{4389113,
  Author         = {Li, Maotang and Cheng, H. D. and Wang, Yejun },
  Title          = {Novel Airborne Real-Time Survey System},
  Journal        = {Instrumentation and Measurement, IEEE Transactions on},
  Volume         = {56},
  Number         = {6},
  Pages          = {2404--2410},
  abstract       = {Although satellite imaging is widely used today, an
                   airborne survey system is still more efficient and
                   effective under certain circumstances. Airborne survey
                   can successfully detect and transmit target information
                   in real time that is useful and critical for related
                   agencies, such as environmental management,
                   natural-disaster monitor and control, etc. Using a
                   real-time synthetic-aperture-radar image processing
                   approach based on a distribution system, image data are
                   synchronized with a global positioning system; a
                   satellite-communication system of data transmission is
                   designed and implemented; and a novel image-compression
                   method, which is wavelet-based, is developed. The main
                   idea of the proposed system is based on a
                   function-distribution personal computer (PC) system:
                   Data preprocessing, post processing, and extraction of
                   images, data processing, image display, and image
                   printing, are performed by different PCs. The method of
                   image transmission proposed here is based on the
                   satellite communication system. The proposed system has
                   the advantage of low cost (both hardware prototype and
                   maintenance) and rapid processing speed (only in
                   several seconds; the disaster images from the air are
                   processed and transmitted to the Disaster Reduction
                   Center and to other users by the satellite
                   communication system). The system has been applied to a
                   real-time disaster monitor (such as a forest fire, a
                   flood, etc.), real-time target tracking, large-scale
                   topographic mapping, environmental survey, etc.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/TIM.2007.907952},
  citeulike-article-id = {4030208},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/TIM.2007.907952},
  keywords       = {-source-ieee-hw-comp-2007-},
  month          = {Dec.},
  posted-at      = {2009-02-10 15:43:38},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/TIM.2007.907952},
  year           = 2007
}

@TechReport{BW94,
  Author         = {Burrows, M. and Wheeler, D. J.},
  Title          = {A block-sorting lossless data compression algorithm},
  Institution    = {Digital SRC Research Report},
  abstract       = {We describe a block-sorting, lossless data compression
                   algorithm, and our implementation of that algorithm. We
                   compare the performance of our implementation with
                   widely available data compressors running on the same
                   hardware. The algorithm works by applying a reversible
                   transformation to a block of input text. The
                   transformation does not itself compress the data, but
                   reorders it to make it easy to compress with simple
                   algorithms such as move-to-front coding. Our algorithm
                   achieves speed comparable to algorithms based on the
                   techniques of Lempel and Ziv, but obtains compression
                   close to the best statistical modelling techniques. The
                   size of the input block must be large (a few kilobytes)
                   to achieve good compression. },
  comment        = {Sehr sehr lesenswertes Original zu bzip2. Gute
                   Erkl\"arung zu Kompression und Dekompression + Hinweise
                   zu entstehenden Codestrukturen die f\"ur
                   Hardwareoptimierungen interessant sein k\"onnten. Gute
                   Kompression erst ab Kb-Blockgr\"o\ss{}en ->
                   Speicherverbrauch - Permutationen der Matrix sicher
                   parallel m\"oglich - gzip 2.71 bit/char vs. bzip2 2.43
                   bit/char. Kombination aus BWT + Move to front + Huffman
                   -> sortieren der rotationen ist das Hauptproblem bei
                   BWT was den Speicherverbrauch betrifft, daf\"ur ist
                   eine statistische Analyse f\"ur Huffman durch Move to
                   front obsolet.},
  file           = {file:///home/roman/workspace/zipchip/papers/Burrows94.pdf},
  keywords       = {roman\_todo},
  year           = 1994
}

@Article{1637517,
  Author         = {Kotteri, K. A. and Bell, A. E. and Carletta, J. E. },
  Title          = {Multiplierless filter Bank design: structures that
                   improve both hardware and image compression performance},
  Journal        = {Circuits and Systems for Video Technology, IEEE
                   Transactions on},
  Volume         = {16},
  Number         = {6},
  Pages          = {776--780},
  abstract       = {Design techniques for high-performance, fixed-point,
                   multiplierless filter banks are presented. Image
                   compression using the biorthogonal 9/7 discrete wavelet
                   transform provides a motivating example. Image
                   compression and hardware performance of two commonly
                   used filter structures, direct and cascade, and two
                   known filter bank structures, nonpolyphase and
                   polyphase, are compared. A technique is shown for
                   designing a fixed-point polyphase filter structure,
                   which is highly efficient from a hardware standpoint,
                   such that image-compression quality is not
                   significantly deteriorated by the use of fixed-point
                   mathematics. The result is a polyphase structure with
                   about twice the throughput rate of nonpolyphase
                   structures, and peak signal-to-noise ratio values for
                   lossy compression within 0.2 decibels of those achieved
                   using floating-point filters.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/TCSVT.2006.876360},
  citeulike-article-id = {4030262},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/TCSVT.2006.876360},
  keywords       = {-source-ieee-hw-comp},
  month          = {June},
  posted-at      = {2009-02-10 15:43:42},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/TCSVT.2006.876360},
  year           = 2006
}

@Article{4351789,
  Author         = {Jung, Sangkil and Hong, Sangjin and Kim, Kyungtae },
  Title          = {On Achieving High Performance Wireless Mesh Networks
                   With Data Fusion},
  Journal        = {World of Wireless, Mobile and Multimedia Networks,
                   2007. WoWMoM 2007. IEEE International Symposium on a},
  Pages          = {1--8},
  abstract       = {Wireless Mesh Network (WMN) supports various types of
                   mesh clients such as sensor networks, cellular;
                   802.11/e Wireless LAN, etc. In this case, it needs to
                   be suitably organized and adopt well-known or newly
                   proposed algorithm to guarantee high and reliable mesh
                   performance. This paper applies RObust Header
                   Compression (ROHC) originate from cellular data network
                   plus packet aggregation to WMNs when mesh clients
                   generate small-size fusion data, i.e., sensor
                   (delivered by UDP) and voice over IP (VOIP) data. We
                   preliminarily analyze ROHC and packet aggregation
                   improve UDP throughput and the number of supported
                   voice calls. However; the improvement does not include
                   a processor's processing overhead for ROHC and packet
                   aggregation, which is examined by measuring ROHC
                   execution time from Intel Pentium 4 and RouterBOARD,
                   and applying the results into NS-2 simulations. The
                   results indicate UDP and voice performance are
                   seriously affected by the processing overhead. As a
                   solution for the performance degradation, we introduce
                   a hardware design for the two algorithms. By using
                   SystemC Hardware Description Language (HDL), we first
                   design hardware specification composed of 1Ghz master
                   clock and 333Mhz BUS system. Then, we integrate the
                   hardware model into previous NS-2 network model, and
                   propose profile-based network/hardware co-simulation
                   method which gives insight how to investigate
                   real-timing characteristics of the hardware model in
                   the view of network behaviors.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/WOWMOM.2007.4351789},
  citeulike-article-id = {4030179},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/WOWMOM.2007.4351789},
  keywords       = {-source-ieee-hw-comp-2007-},
  month          = {June},
  posted-at      = {2009-02-10 15:43:37},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/WOWMOM.2007.4351789},
  year           = 2007
}

@Article{1183487,
  Author         = {Benini, L. and Bruni, D. and Macii, A. and Macii, E. },
  Title          = {Hardware implementation of data compression algorithms
                   for memory energy optimization},
  Journal        = {VLSI, 2003. Proceedings. IEEE Computer Society Annual
                   Symposium on},
  Pages          = {250--251},
  abstract       = {This paper describes implementation details of a
                   hardware compression and decompression unit (CDU) for
                   optimizing energy consumption in processor-based
                   systems. Many algorithms for data compression (i.e.,
                   profile-driven, adaptive, differential) have previously
                   been introduced. In all cases, data compression and
                   decompression are performed on-the-fly on the
                   cache-to-memory path: Uncompressed cache fines are
                   compressed before they are written back to main memory,
                   and decompressed when cache refills occur. This paper
                   completes and extends these previous contributions by
                   providing evidence on the feasibility of the proposed
                   compression architectures by specifically addressing
                   hardware implementation issues. CDU design is targeted
                   towards energy minimization in the cache-bus-memory
                   subsystem with a strict constraint on performance. As a
                   result, average memory energy reductions evaluated on
                   several benchmark programs are around 24\%, at no
                   performance penalty.},
  citeulike-article-id = {4030255},
  date-modified  = {2009-02-12 01:23:39 +0100},
  keywords       = {-source-ieee-hw-comp; ian_todo},
  month          = {Feb.},
  posted-at      = {2009-02-10 15:43:41},
  priority       = {2},
  year           = 2003
}

@Article{933839,
  Author         = {Mukherjee, A. and Motgi, N. and Becker, J. and Friebe,
                   A. and Habermann, C. and Glesner, M. },
  Title          = {Prototyping of efficient hardware algorithms for data
                   compression in future communication systems},
  Journal        = {Rapid System Prototyping, 12th International Workshop
                   on, 2001.},
  Pages          = {58--63},
  abstract       = {Due to the high bandwidth requirements of up to 2
                   Mbit/s in 3rd-generation mobile communication systems,
                   efficient data compression approaches are necessary to
                   reduce communication and storage costs. The status of
                   recent VLSI technologies promises complete
                   system-on-a-chip (SoC) solutions for both mobile and
                   network-based communication systems, including new
                   compression algorithms based on the Burrows-Wheeler
                   transform (BWT). The most complex task of the BWT
                   algorithm is its lexicographic sorting of n cyclic
                   rotations of a given string of n characters. This paper
                   discusses the feasibility and VLSI implementation of
                   this scalable BWT architecture in simulating and
                   prototyping its systolic, highly utilized hardware
                   structure with Virtex FPGAs},
  bdsk-url-1     = {http://dx.doi.org/10.1109/IWRSP.2001.933839},
  citeulike-article-id = {4030279},
  comment        = {stellt den weaversorter algorithmus vor =
                   shiftalgorithmus zum textsortieren. braucht f\"ur 100
                   Zeichen input bei BWT etwa 800 cycles. Vielleicht guter
                   Richtwert? f\"uhrt specialchar f\"ur EOS ein um
                   Zusatzlogik f\"ur periodische strings zu vermeiden.
                   Pseudocode f\"ur VHDL-entwurf des shifters im text.},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/IWRSP.2001.933839},
  file           = {file:///home/roman/workspace/zipchip/papers/Mukherjee01.pdf},
  keywords       = {-source-ieee-hw-comp, fpga, ian\_todo, roman\_info},
  posted-at      = {2009-02-10 15:43:43},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/IWRSP.2001.933839},
  year           = 2001
}

@Article{1201591,
  Author         = {Jas, A. and Ghosh-Dastidar, J. and Mom-Eng Ng and
                   Touba, N.A.},
  Title          = {An efficient test vector compression scheme using
                   selective Huffman coding},
  Journal        = {Computer-Aided Design of Integrated Circuits and
                   Systems, IEEE Transactions on},
  Volume         = {22},
  Number         = {6},
  Pages          = { 797-806},
  abstract       = { This paper presents a compression/decompression
                   scheme based on selective Huffman coding for reducing
                   the amount of test data that must be stored on a tester
                   and transferred to each core in a system-on-a-chip
                   (SOC) during manufacturing test. The test data
                   bandwidth between the tester and the SOC is a
                   bottleneck that can result in long test times when
                   testing complex SOCs that contain many cores. In the
                   proposed scheme, the test vectors for the SOC are
                   stored in compressed form in the tester memory and
                   transferred to the chip where they are decompressed and
                   applied to the cores. A small amount of on-chip
                   circuitry is used to decompress the test vectors. Given
                   the set of test vectors for a core, a modified Huffman
                   code is carefully selected so that it satisfies certain
                   properties. These properties guarantee that the
                   codewords can be decoded by a simple pipelined decoder
                   (placed at the serial input of the core's scan chain)
                   that requires very small area. Results indicate that
                   the proposed scheme can provide test data compression
                   nearly equal to that of an optimum Huffman code with
                   much less area overhead for the decoder.},
  doi            = {10.1109/TCAD.2003.811452},
  file           = {file:///home/roman/workspace/zipchip/papers/Jas03.pdf},
  issn           = {0278-0070},
  keywords       = {roman_todo},
  month          = {June},
  year           = 2003
}

@Article{1291818,
  Author         = {Milward, M. and Nunez, J. L. and Mulvaney, D. },
  Title          = {Design and implementation of a lossless parallel
                   high-speed data compression system},
  Journal        = {Parallel and Distributed Systems, IEEE Transactions on},
  Volume         = {15},
  Number         = {6},
  Pages          = {481--490},
  abstract       = {Logic density increases have made feasible the
                   implementation of multiprocessor systems able to meet
                   the intensive data processing demands of highly
                   concurrent systems. We describe the research and
                   hardware implementation of a high-performance parallel
                   multicompressor chip. A detailed investigation into the
                   performances of alternative input and output routing
                   strategies for realistic data sets demonstrate that the
                   design of parallel compression devices involves
                   important trade offs that affect compression
                   performance, latency, and throughput. The most
                   promising approach is implemented into FPGA hardware
                   and is shown to provide a scalable compression solution
                   at throughputs able to cope with the demands of modern
                   high-bandwidth applications.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/TPDS.2004.7},
  citeulike-article-id = {4030287},
  date-modified  = {2009-02-12 00:49:03 +0100},
  doi            = {http://dx.doi.org/10.1109/TPDS.2004.7},
  keywords       = {-source-ieee-hw-comp, fpga; ian_todo},
  month          = {June},
  posted-at      = {2009-02-10 15:43:44},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/TPDS.2004.7},
  year           = 2004
}

@Article{4564647,
  Author         = {Torbati, A. H. H. N. and Attari, M. A. and Moosavinia,
                   A. },
  Title          = {A new criterion to compare different LDPC codes for
                   hardware implementation},
  Journal        = {Electrical and Computer Engineering, 2008. CCECE 2008.
                   Canadian Conference on},
  Pages          = {000805--000808},
  abstract       = {In this paper we propose a new combinational criterion
                   as a mean for comparison between different LDPC codes
                   in terms of complexity and efficiency. The main idea
                   behind the proposed criterion is to merge different
                   aspects of a code to a unique number. We will show that
                   the new criterion, named ldquoGoodness Numberrdquo or
                   GN, is applicable, and in some circumstances it is
                   preferred to other criteria such as complexity and
                   error probability criteria and at the same time it
                   maintains computational simplicity.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/CCECE.2008.4564647},
  citeulike-article-id = {4030247},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/CCECE.2008.4564647},
  keywords       = {-source-ieee-hw-comp-2007-; ian_todo},
  month          = {May},
  posted-at      = {2009-02-10 15:43:40},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/CCECE.2008.4564647},
  year           = 2008
}

@Article{727008,
  Author         = {Cotner, C.B. and Roberts, T.},
  Title          = {Accelerating data for the warfighter},
  Journal        = {Military Communications Conference, 1998. MILCOM 98.
                   Proceedings., IEEE},
  Volume         = {3},
  Pages          = {1048-1054 vol.3},
  abstract       = {A methodology for efficiently integrating
                   geosynchronous satellite links into communications
                   networks by employing modern digital technology, in
                   particular the asynchronous transfer mode (ATM), is
                   explained. The concept of seamless interoperability is
                   examined, and a vital C4I US Army network (the
                   Warfighter Information Network) which will employ ATM
                   is described. ATM cell structure is explained, and it
                   is shown that the header error correction byte can be
                   stripped from the cell, and idle ATM cells removed to
                   accelerate the delivery of data. A purpose-designed
                   interface, COMSAT's ATM Link Accelerator is then
                   described which uses the header error correction byte
                   and idle cells, as well as both header and data
                   compression, to efficiently provide adaptive error
                   correction for error-free transmission of ATM over the
                   satellite link while maximizing data throughput. The
                   results of tests conducted using this network interface
                   equipment in two separate CECOM facilities (the DSCS
                   ITF at Ft. Monmouth, New Jersey, and the Trojan ATM
                   Test Laboratory at Ft. Huachuca, Arizona) are presented
                   and discussed },
  bdsk-url-1     = {http://dx.doi.org/10.1109/MILCOM.1998.727008},
  date-added     = {2009-02-11 02:30:27 +0100},
  date-modified  = {2009-02-11 02:30:45 +0100},
  doi            = {10.1109/MILCOM.1998.727008},
  keywords       = {ian_todo},
  month          = {Oct},
  year           = 1998
}

@Article{4537134,
  Author         = {Zhan, Wenfa },
  Title          = {An efficient collaborative test data compression
                   scheme based on OLEL coding},
  Journal        = {Computer Supported Cooperative Work in Design, 2008.
                   CSCWD 2008. 12th International Conference on},
  Pages          = {1107--1111},
  abstract       = {A new test data compression scheme is introduced in
                   this paper. The proposed scheme encodes the test data
                   provided by the core vendor, using a new and very
                   effective compression scheme based on OLEL coding. In
                   this scheme, codeword is divided into two parts
                   according to the position: odd bits and even bits. The
                   odd bits of codeword are used to represent the length
                   of runs and the even bits of codeword are used to
                   represent whether a run is finished. The length of
                   codeword is in accordance with the distribution of the
                   length of run in actual test data, and thus,
                   significant compression improvements compared with the
                   already known schemes are achieved. A simple
                   architecture is proposed for decoding the compressed
                   data on chip. Its hardware overhead is very low and
                   comparable to that of the most efficient methods in the
                   literature. Experimental results for the six largest
                   ISCAS-89 benchmark circuits show that the proposed
                   scheme is obviously better than the already known
                   schemes in the aspects of compression ratio and the
                   decompression structure, such as Golomb, FDR, and MTC
                   coding schemes.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/CSCWD.2008.4537134},
  citeulike-article-id = {4030237},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/CSCWD.2008.4537134},
  keywords       = {-source-ieee-hw-comp-2007-},
  month          = {April},
  posted-at      = {2009-02-10 15:43:40},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/CSCWD.2008.4537134},
  year           = 2008
}

@Article{4664366,
  Author         = {Kochanek, J. and Lansky, J. and Uzel, P. and Zemlicka,
                   M. },
  Title          = {The new statistical compression method: Multistream
                   compression},
  Journal        = {Applications of Digital Information and Web
                   Technologies, 2008. ICADIWT 2008. First International
                   Conference on the},
  Pages          = {320--325},
  abstract       = {We present here a new statistical lossless data
                   compression technique. Its performance is a bit higher
                   than the one of arithmetic coding. It can be used in
                   combination with other data transforms to create
                   powerful compressing applications. According to our
                   knowledge the principle, on which the method is built,
                   is new. It is therefore likely that it will be an
                   interesting topic for further research, testing, and
                   improvements. The presented method can be often used as
                   a replacement of some existing methods (Huffman and
                   arithmetic coding) as it often gives better results.
                   This method gives interesting results if applied as
                   final phase of Burrows-Wheeler transform based
                   compression. We suppose that it can be very useful in
                   compression of document databases. It can be also used
                   for image compression, for example as a step processing
                   the output of discrete cosine transformation (after
                   quantization) used in JPEG. The hardware support can
                   highly benefit from the fact that this method can be
                   efficiently pipelined what promises high throughput.},
  annote         = {rein software ausf\"uhrliche analyse und messungen
                   (link auf test-korpus!) pipline-bar neu vorgeschalteter
                   BWT m\"oglich bisserl kompliziert?},
  bdsk-url-1     = {http://dx.doi.org/10.1109/ICADIWT.2008.4664366},
  citeulike-article-id = {4030243},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/ICADIWT.2008.4664366},
  file           = {file:///home/roman/workspace/zipchip/papers/Kochanek08.pdf},
  keywords       = {-source-ieee-hw-comp-2007-; ian\_note; roman\_todo},
  month          = {Aug.},
  posted-at      = {2009-02-10 15:43:40},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/ICADIWT.2008.4664366},
  year           = 2008
}

@Article{541976,
  Author         = {Jer-Min Jou and Shiann-Rong Kuang and Yuh-Lin Chen and
                   Chung-Yuan Chiang},
  Title          = {An on line adaptive data compression chip using
                   arithmetic codes },
  Journal        = {Circuits and Systems, 1996. ISCAS '96., 'Connecting
                   the World'., 1996 IEEE International Symposium on},
  Volume         = {4},
  Pages          = {360-363 vol.4},
  abstract       = {This paper describes the design and implementation of
                   a CMOS VLSI chip for data compression and decompression
                   using adaptive binary arithmetic codes. During the
                   design process, the systematic design methodology of
                   high level synthesis is applied so that both of the
                   minimum of hardware resource and the maximum of
                   processing speed about the chip are compromised
                   soundly. The chip implements a new flexible modeler
                   which estimates the probabilities of binary symbols
                   efficiently using the table-look-up approach with 1024
                   bytes SRAM and 288 bytes ROM. An asynchronous interface
                   circuit for I/O communication of the chip is designed
                   thus the I/O operation and compression operation in the
                   chip can be done in parallel. The concept of design for
                   testability is used and a full scan is implemented in
                   the chip. A prototype 0.8-micro chip has been designed
                   and verified, and fabricated by CIC, it occupies
                   4.2*4.5 mm2 of silicon area. The chip can yield a
                   compression and decompression rate of 3 Mbits/sec with
                   a clock rate of 25 MHz},
  bdsk-url-1     = {http://dx.doi.org/10.1109/ISCAS.1996.541976},
  date-added     = {2009-02-11 02:30:27 +0100},
  date-modified  = {2009-02-11 02:30:39 +0100},
  doi            = {10.1109/ISCAS.1996.541976},
  keywords       = {ian_todo},
  month          = {May},
  year           = 1996
}

@Article{4635066,
  Author         = {Ron, Yongfeng and Liu, Xiaohua and Xu, Wenqiang and
                   Zhang, Wendong },
  Title          = {Multi-channel data compression},
  Journal        = {Aerospace and Electronic Systems Magazine, IEEE},
  Volume         = {23},
  Number         = {9},
  Pages          = {14--21},
  abstract       = {With the development of space telemetry technology, it
                   has brought forward higher requests for large capacity
                   memory of hard recovered solid-state recorders.
                   Therefore, data compression becomes more important. The
                   compression feasibility and potentiality of telemetry
                   data are examined by analyzing the statistical
                   characteristics of actual telemetry data recovered from
                   the recorder. Aiming at the disadvantages of present
                   data format In data compression for multi-channel
                   telemetry data acquisition systems, we introduce data
                   packet structure and a real-time compression algorithm
                   for low complex hardware design put forward
                   accordingly. The principles and implementation of data
                   package compression are described. Simulation results
                   show that this technology can meet the requirements of
                   multi-channel real-time data compression with high
                   compression ratio and fast compression speed, which
                   possesses great application values.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/MAES.2008.4635066},
  citeulike-article-id = {4030233},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/MAES.2008.4635066},
  file           = {file:///home/roman/workspace/zipchip/papers/Ron08.pdf},
  keywords       = {-source-ieee-hw-comp-2007-; ian_todo},
  month          = {Sept.},
  posted-at      = {2009-02-10 15:43:40},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/MAES.2008.4635066},
  year           = 2008
}

@Article{4297284,
  Author         = {Chrysos, G. and Dollas, A. and Bourbakis, N. and
                   Mertoguno, S. },
  Title          = {An Integrated Video Compression, Encryption and
                   Information Hiding Architecture based on the SCAN
                   Algorithm and the Stretch Technology},
  Journal        = {Field-Programmable Custom Computing Machines, 2007.
                   FCCM 2007. 15th Annual IEEE Symposium on},
  Pages          = {327--330},
  abstract       = {SCAN is a class of formal languages for compression,
                   encryption and information hiding. We have previously
                   studied and reported separate hardware implementations
                   of SCAN compression and encryption. This paper presents
                   initial results on the design of a complete single-chip
                   system for the SCAN compression, encryption and
                   information hiding algorithm using the stretch
                   technology with reconfigurable and fixed resources. The
                   result is a simple, low cost, embeddable core combining
                   all three operations seamlessly and the design was
                   fully mapped to the stretch technology.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/FCCM.2007.37},
  citeulike-article-id = {4030188},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/FCCM.2007.37},
  keywords       = {-source-ieee-hw-comp-2007-},
  month          = {April},
  posted-at      = {2009-02-10 15:43:37},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/FCCM.2007.37},
  year           = 2007
}

@Article{1607255,
  Author         = {Franaszek, P. A. and Montano, Lastras L. A. and Peng,
                   Song and Robinson, J. T. },
  Title          = {Data compression with restricted parsings},
  Journal        = {Data Compression Conference, 2006. DCC 2006.
                   Proceedings},
  Pages          = {203--212},
  abstract       = {We consider a class of algorithms related to
                   Lempel-Ziv that incorporate restrictions on the manner
                   in which the data can be parsed with the goal of
                   introducing new tradeoffs between implementation
                   complexity and data compression ratios. Our main
                   motivation lies within the field of compressed memory
                   computer systems. Here requirements include extremely
                   fast decompression and compression speeds, adequate
                   compression performance on small data block lengths,
                   and minimal hardware area and energy requirements. We
                   describe the approach and provide experimental data
                   concerning its compression performance with respect to
                   known alternatives. We show that for a variety of data
                   sets stored in a typical main memory, this direction
                   yields results close to those of earlier techniques,
                   but with significantly lower energy consumption at
                   comparable or better area requirements. The technique
                   thus may be of eventual interest for a number of
                   applications requiring high compression bandwidths and
                   efficient hardware implementation},
  bdsk-url-1     = {http://dx.doi.org/10.1109/DCC.2006.22},
  citeulike-article-id = {4030292},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/DCC.2006.22},
  file           = {file:///home/roman/workspace/zipchip/papers/Franaszek06.pdf},
  keywords       = {-source-ieee-hw-comp; ian\_todo; roman\_todo},
  month          = {March},
  posted-at      = {2009-02-10 15:43:44},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/DCC.2006.22},
  year           = 2006
}

@Article{SSS07:eandi,
  Author         = {Schmid, Ulrich and Steininger, Andreas and Sust,
                   Manfred },
  Title          = {{FIT-IT}-{P}rojekt {DARTS}: {D}ezentrale
                   fehlertolerante {T}aktgenerierung},
  Journal        = {Elektrotechnik {\&} Informationstechnik e{\&}i)
                   1-2/07, 2007},
  annote         = {Zusammenfassung DARTS},
  bdsk-url-1     = {http://www.vmars.tuwien.ac.at/php/pserver/extern/download.php?fileid=1379},
  citeulike-article-id = {4026352},
  date-modified  = {2009-02-12 01:23:39 +0100},
  keywords       = {darts},
  month          = {Jan.},
  posted-at      = {2009-02-09 19:53:16},
  priority       = {2},
  publisher      = {Springer Verlag},
  url            = {http://www.vmars.tuwien.ac.at/php/pserver/extern/download.php?fileid=1379},
  year           = 2007
}

@Article{546466,
  Author         = {Kjelso, M. and Gooch, M. and Jones, S. },
  Title          = {Design and performance of a main memory hardware data
                   compressor},
  Journal        = {EUROMICRO 96. 'Beyond 2000: Hardware and Software
                   Design Strategies'., Proceedings of the 22nd EUROMICRO
                   Conference},
  Pages          = {423--430},
  abstract       = {In this paper we show that hardware main memory data
                   compression is both feasible and worthwhile. We
                   demonstrate that paging due to insufficient memory
                   resources can reduce system performance several fold,
                   and argue that hardware memory compression can
                   eliminate this paging hence providing a substantial
                   performance improvement. We describe the design and
                   implementation of a novel compression method, the
                   X-Match algorithm, which is efficient at compressing
                   small blocks of data and suitable for high-speed
                   hardware implementation. Our experimental investigation
                   shows that on average the X-Match algorithm doubles the
                   memory capacity for commonly used Unix applications.
                   Furthermore, the substantial impact such memory
                   compression has on overall system performance is
                   demonstrated},
  bdsk-url-1     = {http://dx.doi.org/10.1109/EURMIC.1996.546466},
  citeulike-article-id = {4030250},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/EURMIC.1996.546466},
  keywords       = {-source-ieee-hw-comp; ian_todo},
  month          = {Sep},
  posted-at      = {2009-02-10 15:43:41},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/EURMIC.1996.546466},
  year           = 1996
}

@Article{651737,
  Author         = {Snyder, J. H. and Murakami, G. J. and Ruedesueli, L.
                   W. and Palermo, J. },
  Title          = {Implementation and optimization of a real-time PAC
                   codec},
  Journal        = {Industrial Electronics, 1997. ISIE '97., Proceedings
                   of the IEEE International Symposium on},
  Volume         = {1},
  abstract       = {The authors describe an implementation of the
                   perceptual audio coding (PAC) audio compression
                   algorithm on PowerPC hardware designed to support
                   simultaneous real-time compression and decompression of
                   audio for transmission over a network. They describe
                   the algorithm variant used, the target hardware and
                   software development tools, and then describe the
                   various optimizations that reduced the complexity
                   sufficiently to allow the algorithm to run in
                   real-time. They conclude by describing a scheduling
                   algorithm that allows them to manage the complexity
                   associated with the compression of `difficult' music},
  bdsk-url-1     = {http://dx.doi.org/10.1109/ISIE.1997.651737},
  citeulike-article-id = {4030281},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/ISIE.1997.651737},
  keywords       = {-source-ieee-hw-comp},
  month          = {Jul},
  posted-at      = {2009-02-10 15:43:43},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/ISIE.1997.651737},
  year           = 1997
}

@Article{4426356,
  Author         = {Dandawate, Y. H. and Joshi, M. A. and Chitre, A. V. },
  Title          = {Quality Analysis of Color Images Compressed with
                   Enhanced Vector Quantizer Designed Using HSI Color
                   Space},
  Journal        = {Conference on Computational Intelligence and
                   Multimedia Applications, 2007. International Conference
                   on},
  Volume         = {3},
  Pages          = {138--142},
  abstract       = {This paper presents a novel technique of improving the
                   quality of decompressed color images. For compression
                   of images vector quantization (VQ) technique is used in
                   which an enhanced vector quantizer (codebook) is
                   designed using Kohonen 's self organizing feature
                   maps(SOFM) neural networks. While designing the vector
                   quantizer, the training of the self organized network
                   is done by developing an enhanced training image and
                   also by selective training of input vectors. The
                   quality of reconstructed image is evaluated by using
                   image quality measures such as, Structural content,
                   Image fidelity, mean structural similarity index along
                   with conventionally used PSNR and entropy of the images
                   on RGB color space. The vector quantizer is design for
                   HSI color space also and performance is observe as well
                   as compared with RGB color space. The comparison with
                   existing JPEG with respect to storage and quality is
                   noted. This paper aims towards development of dedicated
                   hardware for compression and decompression using VLSI
                   techniques for which Vector quantization is most simple
                   and flexible approach.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/ICCIMA.2007.231},
  citeulike-article-id = {4030186},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/ICCIMA.2007.231},
  keywords       = {-source-ieee-hw-comp-2007-, image-coding},
  month          = {Dec.},
  posted-at      = {2009-02-10 15:43:37},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/ICCIMA.2007.231},
  year           = 2007
}

@Article{4621667,
  Author         = {Chybicki, A. and Moszynski, M. and Pocwiardowski, P. },
  Title          = {Applications of compression techniques for reducing
                   the size of multibeam sonar records},
  Journal        = {Information Technology, 2008. IT 2008. 1st
                   International Conference on},
  Pages          = {1--4},
  abstract       = {High efficiency of multibeam sonar system (MBS)
                   hardware due to operational requirements (i.e. high
                   frequency, high ping rate and high resolution of
                   collected data) results in very large volumes of
                   datasets stored on local hard drives of operatorpsilas
                   station. In this context, the process of archiving of
                   such warehouse of data collected in previous surveys
                   becomes crucial problem. The paper investigates various
                   lossy and lossless compression methods that can be
                   applied to multibeam sonar data to reduce the size of
                   acquired files without loosing relevant information.
                   The specific character of MBS data allows applying
                   various signal, image and video compression methods to
                   achieve better results than when using standard ones.
                   Various techniques of reordering the data were analyzed
                   to achieve best possible compression ratio.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/INFTECH.2008.4621667},
  citeulike-article-id = {4030234},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/INFTECH.2008.4621667},
  keywords       = {-source-ieee-hw-comp-2007-, image-coding},
  month          = {May},
  posted-at      = {2009-02-10 15:43:40},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/INFTECH.2008.4621667},
  year           = 2008
}

@Article{4092143,
  Author         = {Samanta, Rupak and Rabi},
  Title          = {An Enhanced CAM Architecture to Accelerate LZW
                   Compression Algorithm},
  Journal        = {VLSI Design, 2007. Held jointly with 6th International
                   Conference on Embedded Systems., 20th International
                   Conference on},
  Pages          = {824--829},
  abstract       = {This paper presents efficient hardware architecture
                   for Lempel-Ziv-Welch (LZW) data compression algorithm
                   that can perform both encoding and decoding operations
                   simultaneously using a CAM array. An enhanced CAM cell
                   design has been proposed to achieve search and twofold
                   store operations in single access during regular match
                   operations. The proposed architecture utilizes these
                   enhanced CAM cells to accelerate the implementation of
                   the LZW algorithm. The performance of the proposed
                   design is evaluated using the Corpus benchmarks, where
                   on an average a performance improvement of 53times is
                   achieved when compared to the software approach},
  bdsk-url-1     = {http://dx.doi.org/10.1109/VLSID.2007.34},
  citeulike-article-id = {4030184},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/VLSID.2007.34},
  keywords       = {-source-ieee-hw-comp-2007-; ian_todo},
  month          = {Jan.},
  posted-at      = {2009-02-10 15:43:37},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/VLSID.2007.34},
  year           = 2007
}

@Article{4437613,
  Author         = {Anis, E. and Nicolici, N.},
  Title          = {On using lossless compression of debug data in
                   embedded logic analysis},
  Journal        = {Test Conference, 2007. ITC 2007. IEEE International},
  Pages          = {1-10},
  abstract       = {The capacity of on-chip trace buffers employed for
                   embedded logic analysis limits the observation window
                   of a debug experiment. To increase the debug
                   observation window, we propose a novel architecture for
                   embedded logic analysis based on lossless compression.
                   The proposed architecture is particularly useful for
                   in-field debugging of custom circuits that have sources
                   of nondeterministic behavior such as asynchronous
                   interfaces. In order to measure the tradeoff between
                   the area overhead and the increase in the observation
                   window, we also introduce a new compression ratio
                   metric. We use this metric to quantify the performance
                   gain of three lossless compression algorithms suitable
                   for embedded logic analysis.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/TEST.2007.4437613},
  date-added     = {2009-02-11 01:49:20 +0100},
  date-modified  = {2009-02-11 01:49:46 +0100},
  doi            = {10.1109/TEST.2007.4437613},
  issn           = {1089-3539},
  keywords       = {ian_todo},
  month          = {Oct.},
  year           = 2007
}

@Article{1089893,
  Author         = {Schwarz, G.},
  Title          = {Buffer Design for Data Compression Systems},
  Journal        = {Communication Technology, IEEE Transactions on},
  Volume         = {16},
  Number         = {4},
  Pages          = {606-615},
  abstract       = {Design of the output buffer is one of the most
                   important tasks in implementing an adaptive data
                   compression system. Upon proper design of the buffer
                   including such parameters as size, inputoutput data
                   rates, and occupancy control, rests the overall
                   compression efficiency and error performance of the
                   system. The binomial distribution is used to derive an
                   exact model for a synchronous buffer. The Poisson
                   distribution, which provides a reasonable model for a
                   high-speed asynchronous buffer, is shown to yield an
                   error greater than 10 percent in required buffer length
                   for synchronous buffers. Design requirements such as
                   probabilities of overflow and underflow, buffer length,
                   and average buffer fill are derived as functions of
                   compression ratio Ï and the ratio of input-output
                   transmission ratesC. It is shown that the buffer
                   queuing behavior is a function of the ratiorho = C/phi,
                   as well asCand Ï independently. The derived results
                   indicate that restricting buffer overflow by increasing
                   the buffer size is inefficient. Control is suggested in
                   which the aperture of the compression algorithm is
                   changed to control the buffer fill. The design
                   requirements are determined for the zero-order
                   predictor and the first-order interpolator with two
                   degrees of freedom. Using the buffer equations derived
                   and the compression ratio-aperture relationship, the
                   design of a buffer is described. It is shown that
                   doubling the aperture with a resultant doubling in rms
                   error reduces the buffer probability of overflow by a
                   factor of 100.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/TCOM.1968.1089893},
  date-added     = {2009-02-11 01:38:15 +0100},
  date-modified  = {2009-02-11 01:48:41 +0100},
  doi            = {10.1109/TCOM.1968.1089893},
  issn           = {0018-9332},
  keywords       = {ian_todo},
  month          = {August },
  year           = 1968
}

@Article{257334,
  Author         = {Park, H. and Prasanna, V.K.},
  Title          = {Area efficient VLSI architectures for Huffman coding},
  Journal        = {Circuits and Systems II: Analog and Digital Signal
                   Processing, IEEE Transactions on},
  Volume         = {40},
  Number         = {9},
  Pages          = {568-575},
  abstract       = {In this paper, we present simple and area efficient
                   VLSI architectures for Huffman coding, an industrial
                   standard proposed by MPEG, JPEG, and others. We use a
                   memory of size O(n log n) bits to store a Huffman code
                   tree, where a is the number of symbols. This storage
                   scheme supports real-time encoding and decoding. In
                   addition, few simple arithmetic operations are
                   performed on the chip for encoding and decoding. Based
                   on our scheme, we show a design for I-bit symbols. The
                   proposed design requires 256×9 and 64×18-bit memory
                   modules to process 8-bit symbols. The chip occupies a
                   silicon area of 3.5×3.5 mm2 using 1.2 micron CMOSN
                   standard library cells. Compared with a known parallel
                   implementation which requires up to 65536 PE's, the
                   proposed architecture leads to a single PE design. It
                   requires significantly less area than the known single
                   PE design. Different Huffman codes can be stored by
                   changing the contents of the memory, without changing
                   the design},
  doi            = {10.1109/82.257334},
  file           = {file:///home/roman/workspace/zipchip/papers/Park93.pdf},
  issn           = {1057-7130},
  keywords       = {roman_todo},
  month          = {Sep},
  year           = 1993
}

@Article{4211769,
  Author         = {Zhou, Quming and Balakrishnan, K. J. },
  Title          = {Test Cost Reduction for SoC Using a Combined Approach
                   to Test Data Compression and Test Scheduling},
  Journal        = {Design, Automation \& Test in Europe Conference \&
                   Exhibition, 2007. DATE '07},
  Pages          = {1--6},
  abstract       = {A combined approach for implementing system level test
                   compression and core test scheduling to reduce SoC test
                   costs is proposed in this paper. A broadcast scan based
                   test compression algorithm for parallel testing of
                   cores with multiple scan chains is used to reduce the
                   test data of the SoC. Unlike other test compression
                   schemes, the proposed algorithm doesn't require
                   specialized test generation or fault simulation and is
                   applicable with intellectual property (IP) cores. The
                   core testing schedule with compression enabled is
                   decided using a generalized strip packing algorithm.
                   The hardware architecture to implement the proposed
                   scheme is very simple. By using the combined approach,
                   the total test data volume and test application time of
                   the SoC is reduced to a level comparable with the test
                   data volume and test application time of the largest
                   core in the SoC},
  bdsk-url-1     = {http://dx.doi.org/10.1109/DATE.2007.364564},
  citeulike-article-id = {4030174},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/DATE.2007.364564},
  keywords       = {-source-ieee-hw-comp-2007-; ian_todo},
  month          = {April},
  posted-at      = {2009-02-10 15:43:37},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/DATE.2007.364564},
  year           = 2007
}

@Article{4232974,
  Author         = {Rigler, S. and Bishop, W. and Kennings, A. },
  Title          = {FPGA-Based Lossless Data Compression using Huffman and
                   LZ77 Algorithms},
  Journal        = {Electrical and Computer Engineering, 2007. CCECE 2007.
                   Canadian Conference on},
  Pages          = {1235--1238},
  abstract       = {Lossless data compression algorithms are widely used
                   by data communication systems and data storage systems
                   to reduce the amount of data transferred and stored.
                   GZIP is a popular, patent-free compression program that
                   delivers good compression ratios. This paper presents
                   hardware implementations for the LZ77 encoders and
                   Huffman encoders that form the basis for a full
                   hardware implementation of a GZIP encoder. The designs
                   have been implemented as state machines in VHDL in such
                   a way that they are suitable for implementation using
                   either FPGA or ASIC technologies. Performance metrics
                   and resource utilization results obtained for a
                   prototype implementation running on an Altera DE2 board
                   are presented. Ultimately, the goal is to utilized the
                   LZ77 encoders and Huffman encoders described in this
                   paper to build a fully-functional, hardware design for
                   a GZIP encoder that could be used in data communication
                   systems and data storage systems to boost overall
                   system performance.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/CCECE.2007.315},
  citeulike-article-id = {4030276},
  comment        = {Dynamischer Huffman mit den zwei zus\"atzlichen
                   Implementationsregeln nach GZIP um immer einen vollen
                   Baum zu erhalten. Genereller Aufbau des Decoders
                   \"Uberlegungen zum Dictionary bei Huffman FPGA
                   Ressourcenaufwand },
  date-modified  = {2009-02-12 00:42:09 +0100},
  doi            = {http://dx.doi.org/10.1109/CCECE.2007.315},
  file           = {file:///home/roman/workspace/zipchip/papers/Rigler07.pdf},
  keywords       = {-source-ieee-hw-comp, fpga; ian\_todo; roman\_info},
  month          = {April},
  posted-at      = {2009-02-10 15:43:43},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/CCECE.2007.315},
  year           = 2007
}

@Article{4379793,
  Author         = {Dyer, M. and Nooshabadi, S. and Taubman, D. },
  Title          = {Analysis of Multiple Parallel Block Coding in JPEG2000},
  Journal        = {Image Processing, 2007. ICIP 2007. IEEE International
                   Conference on},
  Volume         = {5},
  abstract       = {We present the analysis and results for a system on a
                   chip (SoC) software/hardware codesign platform, for
                   parallel coding in JPEG2000 compression standard. We
                   show that there are optimum numbers of parallel block
                   coders and scheduling granularity per row of
                   codeblocks. The system was implemented on an Altera
                   NIOS II processor with flexible integrated peripheral.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/ICIP.2007.4379793},
  citeulike-article-id = {4030213},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/ICIP.2007.4379793},
  keywords       = {-source-ieee-hw-comp-2007-, image-coding},
  month          = {JanuaryJune February00July-Oct. JanuarySeptember},
  posted-at      = {2009-02-10 15:43:39},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/ICIP.2007.4379793},
  year           = 2007
}

@Article{4601874,
  Author         = {Ghosh, S. and Saha, A. },
  Title          = {Speed-area optimized FPGA implementation for Full
                   Search Block Matching},
  Journal        = {Computer Design, 2007. ICCD 2007. 25th International
                   Conference on},
  Pages          = {13--18},
  abstract       = {This paper presents an FPGA based hardware design for
                   full search block matching (FSBM) based motion
                   estimation (ME) in video compression. The significantly
                   higher resolution of HDTV based applications is
                   achieved by using FSBM based ME. The proposed
                   architecture uses a modification of the
                   sum-of-absolute-differences (SAD) computation in FSBM
                   such that the total number of additions/subtraction
                   operations is drastically reduced. This successfully
                   optimizes the conflicting design requirements of high
                   throughput and small silicon area. Comparison results
                   demonstrate the superior performance of our
                   architecture. Finally, the design of a reconfigurable
                   block matching hardware has been discussed.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/ICCD.2007.4601874},
  citeulike-article-id = {4030189},
  date-modified  = {2009-02-12 00:41:08 +0100},
  doi            = {http://dx.doi.org/10.1109/ICCD.2007.4601874},
  keywords       = {-source-ieee-hw-comp-2007-, fpga; ian_todo},
  month          = {Oct.},
  posted-at      = {2009-02-10 15:43:37},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/ICCD.2007.4601874},
  year           = 2007
}

@Article{4258343,
  Author         = {Biswas, S. and Das, S. R. and Hossain, A. },
  Title          = {VLSI Circuit Test Vector Compression Technique},
  Journal        = {Instrumentation and Measurement Technology Conference
                   Proceedings, 2007. IMTC 2007. IEEE},
  Pages          = {1--6},
  abstract       = {A new test vector compression method for VLSI circuit
                   testing is presented in this paper. The technique is
                   essentially software-based, where a program is loaded
                   into the on-chip processor memory along with the
                   compressed test data sets. To reduce the on-chip
                   storage area and testing time, the large volume of test
                   data is first compressed before downloading into the
                   on-chip processor. The proposed method utilizes a set
                   of adaptive coding techniques for achieving lossless
                   compression. The compression program need not be loaded
                   into the embedded processor, as only the decompression
                   of the test data is is necessary for application by the
                   automatic test equipment (ATE). The technique requires
                   minimal hardware overhead, while the on-chip processor
                   core can be reused for normal operation after testing.
                   The feasibility of the developed approach has been
                   demonstrated through extensive simulation experiments
                   on ISCAS 85 and ISCAS 89 benchmark circuits.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/IMTC.2007.379055},
  citeulike-article-id = {4030218},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/IMTC.2007.379055},
  keywords       = {-source-ieee-hw-comp-2007-},
  month          = {May},
  posted-at      = {2009-02-10 15:43:39},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/IMTC.2007.379055},
  year           = 2007
}

@Article{109154,
  Author         = {Lei, S.-M. and Sun, M.-T.},
  Title          = {An entropy coding system for digital HDTV applications},
  Journal        = {Circuits and Systems for Video Technology, IEEE
                   Transactions on},
  Volume         = {1},
  Number         = {1},
  Pages          = {147-155},
  abstract       = {Run-length coding (RLC) and variable-length coding
                   (VLC) are widely used techniques for lossless data
                   compression. A high-speed entropy coding system using
                   these two techniques is considered for digital high
                   definition television (HDTV) applications.
                   Traditionally, VLC decoding is implemented through a
                   tree-searching algorithm as the input bits are received
                   serially. For HDTV applications, it is very difficult
                   to implement a real-time VLC decoder of this kind due
                   to the very high data rate required. A parallel
                   structured VLC decoder which decodes each codeword in
                   one clock cycle regardless of its length is introduced.
                   The required clock rate of the decoder is thus lower,
                   and parallel processing architectures become easy to
                   adopt in the entropy coding system. The parallel
                   entropy coder and decoder are designed for
                   implementation in two experimental prototype chips
                   which are designed to encode and decode more than 52
                   million samples/s. Some related system issues, such as
                   the synchronization of variable-length codewords and
                   error concealment, are also discussed.},
  doi            = {10.1109/76.109154},
  file           = {file:///home/roman/workspace/zipchip/papers/Lei91.pdf},
  issn           = {1051-8215},
  keywords       = {roman\_todo},
  month          = {March},
  year           = 1991
}

@Article{4211900,
  Author         = {Bonny, T. and Henkel, J. },
  Title          = {Efficient Code Density Through Look-up Table
                   Compression},
  Journal        = {Design, Automation \& Test in Europe Conference \&
                   Exhibition, 2007. DATE '07},
  Pages          = {1--6},
  abstract       = {Code density is a major requirement in embedded system
                   design since it not only reduces the need for the
                   scarce resource memory but also implicitly improves
                   further important design parameters like power
                   consumption and performance. Within this paper we
                   introduce a novel and efficient hardware-supported
                   approach that belongs to the group of statistical
                   compression schemes as it is based on canonical Huffman
                   coding. In particular, our scheme is the first to also
                   compress the necessary Look-up Tables that can become
                   significant in size if the application is large and/or
                   high compression is desired. Our scheme optimizes the
                   number of generated look-up tables to improve the
                   compression ratio. In average, we achieve compression
                   ratios as low as 49\% (already including the overhead
                   of the lookup tables). Thereby, our scheme is entirely
                   orthogonal to approaches that take particularities of a
                   certain instruction set architecture into account. We
                   have conducted evaluations using a representative set
                   of applications and have applied it to three major
                   embedded processor architectures, namely ARM, MIPS and
                   PowerPC},
  bdsk-url-1     = {http://dx.doi.org/10.1109/DATE.2007.364390},
  citeulike-article-id = {4030216},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/DATE.2007.364390},
  keywords       = {-source-ieee-hw-comp-2007-, embedded-systems},
  month          = {April},
  posted-at      = {2009-02-10 15:43:39},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/DATE.2007.364390},
  year           = 2007
}

@Article{982798,
  Author         = {Lee, Tae Y. },
  Title          = {A new algorithm and its implementation for frame
                   recompression},
  Journal        = {Consumer Electronics, IEEE Transactions on},
  Volume         = {47},
  Number         = {4},
  Pages          = {849--854},
  abstract       = {An algorithm and its implementation to recompress
                   video frame data, which is to be stored into external
                   memory, are proposed, thereby enabling reduction of
                   memory requirements and bandwidth. To support random
                   access capability, frames are organized as 1Ã8-pixel
                   arrays, which are then compressed into 32-bit segments.
                   The compression ratio is fixed at 50\%. For both
                   compression efficiency and low complexity, the modified
                   Hadamard transform (MHT) and Golomb-Rice (GR) coding
                   are utilized. Objective and subjective video quality
                   assessments are performed. Efficient hardware designs
                   including Golomb-Rice encoding/decoding and
                   packing/unpacking are also proposed to reduce the total
                   compression/decompression latency cycles. The proposed
                   hardware designs are suitable for the integration with
                   MPEG-2 decoders thanks to fast operation and simple
                   interface logic},
  bdsk-url-1     = {http://dx.doi.org/10.1109/30.982798},
  citeulike-article-id = {4030285},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/30.982798},
  file           = {file:///home/roman/workspace/zipchip/papers/Lee01.pdf},
  keywords       = {-source-ieee-hw-comp; ian_todo},
  month          = {Nov},
  posted-at      = {2009-02-10 15:43:43},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/30.982798},
  year           = 2001
}

@Article{4415798,
  Author         = {Liang, Hua S. and Lei, Shi and Jun, Pang and Jun,
                   Zhang T. and Hui, Wang D. and Huan, Hou C. },
  Title          = {A hardware approach to reconfigurable lossless
                   real-time tracer},
  Journal        = {ASIC, 2007. ASICON '07. 7th International Conference
                   on},
  Pages          = {986--989},
  abstract       = {Address tracing at speed is essential to the debugging
                   or analyzing software programs in a complex system.
                   However, the generation rate and the size of real-time
                   program traces are so huge that real-time program
                   tracing is often infeasible without proper hardware
                   support. In this paper, we present a reconfigurable
                   lossless real-time tracer with three parts. A
                   synthesizable RTL code for the proposed hardware is
                   constructed and the tracer is integrated with a
                   microprocessor. Hardware cost and speed is analyzed and
                   typical programs are used to measure the compression
                   results. The results show that the reconfigurable
                   tracer is capable of real-time compression and
                   outputting the compressed data less than 1 bit per
                   cycle on average at 266 MHz with 172963 mum2 area in
                   0.18 mum CMOS process.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/ICASIC.2007.4415798},
  citeulike-article-id = {4030177},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/ICASIC.2007.4415798},
  keywords       = {-source-ieee-hw-comp-2007-},
  month          = {Oct.},
  posted-at      = {2009-02-10 15:43:37},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/ICASIC.2007.4415798},
  year           = 2007
}

@Article{1318862,
  Author         = {Sakanashi, H. and Iwata, M. and Higuchi, T. },
  Title          = {Evolvable hardware for lossless compression of very
                   high resolution bi-level images},
  Journal        = {Computers and Digital Techniques, IEE Proceedings -},
  Volume         = {151},
  Number         = {4},
  Pages          = {277--286},
  abstract       = {A compression method for image data with very high
                   resolution, and an evolvable hardware chip implementing
                   the process quickly are proposed. The current
                   international standard for bi-level image coding,
                   JBIG2, is modified by the proposed method to achieve
                   high compression ratios, where compression parameters
                   are optimised by search methods (primarily a genetic
                   algorithm that has been extended and specialised for
                   this problem). The results of computer simulations show
                   a 23\% improvement in compression ratios with the
                   proposed method compared to JBIG2, and demonstrate that
                   the optimisation process can be completed very quickly,
                   even with software execution. The experiment shows that
                   when the method is implemented by hardware with an
                   evolvable hardware chip, the processing is dramatically
                   faster than execution with software. Activities
                   concerning ISO standardisation to adopt part of the
                   technology used in this method to the JBIG2 standard
                   are also described.},
  bdsk-url-1     = {http://dx.doi.org/10.1049/ip-cdt:20040015},
  citeulike-article-id = {4030271},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1049/ip-cdt:20040015},
  keywords       = {-source-ieee-hw-comp, fpga},
  month          = {July},
  posted-at      = {2009-02-10 15:43:43},
  priority       = {2},
  url            = {http://dx.doi.org/10.1049/ip-cdt:20040015},
  year           = 2004
}

@Article{4541346,
  Author         = {Chen, Yi H. and Chuang, Tzu D. and Chen, Yu H. and
                   Tsai, Chen H. and Chen, Liang G. },
  Title          = {Frame-parallel design strategy for high definition
                   B-frame H.264/AVC encoder},
  Journal        = {Circuits and Systems, 2008. ISCAS 2008. IEEE
                   International Symposium on},
  Pages          = {29--32},
  abstract       = {High Definition (HD) H.264/AVC video compression is
                   the emerging necessity on nowadays home entertainment
                   environment and so on. However, Although B-frame coding
                   scheme provides better quality, only P-frame encoders
                   are presented due to too high complexity and memory
                   requirement. In this paper, a frame-parallel encoding
                   scheme based on B-frame's data independency is
                   proposed. It can largely reduce the system memory
                   bandwidth and improve the processing capability. Then,
                   the proposed IME and FME scheduling can further enhance
                   the hardware utilization for frame-parallel scheme.
                   Finally, a case study is given to show that the
                   proposed scheme can largely reduce 66\% system
                   bandwidth compared to direct implementation from
                   previous P-frame encoder.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/ISCAS.2008.4541346},
  citeulike-article-id = {4030235},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/ISCAS.2008.4541346},
  keywords       = {-source-ieee-hw-comp-2007-},
  month          = {May},
  posted-at      = {2009-02-10 15:43:40},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/ISCAS.2008.4541346},
  year           = 2008
}

@Misc{Jamro06,
  Author         = {Ernest Jamro, Maciej Wielgosz, Kazimierz Wiatr},
  Title          = {F{PGA} {I}mplementation of the {D}ynamic {H}uffman
                   {E}ncoder},
  HowPublished   = {Proceedings of IFAC Workshop on Programmable Devices
                   and Embedded Systems},
  abstract       = {This paper describes a novel architecture for adaptive
                   Huffman encoder. At first, the architecture for
                   quasi-static Huffman encoder is described. The main
                   part of this encoder is Look-Up Table (LUT). In order
                   to reduce the hardware requirements, the maximum length
                   of the encoded word is limited. This limitation reduces
                   the compression ratio insignificantly which is proved
                   in this paper. The change of the LUT contents in the
                   quasi-static Huffman coder allows relatively simple
                   adaptation of Huffman coder. The main problem is
                   calculation of the new LUT contents. Consequently
                   hardware-software co-design approach is adopted. The
                   most time-consuming operations: counting the input
                   words statistics (histogram) and sorting the resultant
                   histogram is implemented in hardware. The final
                   calculation of the new contents of the LUT and
                   controlling the whole system is achieved by the
                   soft-processor MicroBlaze. The design employs modular
                   design approach. Many independent modules compatible
                   with the OPB bus and Embedded Development Kit (EDK)
                   were designed. },
  comment        = {Lange max. behandelte Wortl\"ange vergr\"o\ss{}ern bei
                   kleinen Datenbl\"ocken den LUT erheblich. schlechtere
                   Werte ab: Symboll\"ange=8, Wortl\"ange=12
                   Ausf\"uhrungen zur Implementierung von Dynamic Huffman
                   Doppelte Frequenz f\"ur RAMS um delays zu minimieren,
                   naheliegend.},
  file           = {file:///home/roman/workspace/zipchip/papers/Jamro06.pdf},
  keywords       = {roman\_info},
  url            = {http://home.agh.edu.pl/~wielgosz/Huffman.pdf},
  year           = 2006
}

@Article{1351936,
  Author         = {Lee, Taeyeon and Park, Jaehong },
  Title          = {Design and implementation for static Huffman encoding
                   hardware with parallel shifting algorithm},
  Journal        = {Nuclear Science Symposium Conference Record, 2003 IEEE},
  Volume         = {2},
  abstract       = {This paper presents an implementation of static
                   Huffman encoding hardware for real-time lossless
                   compression in the ECAL of the CMS detector. The
                   construction of the Huffman encoding hardware shows an
                   implementation for optimizing its logic size. The
                   number of logic gates of the parallel shift operation
                   for the hardware is analyzed. Two kinds of
                   implementation methods of the parallel shift operation
                   are compared in aspect of logic size. The experiment
                   with the hardware on a simulated ECAL environment
                   covering 99.9999\% of original distribution shows
                   promising result with the simulation that the
                   compression rate was 4.0039 and the maximum length of
                   the stored data in the input buffer was 44.},
  citeulike-article-id = {4030278},
  date-modified  = {2009-02-12 01:23:39 +0100},
  keywords       = {-source-ieee-hw-comp, fpga, ian_todo},
  month          = {Oct.},
  posted-at      = {2009-02-10 15:43:43},
  priority       = {2},
  year           = 2003
}

@Article{4032075,
  Author         = {Ma, K. J. and Bartos, R. },
  Title          = {Analysis of Transport Optimization Techniques},
  Journal        = {Web Services, 2006. ICWS '06. International Conference
                   on},
  Pages          = {611--620},
  abstract       = {The popularity of Web-based transactions and the need
                   for more sophisticated content distribution methods has
                   helped to fuel the rapid growth of Web Service
                   adoption, specifically, HTTP-bound Web services. Secure
                   and efficient content delivery has long been a
                   requirement of traditional Web-based distribution
                   schemes, and existing Web infrastructure provides
                   numerous options for securing and optimizing HTTP. Two
                   exemplary technologies are SSL/TLS and HTTP
                   compression. While efforts to solidify the more
                   granular WS-Security standards are ongoing, and methods
                   for XML message compression schemes continue to be
                   investigated, HTTP provides an interim solution,
                   supporting transactional security and message
                   compression. The SSL/TLS and HTTP compression
                   technologies have become commoditized and pervasive.
                   And with the trend in content delivery toward hardware
                   offload for these functions, modern data centers have
                   begun to raise the bar for performance. In this paper,
                   we examine three different paradigms for implementing
                   SSL/TLS and HTTP compression: software-based
                   functionality, server-resident hardware accelerators,
                   and centralized network-resident hardware accelerators.
                   We discuss the trade-offs between the two different
                   offload techniques (i.e., PCI accelerator vs. network
                   proxy) and explore their relationship to the current
                   performance activities, in the field of Web services.
                   In analyzing the results for SSL/TLS offload, and the
                   effects of compression, in conjunction with SSL/TLS, we
                   draw parallels with the efforts of WS-Security and XML
                   message compression. Although optimizations for
                   software-based cryptography continues to advance, the
                   potential for hardware-based acceleration should not be
                   overlooked. We discuss our results and address
                   deployment scenarios for optimizing Web-based
                   transactions, and the future optimization of Web
                   Service transactions},
  bdsk-url-1     = {http://dx.doi.org/10.1109/ICWS.2006.30},
  citeulike-article-id = {4030275},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/ICWS.2006.30},
  keywords       = {-source-ieee-hw-comp; ian_todo},
  month          = {Sept.},
  posted-at      = {2009-02-10 15:43:43},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/ICWS.2006.30},
  year           = 2006
}

@Article{4061029,
  Author         = {Shoushun, Chen and Bermak, Amine and Yan, Wang and
                   Martinez, Dominique },
  Title          = {Adaptive-Quantization Digital Image Sensor for
                   Low-Power Image Compression},
  Journal        = {Circuits and Systems I: Regular Papers, IEEE
                   Transactions on},
  Volume         = {54},
  Number         = {1},
  Pages          = {13--25},
  abstract       = {The recent emergence of new applications in the area
                   of wireless video sensor network and ultra-low-power
                   biomedical applications (such as the wireless camera
                   pill) have created new design challenges and frontiers
                   requiring extensive research work. In such
                   applications, it is often required to capture a large
                   amount of data and process them in real time while the
                   hardware is constrained to take very little physical
                   space and to consume very little power. This is only
                   possible using custom single-chip solutions integrating
                   image sensor and hardware-friendly image compression
                   algorithms. This paper proposes an adaptive
                   quantization scheme based on boundary adaptation
                   procedure followed by an online quadrant tree
                   decomposition processing enabling low power and yet
                   robust and compact image compression processor
                   integrated together with a digital CMOS image sensor.
                   The image sensor chip has been implemented using
                   0.35-mum CMOS technology and operates at 3.3 V.
                   Simulation and experimental results show compression
                   figures corresponding to 0.6-0.8 bit per pixel, while
                   maintaining reasonable peak signal-to-noise ratio
                   levels and very low operating power consumption. In
                   addition, the proposed compression processor is
                   expected to benefit significantly from higher
                   resolution and Megapixels CMOS imaging technology},
  bdsk-url-1     = {http://dx.doi.org/10.1109/TCSI.2006.887460},
  citeulike-article-id = {4030209},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/TCSI.2006.887460},
  keywords       = {-source-ieee-hw-comp-2007-, image-coding},
  month          = {Jan.},
  posted-at      = {2009-02-10 15:43:38},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/TCSI.2006.887460},
  year           = 2007
}

@Article{4584270,
  Author         = {Yanez, Nunez J. L. and Chen, Xiaolin and Canagarajah,
                   N. and Vitulli, R. },
  Title          = {Statistical Lossless Compression of Space Imagery and
                   General Data in a Reconfigurable Architecture},
  Journal        = {Adaptive Hardware and Systems, 2008. AHS '08. NASA/ESA
                   Conference on},
  Pages          = {172--177},
  abstract       = {This paper investigates an universal algorithm and
                   hardware architecture for context-based statistical
                   lossless compression of multiple types of data using
                   FPGA (field programmable gate array) devices which
                   support partial and dynamic reconfiguration. The
                   proposed system enables optimal modeling strategies for
                   each source type whilst entropy coding of the modeling
                   output is performed using a statically configured
                   arithmetic coding engine. Spacecraft communications
                   typically involve large amounts of information captured
                   from different sensors that must be transmitted without
                   any loss. The statistical redundancies present in this
                   data can be removed efficiently using the proposed
                   reconfigurable compression technology.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/AHS.2008.9},
  citeulike-article-id = {4030245},
  date-modified  = {2009-02-12 00:39:06 +0100},
  doi            = {http://dx.doi.org/10.1109/AHS.2008.9},
  keywords       = {-source-ieee-hw-comp-2007-, fpga; ian_todo},
  month          = {June},
  posted-at      = {2009-02-10 15:43:40},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/AHS.2008.9},
  year           = 2008
}

@InProceedings{Larsson96a,
  Author         = {Larsson, Jesper N. },
  Title          = {Extended Application of Suffix Trees to Data
                   Compression},
  BookTitle      = {Proceedings of the IEEE Data Compression Conference},
  Pages          = {190--199},
  bdsk-url-1     = {http://www.larsson.dogma.net/dccpaper.pdf},
  citeulike-article-id = {4026353},
  date-modified  = {2009-02-12 01:23:39 +0100},
  keywords       = {lz; ian_todo},
  month          = {March--April},
  posted-at      = {2009-02-09 19:53:16},
  priority       = {2},
  url            = {http://www.larsson.dogma.net/dccpaper.pdf},
  year           = 1996
}

@Article{4449526,
  Author         = {Voyiatzis, I. and Antonopoulou, H. },
  Title          = {Decoder-based Decompression for test sets containing
                   don't cares},
  Journal        = {Design \& Technology of Integrated Systems in
                   Nanoscale Era, 2007. DTIS. International Conference on},
  Pages          = {229--232},
  abstract       = {A novel scheme for decompressing test sets containing
                   don't care values is presented. Compared to traditional
                   scan-based schemes, the proposed schemes provides for
                   lower test application time, lower hardware overhead
                   and lower power consumption during the application of
                   the test set.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/DTIS.2007.4449526},
  citeulike-article-id = {4030221},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/DTIS.2007.4449526},
  keywords       = {-source-ieee-hw-comp-2007-},
  month          = {Sept.},
  posted-at      = {2009-02-10 15:43:39},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/DTIS.2007.4449526},
  year           = 2007
}

@Article{1652153,
  Author         = {Ali, N. and Lauria, M.},
  Title          = {Improving the Performance of Remote I/O Using
                   Asynchronous Primitives},
  Journal        = {High Performance Distributed Computing, 2006 15th IEEE
                   International Symposium on},
  Pages          = {218-228},
  abstract       = {An increasing number of scientific applications need
                   efficient access to large datasets held at remote
                   storage facilities. However, despite the availability
                   of high-speed Internet backbones, the performance
                   penalty of remote I/O is still relatively high compared
                   to local I/O. In this paper we describe different ways
                   in which asynchronous primitives can be used to improve
                   the performance of remote I/O in the grid environment.
                   We have implemented and evaluated three optimization
                   techniques using asynchronous primitives. These
                   primitives have been integrated into SEMPLAR, a
                   high-performance, remote I/O library based on the SDSC
                   storage resource broker. Based on measurements of
                   representative high-performance applications running on
                   three different clusters, we show that different
                   optimization techniques work best for each specific
                   combination of application and platform
                   characteristics. We achieved over 90% overlap between
                   the computation and I/O phase of two applications by
                   using an asynchronous version of remote I/O primitives.
                   We were able to increase the average read and write
                   bandwidth of the ROMIO perf benchmark by 96% and 43%
                   respectively by moving data concurrently over multiple
                   remote connections. Finally, we experienced an
                   improvement of up to 84% in the average write bandwidth
                   when using asynchronous, on-the-fly data compression},
  bdsk-url-1     = {http://dx.doi.org/10.1109/HPDC.2006.1652153},
  date-added     = {2009-02-11 01:49:20 +0100},
  date-modified  = {2009-02-11 01:49:51 +0100},
  doi            = {10.1109/HPDC.2006.1652153},
  issn           = {1082-8907},
  keywords       = {ian_todo},
  month          = {0-0 },
  year           = 2006
}

@Article{Huang00a,
  Author         = {Huang, Wei J. and Mccluskey, E. J. },
  Title          = {Transient errors and rollback recovery in LZ
                   compression},
  Journal        = {Dependable Computing, 2000. Proceedings. 2000 Pacific
                   Rim International Symposium on},
  Pages          = {128--135},
  abstract       = {This paper analyzes the data integrity of one of the
                   most widely used lossless data compression techniques,
                   Lempel-Ziv (LZ) compression. In this algorithm, because
                   the data reconstruction from compressed codewords
                   relies on previously decoded results, a transient error
                   during compression may propagate to the decoder and
                   cause a significant corruption in the reconstructed
                   data. To recover the system from transient faults, we
                   designed two rollback error recovery schemes for the LZ
                   compression hardware, the ``reload-retry'' and
                   ``direct-retry'' schemes. Statistical analyses show
                   that the ``reload-retry'' scheme can recover the LZ
                   compression process from transient faults in one
                   dictionary reload cycle with a small amount of hardware
                   redundancy. The ``direct-retry'' scheme can recover
                   normal operations with a shorter latency but with a
                   small degradation in the compression ratio},
  bdsk-url-1     = {http://dx.doi.org/10.1109/PRDC.2000.897295},
  citeulike-article-id = {4030296},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/PRDC.2000.897295},
  keywords       = {-source-ieee-hw-lz, hw, lz; ian_todo},
  posted-at      = {2009-02-10 15:43:44},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/PRDC.2000.897295},
  year           = 2000
}

@Article{4161494,
  Author         = {Kizhner, S. and Patel, U. D. and Vootukuru, M. },
  Title          = {On Representative Spaceflight Instrument and
                   Associated Instrument Sensor Web Framework},
  Journal        = {Aerospace Conference, 2007 IEEE},
  Pages          = {1--10},
  abstract       = {Sensor Web-based adaptation and sharing of space
                   flight mission resources, including those of the
                   Space-Ground and Control-User communication segments,
                   could greatly benefit from utilization of heritage
                   Internet Protocols and devices applied for Spaceflight
                   (SpacelP). This had been successfully demonstrated by a
                   few recent spaceflight experiments. However, while
                   terrestrial applications of Internet protocols are well
                   developed and understood (mostly due to billions of
                   dollars in investments by the military and industry),
                   the spaceflight application of Internet protocols is
                   still in its infancy. Progress in the developments of
                   SpacelP-enabled instrument components will largely
                   determine the SpacelP utilization of those investments
                   and acceptance in years to come. Likewise SpacelP, the
                   development of commercial real-time and instrument
                   co-located computational resources, data compression
                   and storage, can be enabled on-board a spacecraft
                   instrument and, in turn, support a powerful application
                   to Sensor Web-based design of a spaceflight instrument.
                   These are presently only co-located with the spacecraft
                   Command and Data Handling System (C\&DH). Sensor
                   Web-enabled re-configuration and adaptation of
                   structures for hardware resources and information
                   systems on instrument level will commence application
                   of Field Programmable Gate Arrays (FPGA) and other
                   aerospace programmable logic devices for what this
                   technology was intended. These are a few obvious
                   potential benefits of Sensor Web technologies for
                   spaceflight applications on instrument level. However,
                   they are still waiting to be explored. This is because
                   there is a need for a new approach to spaceflight
                   instrumentation in order to make these mature sensor
                   web technologies applicable for spaceflight. In this
                   paper we present an approach in developing related and
                   enabling spaceflight instrument-level technologies
                   based on the new concept of a representative Instrument
                   Sensor Web (ISW). This concept widens the scope of-
                   heritage sensor webs and facilitates the application of
                   sensor web technologies to complex representative
                   instruments onboard future spacecrafts.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/AERO.2007.353084},
  citeulike-article-id = {4030224},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/AERO.2007.353084},
  keywords       = {-source-ieee-hw-comp-2007-, fpga},
  month          = {March},
  posted-at      = {2009-02-10 15:43:39},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/AERO.2007.353084},
  year           = 2007
}

@Article{816214,
  Author         = {Redinbo, G. B. },
  Title          = {Hardware fault tolerance in arithmetic coding for data
                   compression},
  Journal        = {Dependable Computing, 1999. Proceedings. 1999 Pacific
                   Rim International Symposium on},
  Pages          = {70--77},
  abstract       = {New fault tolerance techniques are presented for
                   protecting a lossless compression algorithm, arithmetic
                   coding, whose recursive nature makes it vulnerable to
                   temporary hardware failures. The fundamental arithmetic
                   operations are protected by low-cost residue codes,
                   employing fault tolerance in multiplications and
                   additions. Additional fault-tolerant design techniques
                   are developed to protect other critical steps such as
                   normalization and rounding, bit stuffing and index
                   selection. For example, the decoding step that selects
                   the next symbol is checked by comparing local values
                   with estimates already calculated in other parts of the
                   decoding structure. Bit stuffing, a procedure for
                   limiting very long carry propagations, is checked
                   through modified residue values, whereas normalization
                   and rounding after multiplication are protected by
                   efficiently modifying the multiplier to produce residue
                   segments},
  bdsk-url-1     = {http://dx.doi.org/10.1109/PRDC.1999.816214},
  citeulike-article-id = {4030280},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/PRDC.1999.816214},
  keywords       = {-source-ieee-hw-comp; ian_todo},
  posted-at      = {2009-02-10 15:43:43},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/PRDC.1999.816214},
  year           = 1999
}

@Article{Hwang01a,
  Author         = {Hwang, Shih A. and Wu, Cheng W. },
  Title          = {Unified VLSI systolic array design for LZ data
                   compression},
  Journal        = {Very Large Scale Integration (VLSI) Systems, IEEE
                   Transactions on},
  Volume         = {9},
  Number         = {4},
  Pages          = {489--499},
  abstract       = {Hardware implementation of data compression algorithms
                   is receiving increasing attention due to exponentially
                   expanding network traffic and digital data storage
                   usage. In this paper, we propose several serial
                   one-dimensional and parallel two-dimensional
                   systolic-arrays for Lempel-Ziv data compression. A VLSI
                   chip implementing our optimal linear array is
                   fabricated and tested. The proposed array architecture
                   is scalable. Also, multiple chips (linear arrays) can
                   be connected in parallel to implement the parallel
                   array structure and provide a proportional speedup},
  bdsk-url-1     = {http://dx.doi.org/10.1109/92.931226},
  citeulike-article-id = {4030297},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/92.931226},
  keywords       = {-source-ieee-hw-lz, hw, lz, ian_todo},
  month          = {Aug},
  posted-at      = {2009-02-10 15:43:44},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/92.931226},
  year           = 2001
}

@Article{4545469,
  Author         = {Chen, Xiaolin and Canagarajah, Nishan and Jose and
                   Vitulli, Raffaele },
  Title          = {Hardware architecture for lossless image compression
                   based on context-based modeling and arithmetic coding},
  Journal        = {SOC Conference, 2007 IEEE International},
  Pages          = {251--254},
  abstract       = {In this paper we present a novel hardware architecture
                   for context-based statistical lossless image
                   compression, as part of a dynamically reconfigurable
                   architecture for universal lossless compression. A
                   gradient-adjusted prediction and context modeling
                   algorithm is adapted to a pipelined scheme for low
                   complexity and high throughput. Our proposed system
                   improves image compression ratio while keeping low
                   hardware complexity. This system is designed for a
                   Xilinx Virtex4 FPGA core and optimized to achieve a 123
                   MHz clock frequency for real-time processing.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/SOCC.2007.4545469},
  citeulike-article-id = {4030267},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/SOCC.2007.4545469},
  keywords       = {-source-ieee-hw-comp; ian_todo},
  month          = {Sept.},
  posted-at      = {2009-02-10 15:43:42},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/SOCC.2007.4545469},
  year           = 2007
}

@Article{4579438,
  Author         = {Bhuyan, M. S. and Amin, N. and Madesa, M. A. H. and
                   Islam, M. S. },
  Title          = {VLSI implementation of Inverse Discrete Wavelet
                   Transform for JPEG 2000},
  Journal        = {Computer and information technology, 2008. iccit 2007.
                   10th international conference on},
  Pages          = {1--5},
  abstract       = {This paper presents hardware design flow of the
                   inverse discrete wavelet transform (IDWT) core which is
                   the second-most computationally intensive block in JPEG
                   2000 image compression standard. Lifting scheme (LS) is
                   implemented in designing the IDWT hardwire module that
                   reduces the number of execution steps involved in
                   computation to almost one-half of those needed with a
                   conventional convolution approach. In addition, the LS
                   is amenable to ldquoin-placerdquo computation, so that
                   the IDWT can be implemented in low memory systems. The
                   IDWT module does not comprise any hardware multiplier
                   unit and therefore suitable for development of high
                   performance image processor. The IDWT module has been
                   developed in VHDL using Quartus II from Altera. The
                   VHDL model is validated through simulation using
                   ModelSim-Altera. Simulation results show the IDWT
                   module can perform three levels inverse transform on a
                   256times256 forward transformed image in 8.7 ms.
                   Latency of the system is calculated 50 ns and the power
                   dissipation by the device is 662 mW. The IDWT module
                   consumes just 57 combinational ALUTs and 60 logic
                   registers of a Stratix II device, and runs at 300 MHz
                   clock frequency, reaches a speed performance suitable
                   for several real-time applications. Throughput in terms
                   of input coefficients processed per second of the IDWT
                   core is 7.13Msamples. The motivation in designing is to
                   reduce its complexity, enhance its performance and to
                   make it suitable development on a reconfigurable FPGA
                   based platform for VLSI implementation.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/ICCITECHN.2007.4579438},
  citeulike-article-id = {4030172},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/ICCITECHN.2007.4579438},
  keywords       = {-source-ieee-hw-comp-2007-},
  month          = {Dec.},
  posted-at      = {2009-02-10 15:43:37},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/ICCITECHN.2007.4579438},
  year           = 2007
}

@Article{4556845,
  Author         = {Jedidi, A. and Rejeb, B. and Abid, M. },
  Title          = {Design of Fractal Image Compression on SOC},
  Journal        = {Symposium on VLSI, 2008. ISVLSI '08. IEEE Computer
                   Society Annual},
  Pages          = {479--482},
  abstract       = {The technological revolution during the last years has
                   carried out a great evolution especially in the
                   multimedia domain. Nowadays, almost everyone benefits
                   from various video applications such as TV broadcasting
                   and video conferencing. This progress involves
                   particularly an increase in the capacity of digital
                   data transmission. As a result, data compression became
                   increasingly significant for storage and transmission.
                   In this paper, we present an algorithm for fractal
                   image compression based on SOC in real time. The
                   algorithm consists of a hardware and software part. The
                   hardware part supports an expensive calculation;
                   therefore it is conceived on RTL level. The coding
                   algorithm was implemented on the Altera chart
                   STRATIX-I. The functional blocks were implemented with
                   clock rate of 410 MHz with a maximum flow data input of
                   13.2 Gbit/s.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/ISVLSI.2008.26},
  citeulike-article-id = {4030246},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/ISVLSI.2008.26},
  keywords       = {-source-ieee-hw-comp-2007-, image-coding},
  month          = {April},
  posted-at      = {2009-02-10 15:43:40},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/ISVLSI.2008.26},
  year           = 2008
}

@Article{4564580,
  Author         = {Huynh, Thuan and Cao, Thuong and Tran, Diem and
                   Nguyen, Phuong and Dinh, Anh },
  Title          = {Designing a hardware accelerator for vector
                   quantization as a component of a SoPC},
  Journal        = {Electrical and Computer Engineering, 2008. CCECE 2008.
                   Canadian Conference on},
  Pages          = {000479--000484},
  abstract       = {A flexible accelerator hardware for full-search vector
                   quantization (VQ) has been developed as a component for
                   a system on a programmable chip (SOPC) to use in
                   real-time image compression and recognition
                   applications. In this system, the number of elements
                   for each codeword and the number of codewords in the
                   system can be changed easily for different applications
                   with the use of an embedded CPU. The architecture
                   allows to use look up tables (LUTs), single-instruction
                   multiple-data (SIMD) and two-stage pipeline
                   architecture. This leads to high speed operation which
                   is quite suitable for real-time applications. In
                   addition, an improved method for image compression
                   using VQ was developed using this architecture. The
                   technique was implemented and successfully tested in an
                   Altera Stratix II DSP FPGA Development Kit.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/CCECE.2008.4564580},
  citeulike-article-id = {4030266},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/CCECE.2008.4564580},
  keywords       = {-source-ieee-hw-comp, image-coding},
  month          = {May},
  posted-at      = {2009-02-10 15:43:42},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/CCECE.2008.4564580},
  year           = 2008
}

@Article{519578,
  Author         = {Mcneill, D. K. and Card, H. C. },
  Title          = {Data compression at low power using soft competitive
                   learning},
  Journal        = {Communications, Computers, and Signal Processing,
                   1995. Proceedings. IEEE Pacific Rim Conference on},
  Pages          = {497--500},
  abstract       = {This paper examines a variety of issues relating to
                   the analog hardware implementation of the soft
                   competitive neural learning algorithm and its
                   suitability for use in data compression applications.
                   Specifically, we investigate the impact of realizing
                   the theoretical learning algorithm in imperfect analog
                   structures constructed in a traditional CMOS
                   fabrication process. Empirical measurements of
                   previously fabricated neural circuit elements are used
                   to produce suitable hardware models which accurately
                   characterize the fabrication process and a typical
                   operating environment. These models are used to
                   evaluate the tolerance of the soft competitive learning
                   algorithm to expected system variations including
                   various noise and device effects. The analog neural
                   circuits make extensive use of a CMOS implementation of
                   the Gilbert multiplier which is the primary
                   computational element for our learning computations. We
                   have found, through the aid of simulations based on
                   these hardware models, that the circuit effects are not
                   significant if zero-thresholding is used to compensate
                   for multiplier zero-crossing offsets. These results
                   indicate that this algorithm is very robust in the
                   presence of moderate circuit limitations. As a result,
                   such circuits would be well suited for applications
                   requiring data compression with low power consumption,
                   as might be encountered in the production of compact
                   consumer products for portable computing},
  bdsk-url-1     = {http://dx.doi.org/10.1109/PACRIM.1995.519578},
  citeulike-article-id = {4030257},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/PACRIM.1995.519578},
  keywords       = {-source-ieee-hw-comp; ian_todo},
  month          = {May},
  posted-at      = {2009-02-10 15:43:42},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/PACRIM.1995.519578},
  year           = 1995
}

@Article{4379408,
  Author         = {Vasa, L. and Skala, V. },
  Title          = {CODDYAC: Connectivity Driven Dynamic Mesh Compression},
  Journal        = {3DTV Conference, 2007},
  Pages          = {1--4},
  abstract       = {Compression of 3D mesh animations is a topic that has
                   received increased attention in recent years, due to
                   increasing capabilities of modern processing and
                   displaying hardware. In this paper we present an
                   improved approach based on known techniques, such as
                   principal component analysis (PCA) and EdgeBreaker,
                   which allows efficient encoding of highly detailed
                   dynamic meshes, exploiting both spatial and temporal
                   coherence. We present the results of our method
                   compared with similar approaches described in
                   literature, showing that using our approach we can
                   achieve better performance in terms of rate/distortion
                   ratio.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/3DTV.2007.4379408},
  citeulike-article-id = {4030223},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/3DTV.2007.4379408},
  keywords       = {-source-ieee-hw-comp-2007-, image-coding; ian_todo},
  month          = {May},
  posted-at      = {2009-02-10 15:43:39},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/3DTV.2007.4379408},
  year           = 2007
}

@Article{4525599,
  Author         = {Wang, You C. and Hsieh, Yao Y. and Tseng, Yu C. },
  Title          = {Compression and Storage Schemes in a Sensor Network
                   with Spatial and Temporal Coding Techniques},
  Journal        = {Vehicular Technology Conference, 2008. VTC Spring
                   2008. IEEE},
  Pages          = {148--152},
  abstract       = {Wireless sensor networks provide a convenient manner
                   to monitor the physical environments. How to extend the
                   network lifetime by reducing the amount of message
                   transmissions is a critical issue. In this paper, we
                   propose a multiresolution compression and storage (MCS)
                   framework to compress and preserve sensing data in a
                   wireless sensor network. Our MCS framework adopts
                   spatial and temporal compression schemes to reduce the
                   amount of message transmissions, so the network
                   lifetime can be prolonged and the network congestion
                   can be alleviated. In addition, we also develop a
                   storage mechanism to maintain sensing data in sensor
                   nodes, so that users can query more detailed data when
                   necessary. Our proposed methods consider the hardware
                   limitations of sensor nodes. We also implement a
                   prototyping system on the MICAz Mote platform.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/VETECS.2008.43},
  citeulike-article-id = {4030248},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/VETECS.2008.43},
  keywords       = {-source-ieee-hw-comp-2007-},
  month          = {May},
  posted-at      = {2009-02-10 15:43:41},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/VETECS.2008.43},
  year           = 2008
}

@Article{4014456,
  Author         = {Qian, Shen E. and Bergeron, M. and Cunningham, I. and
                   Gagnon, L. and Hollinger, A. },
  Title          = {Near lossless data compression onboard a hyperspectral
                   satellite},
  Journal        = {Aerospace and Electronic Systems, IEEE Transactions on},
  Volume         = {42},
  Number         = {3},
  Pages          = {851--866},
  abstract       = {To deal with the large volume of data produced by
                   hyperspectral sensors, the Canadian Space Agency (CSA)
                   has developed and patented two near lossless data
                   compression algorithms for use onboard a hyperspectral
                   satellite: successive approximation multi-stage vector
                   quantization (SAMVQ) and hierarchical self-organizing
                   cluster vector quantization (HSOCVQ). This paper
                   describes the two compression algorithms and
                   demonstrates their near lossless feature. The
                   compression error introduced by the two compression
                   algorithms was compared with the intrinsic noise of the
                   original data that is caused by the instrument noise
                   and other noise sources such as calibration and
                   atmospheric correction errors. The experimental results
                   showed that the compression error was not larger than
                   the intrinsic noise of the original data when a test
                   data set was compressed at a compression ratio of 20:1.
                   The overall noise in the reconstructed data that
                   contains both the intrinsic noise and the compression
                   error is even smaller than the intrinsic noise when the
                   data is compressed using SAMVQ. A multi-disciplinary
                   user acceptability study has been carried out in order
                   to evaluate the impact of the two compression
                   algorithms on hyperspectral data applications. This
                   paper briefly summarizes the evaluation results of the
                   user acceptability study. A prototype hardware
                   compressor that implements the two compression
                   algorithms has been built using field programmable gate
                   arrays (FPGAs) and benchmarked. The compression ratio
                   and fidelity achieved by the hardware compressor are
                   similar to those obtained by software simulation},
  bdsk-url-1     = {http://dx.doi.org/10.1109/TAES.2006.248183},
  citeulike-article-id = {4030282},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/TAES.2006.248183},
  keywords       = {-source-ieee-hw-comp, fpga, image-coding},
  month          = {July},
  posted-at      = {2009-02-10 15:43:43},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/TAES.2006.248183},
  year           = 2006
}

@Article{Kao07a,
  Author         = {Kao, Chung F. and Huang, Shyh M. and Huang, I. J. },
  Title          = {A Hardware Approach to Real-Time Program Trace
                   Compression for Embedded Processors},
  Journal        = {Circuits and Systems I: Regular Papers, IEEE
                   Transactions on},
  Volume         = {54},
  Number         = {3},
  Pages          = {530--543},
  abstract       = {Collecting the program execution traces at full speed
                   is essential to the analysis and debugging of real-time
                   software behavior of a complex system. However, the
                   generation rate and the size of real-time program
                   traces are so huge such that real-time program tracing
                   is often infeasible without proper hardware support.
                   This paper presents a hardware approach to compress
                   program execution traces in real time in order to
                   reduce the trace size. The approach consists of three
                   modularized phases: 1) branch/target filtering; 2)
                   branch/target address encoding; 3) Lempel-Ziv
                   (LZ)-based data compression. A synthesizable RTL code
                   for the proposed hardware is constructed to analyze the
                   hardware cost and speed and typical multimedia
                   benchmarks are used to measure the compression results.
                   The results show that our hardware is capable of
                   real-time compression and achieving compression ratio
                   of 454:1, far better than 5:1 achieved by typical
                   existing hardware approaches. Furthermore, our
                   modularized approach makes it possible to trade off
                   between the hardware cost (typically from 1 to 50K
                   gates) and the achievable compression ratio (typically
                   from 5:1 to 454:1)},
  bdsk-url-1     = {http://dx.doi.org/10.1109/TCSI.2006.887613},
  citeulike-article-id = {4030300},
  date-modified  = {2009-02-10 22:30:30 +0100},
  doi            = {http://dx.doi.org/10.1109/TCSI.2006.887613},
  keywords       = {-source-ieee-hw-lz, embedded-systems, hw, lz; ian_todo},
  month          = {March},
  posted-at      = {2009-02-10 15:43:45},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/TCSI.2006.887613},
  year           = 2007
}

@Article{Jung95a,
  Author         = {Jung, Bongjin and Burleson, W. P. },
  Title          = {Real-time VLSI compression for high-speed wireless
                   local area networks},
  Journal        = {Data Compression Conference, 1995. DCC '95.
                   Proceedings},
  abstract       = {Summary form only presented; substantially as follows.
                   Presents a new compact, power-efficient, and scalable
                   VLSI array for the first Lempel-Ziv (LZ) algorithm to
                   be used in high-speed wireless data communication
                   systems. Lossless data compression can be used to
                   inexpensively halve the amount of data to be
                   transmitted, thus improving the effective bandwidth of
                   the communication channel and in turn, the overall
                   network performance. For wireless networks, the data
                   rate and latency requirement are appropriate for a
                   dedicated VLSI implementation of LZ compression. The
                   nature of wireless networks requires that any
                   additional VLSI hardware also be small, low-power and
                   inexpensive. The architecture uses a novel custom
                   systolic array and a simple dictionary FIFO which is
                   implemented using conventional SRAM. The architecture
                   consists of M simple processing elements where M is the
                   maximum length of the string to be replaced with a
                   codeword, which for practical LAN applications, can
                   range from 16 to 32. The systolic cell has been
                   optimized to remove any superfluous state information
                   or logic, thus making it completely dedicated to the
                   task of LZ compression. A prototype chip has been
                   implemented using 2 Î¼s CMOS technology. Using M=32,
                   and assuming a 2:1 compression ratio, the system can
                   process approximately 90 Mbps with a 100 MHz clock rate},
  bdsk-url-1     = {http://dx.doi.org/10.1109/DCC.1995.515541},
  citeulike-article-id = {4030304},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/DCC.1995.515541},
  keywords       = {-source-ieee-hw-lz, hw, lz, ian_todo},
  month          = {Mar},
  posted-at      = {2009-02-10 15:43:45},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/DCC.1995.515541},
  year           = 1995
}

@Article{305931,
  Author         = {Nakano, Y. and Yahagi, H. and Okada, Y. and Yoshida,
                   S.},
  Title          = {Highly efficient universal coding with classifying to
                   subdictionaries for text compression},
  Journal        = {Data Compression Conference, 1994. DCC '94.
                   Proceedings},
  Volume         = {},
  Number         = {},
  Pages          = {234-243},
  abstract       = {Describes a practical, locally adaptive data
                   compression algorithm of the LZ78 class. According to
                   the Lempel-Ziv incremental parsing rule, the boundary
                   of a string is not related to the statistical history
                   modeled by finite-state sources. The authors have
                   already reported an algorithm classifying to
                   subdictionaries (CSD), which uses multiple
                   subdictionaries and conditions the current string by
                   using the previous one to obtain a higher compression
                   ratio for image compression. They present a practical
                   implementation of this method for any kind of data, and
                   show that CSD was more efficient than LZC when the UNIX
                   facility for compression. The compression performance
                   of CSD was about 10% better than the LZC with the
                   practical dictionary size, an 8K-entry dictionary when
                   the test data were used form Calgary Compression
                   Corpus. Using hashing, the processing speed of the CSD
                   became as fast as the LZC, though the CSD algorithm was
                   more complicated than the LZC},
  doi            = {10.1109/DCC.1994.305931},
  file           = {file:///home/roman/workspace/zipchip/papers/Nakano94.pdf},
  issn           = {},
  keywords       = {roman\_todo},
  month          = {Mar},
  year           = 1994
}

@Article{4463327,
  Author         = {Narasimhan, S. and Zhou, Yu and Chiel, H. J. and
                   Bhunia, S. },
  Title          = {Low-Power VLSI Architecture for Neural Data
                   Compression Using Vocabulary-based Approach},
  Journal        = {Biomedical Circuits and Systems Conference, 2007.
                   BIOCAS 2007. IEEE},
  Pages          = {134--137},
  abstract       = {Modern-day bio-implantable chips for neural prostheses
                   cannot monitor a large number of electrodes at the same
                   time since they suffer from excessively high data
                   rates. Hence, it is imperative to design area and
                   power-efficient digital circuits for appropriate
                   conditioning of the recorded neural signal in order to
                   remain within the bandwidth constraint. Previously, we
                   have proposed an algorithm for neural data compression,
                   which incorporates the concept of creating and
                   maintaining a dynamic vocabulary of neural spike
                   waveforms represented as wavelet transform
                   coefficients. In this paper, we propose an appropriate
                   architecture for low-power and area-efficient VLSI
                   implementation of the scheme. Based on simulation
                   results, the hardware consumes 3.55 muW and 0.36 mW
                   power using 0.18 mum CMOS technology for 1-channel and
                   100-channel neural recording applications,
                   respectively.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/BIOCAS.2007.4463327},
  citeulike-article-id = {4030187},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/BIOCAS.2007.4463327},
  keywords       = {-source-ieee-hw-comp-2007-, ian_todo},
  month          = {Nov.},
  posted-at      = {2009-02-10 15:43:37},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/BIOCAS.2007.4463327},
  year           = 2007
}

@Article{1488545,
  Author         = {Sandoval, Morales M. and Uribe, Feregrino C. },
  Title          = {A Hardware Architecture for Elliptic Curve
                   Cryptography and Lossless Data Compression},
  Journal        = {Electronics, Communications and Computers, 2005.
                   CONIELECOMP 2005. Proceedings. 15th International
                   Conference on},
  Pages          = {113--118},
  abstract       = {We present a hardware architecture that combines
                   Elliptic Curve Cryptography (ECC) and lossless data
                   compression in a single chip. Input data is compressed
                   using a dictionary-based lossless data compressor
                   before encryption, then; two elliptic curve
                   cryptographic algorithms can be applied to the
                   compressed data: ECIES for encryption or ECDSA for
                   digital signature. Applying data compression presents
                   three advantages: first, the improvement in the
                   cryptographic module throughput by reducing the amount
                   of data to be encrypted; second, the higher utilization
                   of the available bandwidth if encrypted data is
                   transmitted across a public network and third, the
                   increment of the difficulty to recover the original
                   information. The architecture was described in VHDL and
                   synthesized for a Xilinx FPGA device. The results
                   achieved show that it is possible to combine these two
                   algorithms in a single chip while gathering the
                   advantages of compression and cryptography. This work
                   is novel in the sense that no such algorithm
                   combination has been reported neither a hardware
                   implementation of elliptic curve cryptographic schemes.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/CONIEL.2005.8},
  citeulike-article-id = {4030277},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/CONIEL.2005.8},
  keywords       = {-source-ieee-hw-comp, fpga; ian_todo},
  month          = {Feb.},
  posted-at      = {2009-02-10 15:43:43},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/CONIEL.2005.8},
  year           = 2005
}

@Article{4253152,
  Author         = {Kamboh, A. M. and Raetz, M. and Mason, A. and Oweiss,
                   K. },
  Title          = {Area-Power Efficient Lifting-Based DWT Hardware for
                   Implantable Neuroprosthetics},
  Journal        = {Circuits and Systems, 2007. ISCAS 2007. IEEE
                   International Symposium on},
  Pages          = {2371--2374},
  abstract       = {Discrete wavelet transform (DWT) has been shown to
                   provide exceptionally efficient data compression for
                   neural records. This paper describes an area-power
                   minimized hardware implementation of the lifting scheme
                   for multi-level, multi-channel DWT. Performance
                   tradeoffs and key design decisions for implantable
                   neuroprosthetics are analyzed. A 32-channel, 4-level
                   version of the circuit has been custom designed in
                   0.18mum CMOS and occupies only 0.16mm2.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/ISCAS.2007.377936},
  citeulike-article-id = {4030175},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/ISCAS.2007.377936},
  keywords       = {-source-ieee-hw-comp-2007-},
  month          = {May},
  posted-at      = {2009-02-10 15:43:37},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/ISCAS.2007.377936},
  year           = 2007
}

@Article{1344287,
  Author         = {Lee, Taeyeon and Park, Jaehong },
  Title          = {Design and implementation of static Huffman encoding
                   hardware using a parallel shifting algorithm},
  Journal        = {Nuclear Science, IEEE Transactions on},
  Volume         = {51},
  Number         = {5},
  Pages          = {2073--2080},
  abstract       = {This work discusses the implementation of static
                   Huffman encoding hardware for real-time lossless
                   compression for the electromagnetic calorimeter in the
                   CMS experiment. The construction of the Huffman
                   encoding hardware illustrates the implementation for
                   optimizing the logic size. The number of logic gates in
                   the parallel shift operation required for the hardware
                   was examined. The experiment with a simulated
                   environment and an FPGA shows that the real-time
                   constraint has been fulfilled and the design of the
                   buffer length is appropriate.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/TNS.2004.834715},
  citeulike-article-id = {4030261},
  comment        = {Interessante Referenzen. paralellized Left shift
                   algorithm and Right shift algorithm. Berechnung des
                   Gateverbrauchs.},
  date-modified  = {2009-02-12 00:48:57 +0100},
  doi            = {http://dx.doi.org/10.1109/TNS.2004.834715},
  file           = {file:///home/roman/workspace/zipchip/papers/Lee04.pdf},
  keywords       = {-source-ieee-hw-comp, fpga; ian\_todo; roman\_info},
  month          = {Oct.},
  posted-at      = {2009-02-10 15:43:42},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/TNS.2004.834715},
  year           = 2004
}

@Article{4636093,
  Author         = {Becchi, M. and Franklin, M. and Crowley, P. },
  Title          = {A workload for evaluating deep packet inspection
                   architectures},
  Journal        = {Workload Characterization, 2008. IISWC 2008. IEEE
                   International Symposium on},
  Pages          = {79--89},
  abstract       = {High-speed content inspection of network traffic is an
                   important new application area for programmable
                   networking systems, and has recently led to several
                   proposals for high-performance regular expression
                   matching. At the same time, the number and complexity
                   of the patterns present in well-known network intrusion
                   detection systems has been rapidly increasing. This
                   increase is important since both the practicality and
                   the performance of specific pattern matching designs
                   are strictly dependent upon characteristics of the
                   underlying regular expression set. However, a commonly
                   agreed upon workload for the evaluation of deep packet
                   inspection architectures is still missing, leading to
                   frequent unfair comparisons, and to designs lacking in
                   generality or scalability. In this paper, we propose a
                   workload for the evaluation of regular expression
                   matching architectures. The workload includes a regular
                   expression model and a traffic generator, with the
                   former characterizing different levels of
                   expressiveness within rule-sets and the latter
                   characterizing varying degrees of malicious network
                   activity. The proposed workload is used here to
                   evaluate designs (e.g., different memory layouts and
                   hardware organizations) where the matching algorithm is
                   based on compressed deterministic and non deterministic
                   finite automata (DFAs and NFAs).},
  bdsk-url-1     = {http://dx.doi.org/10.1109/IISWC.2008.4636093},
  citeulike-article-id = {4030198},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/IISWC.2008.4636093},
  keywords       = {-source-ieee-hw-comp-2007-},
  month          = {Sept.},
  posted-at      = {2009-02-10 15:43:38},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/IISWC.2008.4636093},
  year           = 2008
}

@Article{4341496,
  Author         = {Corsonello, P. and Perri, S. and Staino, G. and
                   Lanuzza, M. and Cocorullo, G. },
  Title          = {Design and Implementation of a 90nm Low bit-rate Image
                   Compression Core},
  Journal        = {Digital System Design Architectures, Methods and
                   Tools, 2007. DSD 2007. 10th Euromicro Conference on},
  Pages          = {383--389},
  abstract       = {This paper presents a low-cost, high throughput
                   discrete wavelet transform-based image compressor. The
                   hardware solution proposed here exploits a modified set
                   partitioning in hierarchical trees (SPIHT) algorithm
                   and ensures that appropriate reconstructed image
                   qualities can be achieved also for compression ratios
                   over 100:1. Obtained results demonstrate that a maximum
                   data rate of about 23 Mpixels/s can be sustained on a
                   64x64 size tile. In 90 nm technology, the required area
                   is only 1.77 mm2. To obtain higher performance,
                   multiples cores can be used in a parallel
                   implementation.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/DSD.2007.4341496},
  citeulike-article-id = {4030190},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/DSD.2007.4341496},
  keywords       = {-source-ieee-hw-comp-2007-, image-coding},
  month          = {Aug.},
  posted-at      = {2009-02-10 15:43:37},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/DSD.2007.4341496},
  year           = 2007
}

@Article{1106658,
  Author         = {Mitra, T. and Chiueh, Tzi C. },
  Title          = {An FPGA implementation of triangle mesh decompression},
  Journal        = {Field-Programmable Custom Computing Machines, 2002.
                   Proceedings. 10th Annual IEEE Symposium on},
  Pages          = {22--31},
  abstract       = {This paper presents an FPGA-based design and
                   implementation of a three dimensional (3D)) triangle
                   mesh decompressor. Triangle mesh is the dominant
                   representation of 3D geometric models. The prototype
                   decompressor is based on a simple and highly efficient
                   triangle mesh compression algorithm, called BFT mesh
                   encoding. To the best of our knowledge, this is the
                   first hardware implementation of triangle mesh
                   decompression. The decompressor can be added at the
                   front-end of a 3D graphics card sitting on the PCI/AGP
                   bus. It can reduce the bandwidth requirement on the bus
                   between the host and the graphics card by up to 80
                   compared to standard triangle mesh representations.
                   Other mesh decompression algorithms with comparable
                   compression efficiency to BFT mesh encoding are too
                   complex to be implemented in hardware.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/FPGA.2002.1106658},
  citeulike-article-id = {4030293},
  date-modified  = {2009-02-12 00:49:47 +0100},
  doi            = {http://dx.doi.org/10.1109/FPGA.2002.1106658},
  keywords       = {-source-ieee-hw-comp, fpga, image-coding; ian_todo},
  posted-at      = {2009-02-10 15:43:44},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/FPGA.2002.1106658},
  year           = 2002
}

@Article{1431103,
  Author         = {Kotteri, K. A. and Barua, S. and Bell, A. E. and
                   Carletta, J. E. },
  Title          = {A comparison of hardware implementations of the
                   biorthogonal 9/7 DWT: convolution versus lifting},
  Journal        = {Circuits and Systems II: Express Briefs, IEEE
                   Transactions on},
  Volume         = {52},
  Number         = {5},
  Pages          = {256--260},
  abstract       = {The filter bank approach for computing the discrete
                   wavelet transform (DWT), which we call the convolution
                   method, can employ either a nonpolyphase or polyphase
                   structure. This work compares filter banks with an
                   alternative polyphase structure for calculating the
                   DWT-the lifting method. We look at the traditional
                   lifting structure and a recently proposed "flipping"
                   structure for implementing lifting. All filter bank
                   structures are implemented on an Altera
                   field-programmable gate array. The quantization of the
                   coefficients (for implementation in fixed-point
                   hardware) plays a crucial role in the performance of
                   all structures, affecting both image compression
                   quality and hardware metrics. We design several
                   quantization methods and compare the best design for
                   each approach: the nonpolyphase filter bank, the
                   polyphase filter bank, the lifting and flipping
                   structures. The results indicate that for the same
                   image compression performance, the flipping structure
                   gives the smallest and fastest, low-power hardware.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/TCSII.2005.843496},
  citeulike-article-id = {4030259},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/TCSII.2005.843496},
  keywords       = {-source-ieee-hw-comp; ian_todo},
  month          = {May},
  posted-at      = {2009-02-10 15:43:42},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/TCSII.2005.843496},
  year           = 2005
}

@Article{Uribe01a,
  Author         = {Uribe, C. F. and Jones, S. R. },
  Title          = {Optimisation of PPMC model for hardware implementation},
  Journal        = {Digital Systems, Design, 2001. Proceedings. Euromicro
                   Symposium on},
  Pages          = {120--126},
  abstract       = {The development of new and more powerful applications
                   in data communications and computer systems has
                   required an ever-increasing capacity to handle large
                   amounts of data. Lossless data compression techniques
                   have been developed to exploit further available
                   bandwidth of such systems by reducing the amount of
                   data to transmit or store. They have been implemented
                   in both software and hardware. The former approach
                   provides good compression ratios but presents speed
                   limitations. The latter approach offers the possibility
                   of high-speed compression to suit the most demanding
                   applications. Current available hardware
                   implementations are based mainly on LZ (Lempel-Ziv)
                   class of compression schemes. Experience suggests that
                   classical statistical methods, particularly PPM
                   (Prediction by Partial Matching) class of algorithms,
                   are impractical for being too slow and resource hungry
                   for hardware realisation. However, there seems to have
                   been relatively little work looking at the potential
                   for reorganising and restructuring the algorithm for
                   hardware implementation. This paper presents a version
                   of the PPMC class of algorithms structured for
                   efficient hardware support and analyses the issues of
                   its hardware implementation},
  bdsk-url-1     = {http://dx.doi.org/10.1109/DSD.2001.952251},
  citeulike-article-id = {4030306},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/DSD.2001.952251},
  keywords       = {-source-ieee-hw-lz, hw, lz; ian_todo},
  posted-at      = {2009-02-10 15:43:45},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/DSD.2001.952251},
  year           = 2001
}

@Article{4737024,
  Author         = {Guo, Wei and Xie, Jing and Zhang, Zhicheng },
  Title          = {Low Complexity Deblocking Algorithm and Implementation
                   with a Configurable Processor},
  Journal        = {Advanced Computer Theory and Engineering, 2008. ICACTE
                   '08. International Conference on},
  Pages          = {581--584},
  abstract       = {A low complexity algorithm for post-processing
                   deblocking and implementation with a configurable
                   processor designed by us is presented in this paper.
                   The algorithm is aimed at improving subjective visual
                   effect and reducing the complexity for hardware design.
                   According to the masking effect of human visual system
                   (HVS), the blocking artifacts can be recognized only
                   when the blocking artifacts continually appear on the
                   boundaries of the blocks. Based on this, we proposed a
                   novel algorithm that the deblocking will be processed
                   only on the region which has simple image and mass
                   blocking artifacts. Undesirable blue can be prevented.
                   The hardware implementation with a configurable
                   processor improves flexibility in application for
                   different video compression standards and shorts the
                   time-to-market. Our configurable processor is based on
                   Transport Triggered Architecture (TTA). The design in
                   RTL is synthesized using 0.18um TSMC library. The
                   simulation result shows that the proposed algorithm can
                   earn better visual effect and hardware architecture can
                   meet the real-time deblocking processing for CIF format
                   video sequence at a system clock of 70MHz.},
  annote         = {entfernen von block-artefakten (video) -> andere
                   baustelle einzig Transport Triggered Architecture (TTA)
                   vielleicht spannend},
  bdsk-url-1     = {http://dx.doi.org/10.1109/ICACTE.2008.21},
  citeulike-article-id = {4030240},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/ICACTE.2008.21},
  keywords       = {-source-ieee-hw-comp-2007-, fad; ian_note},
  month          = {Dec.},
  posted-at      = {2009-02-10 15:43:40},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/ICACTE.2008.21},
  year           = 2008
}

@Article{Huang00b,
  Author         = {Huang, W. J. and Saxena, N. and Mccluskey, E. J. },
  Title          = {A reliable LZ data compressor on reconfigurable
                   coprocessors},
  Journal        = {Field-Programmable Custom Computing Machines, 2000
                   IEEE Symposium on},
  Pages          = {249--258},
  abstract       = {Data compression techniques based on the Lempel-Ziv
                   (LZ) algorithm are widely used in a variety of
                   applications, especially in communications and data
                   storage. However, since the LZ algorithm involves a
                   considerable amount of parallel comparisons, it may be
                   difficult to achieve a very high throughput using
                   software approaches on general-purpose processors. In
                   addition, error propagation due to single-bit transient
                   errors during LZ compression causes a data integrity
                   problem. We present an implementation of LZ data
                   compression on reconfigurable hardware with concurrent
                   error detection for high performance and reliability.
                   Our approach achieves 100 Mbps throughput using four
                   Xilinx 4036XLA FPGA chips. We also present an inverse
                   comparison technique for LZ compression to guarantee
                   data integrity with less area overhead than traditional
                   systems based on duplication. The resulting execution
                   time overhead and compression ratio degradation due to
                   concurrent error detection is also minimized},
  bdsk-url-1     = {http://dx.doi.org/10.1109/FPGA.2000.903412},
  citeulike-article-id = {4030307},
  date-modified  = {2009-02-12 00:50:07 +0100},
  doi            = {http://dx.doi.org/10.1109/FPGA.2000.903412},
  keywords       = {-source-ieee-hw-lz, fpga, hw, lz; ian_todo},
  posted-at      = {2009-02-10 15:43:45},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/FPGA.2000.903412},
  year           = 2000
}

@Article{1250076,
  Author         = {Nunez, J. L. and Jones, S. },
  Title          = {Run-length coding extensions for high performance
                   hardware data compression},
  Journal        = {Computers and Digital Techniques, IEE Proceedings -},
  Volume         = {150},
  Number         = {6},
  Pages          = {387--395},
  abstract       = {The optimal placement of a run-length coding extension
                   to a dictionary-based lossless data compression
                   algorithm is investigated. A hardware implementation of
                   the proposed extension is completed and integrated into
                   an existing design. The new hardware is benchmarked
                   against commercially available software and hardware
                   compression methods. Run-length coding replaces
                   repetitive, identical sequences of a symbol with a pair
                   formed by a code indicating the repeating symbol plus a
                   code indicating the length or number of repetitions
                   that occurred. This method of coding is of limited use
                   for general compression due to its relatively poor
                   performance. However, good results can be achieved when
                   used as an extension to a general lossless data
                   compression algorithm where it can improve compression,
                   targeting the coding of repetitive data formats such as
                   zeroes in memory or fax pages or a uniform colour in
                   image backgrounds. It is shown that the run-length
                   coding method can be extended to support more complex
                   repeating patterns with little extra cost in terms of
                   hardware or speed but providing significant superior
                   compression performance.},
  bdsk-url-1     = {http://dx.doi.org/10.1049/ip-cdt:20030750},
  citeulike-article-id = {4030270},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1049/ip-cdt:20030750},
  keywords       = {-source-ieee-hw-comp; ian_todo},
  month          = {Nov.},
  posted-at      = {2009-02-10 15:43:42},
  priority       = {2},
  url            = {http://dx.doi.org/10.1049/ip-cdt:20030750},
  year           = 2003
}

@Article{1583665,
  Author         = {Dandalis, A. and Prasanna, V. K. },
  Title          = {Configuration compression for FPGA-based embedded
                   systems},
  Journal        = {Very Large Scale Integration (VLSI) Systems, IEEE
                   Transactions on},
  Volume         = {13},
  Number         = {12},
  Pages          = {1394--1398},
  abstract       = {Field programmable gate arrays (FPGAs) are a promising
                   technology for developing high-performance embedded
                   systems. The density and performance of FPGAs have
                   drastically improved over the past few years.
                   Consequently, the size of the configuration bit-streams
                   has also increased considerably. As a result, the
                   cost-effectiveness of FPGA-based embedded systems is
                   significantly affected by the memory required for
                   storing various FPGA configurations. This paper
                   proposes a novel compression technique that reduces the
                   memory required for storing FPGA configurations and
                   results in high decompression efficiency. Decompression
                   efficiency corresponds to the decompression hardware
                   cost as well as the decompression rate. The proposed
                   technique is applicable to any SRAM-based FPGA device
                   since configuration bit-streams are processed as raw
                   data. The required decompression hardware is simple and
                   the decompression rate scales with the speed of the
                   memory used for storing the configuration bit-streams.
                   Moreover, the time to configure the device is not
                   affected by our compression technique. Using our
                   technique, we demonstrate up to 41\% savings in memory
                   for configuration bit-streams of several real-world
                   applications.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/TVLSI.2005.862721},
  citeulike-article-id = {4030288},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/TVLSI.2005.862721},
  keywords       = {-source-ieee-hw-comp, embedded-systems, fpga; ian_todo},
  month          = {Dec.},
  posted-at      = {2009-02-10 15:43:44},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/TVLSI.2005.862721},
  year           = 2005
}

@Article{4261264,
  Author         = {Bonny, T. and Henkel, J. },
  Title          = {Instruction Splitting for Efficient Code Compression},
  Journal        = {Design Automation Conference, 2007. DAC '07. 44th
                   ACM/IEEE},
  Pages          = {646--651},
  abstract       = {The size of embedded software is rising at a rapid
                   pace. It is often challenging and time consuming to fit
                   an amount of required software functionality within a
                   given hardware resource budget. Code compression is a
                   means to alleviate the problem. In this paper we
                   introduce a novel and efficient hardware-supported
                   approach. Our scheme reduces the size of the generated
                   decoding table by splitting instructions into portions
                   of varying size (called patterns) before Huffman coding
                   compression is applied. It improves the final
                   compression ratio (including all overhead that incurs)
                   by more than 20\% compared to known schemes based on
                   Huffman coding. We achieve allover compression ratios
                   as low as 44\%. Thereby, our scheme is orthogonal to
                   approaches that take particularities of a certain
                   instruction set architectures into account. We have
                   conducted evaluations using a representative set of
                   applications and have applied it to two major embedded
                   processors, namely ARM and MIPS.},
  citeulike-article-id = {4030185},
  date-modified  = {2009-02-12 01:23:39 +0100},
  keywords       = {-source-ieee-hw-comp-2007-, embedded-systems},
  month          = {June},
  posted-at      = {2009-02-10 15:43:37},
  priority       = {2},
  year           = 2007
}

@Article{966496,
  Author         = {Abali, B. and Shen, Xiaowei and Franke, H. and Poff,
                   D. E. and Smith, T. B. },
  Title          = {Hardware compressed main memory: operating system
                   support and performance evaluation},
  Journal        = {Computers, IEEE Transactions on},
  Volume         = {50},
  Number         = {11},
  Pages          = {1219--1233},
  abstract       = {A new memory subsystem, called Memory Xpansion
                   Technology (MXT), has been built for compressing main
                   memory contents. MXT effectively doubles the physically
                   available memory transparently to the CPUs,
                   input/output devices, device drivers, and application
                   software. An average compression ratio of two or
                   greater has been observed for many applications. Since
                   compressibility of memory contents varies dynamically,
                   the size of the memory managed by the operating system
                   is not fixed. In this paper, we describe operating
                   system techniques that can deal with such dynamically
                   changing memory sizes. We also demonstrate the
                   performance impact of memory compression using the SPEC
                   CPU2000 and SPECweb99 benchmarks. Results show that the
                   hardware compression of memory has a negligible
                   performance penalty compared to a standard memory for
                   many applications. For memory starved applications and
                   benchmarks such as SPECweb99, memory compression
                   improves the performance significantly. Results also
                   show that the memory contents of many applications can
                   be compressed, usually by a factor of two to one},
  bdsk-url-1     = {http://dx.doi.org/10.1109/12.966496},
  citeulike-article-id = {4030284},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/12.966496},
  keywords       = {-source-ieee-hw-comp; ian_todo},
  month          = {Nov},
  posted-at      = {2009-02-10 15:43:43},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/12.966496},
  year           = 2001
}

@Article{4295292,
  Author         = {Musin, S. B. and Ivaniuk, A. A. and Yarmolik, V. N. },
  Title          = {Multiple Errors Detection Technique for RAM},
  Journal        = {Design and Diagnostics of Electronic Circuits and
                   Systems, 2007. DDECS '07. IEEE},
  Pages          = {1--4},
  abstract       = {This paper introduces a new technique for multiple
                   errors detection for RAM based on the Self-Adjusting
                   Output Data Compression (SAODC). The proposed technique
                   needs an extra block to be injected into SAODC unit.
                   This block performs evaluation of supplementary bits,
                   which are added to each compressed address. All pairs
                   of address bits iterated and their product gives the
                   supplemented bit. Unlike an earlier works, our
                   technique allows to detect multiple errors of even
                   error rate. Besides, we consider comparative
                   implementation and hardware overhead for proposed and
                   standard approaches.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/DDECS.2007.4295292},
  citeulike-article-id = {4030226},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/DDECS.2007.4295292},
  keywords       = {-source-ieee-hw-comp-2007-},
  month          = {April},
  posted-at      = {2009-02-10 15:43:39},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/DDECS.2007.4295292},
  year           = 2007
}

@Article{815613,
  Author         = {Sakanashi, H. and Iwata, M. and Keymulen, D. and
                   Murakawa, M. and Kajitani, I. and Tanaka, M. and
                   Higuchi, T. },
  Title          = {Evolvable hardware chips and their applications},
  Journal        = {Systems, Man, and Cybernetics, 1999. IEEE SMC '99
                   Conference Proceedings. 1999 IEEE International
                   Conference on},
  Volume         = {5},
  abstract       = {The paper describes Evolvable Hardware (EHW) chips and
                   their industrial applications. EHW refers to hardware
                   devices that can adjust their circuit structure to
                   adapt to varying environments. Unlike traditional
                   hardware, EHW is capable of autonomously changing its
                   functionality whilst operating in a real environment,
                   and thus represents a major new approach to hardware
                   design and development. A number of the applications of
                   EHW currently under development at the Electrotechnical
                   Laboratory are introduced, and in particular, a data
                   compression EHW chip for electrophotographic printers,
                   which has achieved compression ratios twice those of
                   the international standard methods, is described in
                   some detail},
  bdsk-url-1     = {http://dx.doi.org/10.1109/ICSMC.1999.815613},
  citeulike-article-id = {4030256},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/ICSMC.1999.815613},
  keywords       = {-source-ieee-hw-comp; ian_todo},
  posted-at      = {2009-02-10 15:43:42},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/ICSMC.1999.815613},
  year           = 1999
}

@Article{4703231,
  Author         = {Dyer, M. and Nooshabadi, S. and Taubman, D. },
  Title          = {Design and Analysis of System on a Chip Encoder for
                   JPEG2000},
  Journal        = {Circuits and Systems for Video Technology, IEEE
                   Transactions on},
  Volume         = {19},
  Number         = {2},
  Pages          = {215--225},
  abstract       = {Much work has been performed on optimizing the
                   throughput of the block coding system within JPEG2000.
                   However, the question remains as to whether providing
                   parallel simple block coders provides a cheaper method
                   of increasing throughput than complicated optimized
                   block coders. We present the analysis and results for a
                   system on a chip (SoC) software/hardware codesign
                   platform, for parallel coding in JPEG2000 compression
                   standard. We design both a simple and a high
                   performance, optimized peripheral encoder as a hardware
                   accelerator for the JPEG2000 SoC encoding system. The
                   system is implemented on an Altera NIOS II processor
                   with flexible integrated peripheral. We show that there
                   are optimum numbers of parallel block coders and
                   scheduling granularity per row of codeblocks, and that
                   parallel optimized encoders outperform parallel simple
                   encoders. We also demonstrate that the block coding
                   system becomes work starved rather than memory blocked
                   when many parallel coders are present, indicating a
                   discrete wavelet transform bottleneck.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/TCSVT.2008.2009245},
  citeulike-article-id = {4030228},
  date-modified  = {2009-02-10 22:30:30 +0100},
  doi            = {http://dx.doi.org/10.1109/TCSVT.2008.2009245},
  keywords       = {-source-ieee-hw-comp-2007-; ian_todo},
  month          = {Feb.},
  posted-at      = {2009-02-10 15:43:39},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/TCSVT.2008.2009245},
  year           = 2009
}

@Article{4253482,
  Author         = {Abd and Salama, A. E. and Khalil, A. H. },
  Title          = {Design and Implementation of FPGA-based Systolic Array
                   for LZ Data Compression},
  Journal        = {Circuits and Systems, 2007. ISCAS 2007. IEEE
                   International Symposium on},
  Pages          = {3691--3695},
  abstract       = {Hardware implementation of data compression algorithms
                   is receiving increasing attention due to exponentially
                   expanding network traffic and digital data storage
                   usage. Among lossless data compression algorithms for
                   hardware implementation, Lempel-Ziv algorithm is one of
                   the most widely used. The main objective of this paper
                   is to enhance the efficiency of systolic-array approach
                   for implementation of Lempel-Ziv algorithm. The
                   proposed implementation is area and speed efficient.
                   The compression rate is increased by more than 40\% and
                   the design area is decreased by more than 30\%. The
                   effect of the selected buffer's size on the compression
                   ratio is analyzed. An FPGA implementation of the
                   proposed design is carried out. It verifies that data
                   can be compressed and decompressed on-the-fly.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/ISCAS.2007.378644},
  citeulike-article-id = {4030178},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/ISCAS.2007.378644},
  keywords       = {-source-ieee-hw-comp-2007-, fpga; ian_todo},
  month          = {May},
  posted-at      = {2009-02-10 15:43:37},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/ISCAS.2007.378644},
  year           = 2007
}

@Article{4488798,
  Author         = {Mihajlovic, B. and Zilic, Z. and Radecka, K. },
  Title          = {Compression and encryption of self-test programs for
                   wireless sensor network nodes},
  Journal        = {Circuits and Systems, 2007. MWSCAS 2007. 50th Midwest
                   Symposium on},
  Pages          = {1344--1347},
  abstract       = {This paper considers the in-system testing of wireless
                   sensor network (WSN) nodes that operate with strict
                   limits on energy and cost, and are prone to malicious
                   interference. We show that while performing testing
                   using software-based self-test (SBST) programs has
                   clear advantages, node energy efficiency can be
                   increased by compressing self-test programs before
                   distributing them over the wireless interface. We
                   demonstrate a node energy savings by using an adaptive
                   test program compression scheme with a small memory
                   footprint based upon the BSTW algorithm in conjunction
                   with Golomb-Rice coding. As well, we assist in
                   providing security during the testing process by
                   offloading cryptographic functions to hardware for a
                   measure of protection against malicious interference
                   during testing.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/MWSCAS.2007.4488798},
  citeulike-article-id = {4030193},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/MWSCAS.2007.4488798},
  keywords       = {-source-ieee-hw-comp-2007-},
  month          = {Aug.},
  posted-at      = {2009-02-10 15:43:38},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/MWSCAS.2007.4488798},
  year           = 2007
}

@Article{1715326,
  Author         = {Lin, Ming B. and Lee, Jang F. and Jan, G. E. },
  Title          = {A Lossless Data Compression and Decompression
                   Algorithm and Its Hardware Architecture},
  Journal        = {Very Large Scale Integration (VLSI) Systems, IEEE
                   Transactions on},
  Volume         = {14},
  Number         = {9},
  Pages          = {925--936},
  abstract       = {In this paper, we propose a new two-stage hardware
                   architecture that combines the features of both
                   parallel dictionary LZW (PDLZW) and an approximated
                   adaptive Huffman (AH) algorithms. In this architecture,
                   an ordered list instead of the tree-based structure is
                   used in the AH algorithm for speeding up the
                   compression data rate. The resulting architecture shows
                   that it not only outperforms the AH algorithm at the
                   cost of only one-fourth the hardware resource but it is
                   also competitive to the performance of LZW algorithm
                   (compress). In addition, both compression and
                   decompression rates of the proposed architecture are
                   greater than those of the AH algorithm even in the case
                   realized by software},
  bdsk-url-1     = {http://dx.doi.org/10.1109/TVLSI.2006.884045},
  citeulike-article-id = {4030273},
  comment        = {Performance Vergleiche des LZW in Kompination mit
                   Huffman mit verschiedenen Optimierungen. Detailierte
                   Architekturangaben Detaillierte Erl\"auterungen zu den
                   Algorithem Genaue Speicherverbrauchsangaben},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/TVLSI.2006.884045},
  file           = {file:///home/roman/workspace/zipchip/papers/Lin06.pdf},
  keywords       = {-source-ieee-hw-comp; ian\_todo; roman\_info},
  month          = {Sept.},
  posted-at      = {2009-02-10 15:43:43},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/TVLSI.2006.884045},
  year           = 2006
}

@Article{4545806,
  Author         = {Iyer, V. and Rammurthy, G. and Srinivas, M. B. },
  Title          = {Training Data Compression Algorithms and Reliability
                   in Large Wireless Sensor Networks},
  Journal        = {Sensor Networks, Ubiquitous and Trustworthy Computing,
                   2008. SUTC '08. IEEE International Conference on},
  Pages          = {480--485},
  abstract       = {With the availability of low-cost sensor nodes there
                   have been many standards developed to integrate and
                   network these nodes to form a reliable network allowing
                   many different types of hardware vendors to coexist.
                   Most of these solutions however have aimed at
                   industry-specific interoperability but not the size of
                   the sensor network and the large amount of data which
                   is collected in course of its lifetime. In this paper
                   we use well studied data compression algorithms which
                   optimize on bringing down the data redundancy which is
                   related to correlated sensor readings and using a
                   probability model to efficiently compress data at the
                   cluster heads. As in the case of sensor networks the
                   data reliability goes down as the network resource
                   depletes and these types of networks lacks any central
                   synchronization making it even more a global problem to
                   compare different reading at the central coordinator.
                   The complexity of calibrating each sensor and using an
                   adaptable measured threshold to correct the reading
                   from sensors is a severe drain in terms of network
                   resources and energy consumption. In this paper we
                   separate the task of comparative global analysis to a
                   central coordinator and use a reference PMax which is a
                   normalized probability of individual source which
                   reflects the current lifetime reliability of the
                   sensors calculated at the cluster heads which then is
                   compared with the current global reliability index
                   based on all the PMax of cluster heads. As this
                   implementation does not need any synchronization at the
                   local nodes it uses compress once and stamp locally
                   without any threshold such as application specific
                   calibration values (0-42degF) and the summarization can
                   be application Independent making It more a sensor
                   network reliability Index and using It independent of
                   the actual measured values.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/SUTC.2008.48},
  citeulike-article-id = {4030196},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/SUTC.2008.48},
  keywords       = {-source-ieee-hw-comp-2007-},
  month          = {June},
  posted-at      = {2009-02-10 15:43:38},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/SUTC.2008.48},
  year           = 2008
}

@Article{4685262,
  Author         = {Fong, Simon },
  Title          = {On improving the lightweight video encryption
                   algorithms for real-time video transmission},
  Journal        = {Communications and Networking in China, 2008. ChinaCom
                   2008. Third International Conference on},
  Pages          = {1287--1293},
  abstract       = {Fast lightweight encryption algorithms must be
                   developed to satisfy the level of security and the real
                   time constraints. However the recently proposed
                   lightweight MPEG video encryption algorithms that have
                   been proposed in the past suffer from certain
                   drawbacks. While some of them require hardware support,
                   the others are weak or reduce the MPEG compression
                   ratio. The lightweight encryption algorithm called VEA
                   is fast, satisfies the real time constraint and does
                   not reduce the MPEG compression ratio. However, it
                   relies on the key generator to generate a good
                   encryption key and it cannot withstand the
                   known-plaintext attack. In this paper, improvements to
                   the VEA are proposed namely the Rotation algorithm, the
                   XOR algorithm and one that combines VEA with IDEA
                   (I-VEA). They are able to secure MPEG video with
                   minimal computational overhead, which do not reduce the
                   MPEG compression ratio, do not rely upon the key
                   generator to generate an effective key and can better
                   resist the known-plaintext attack. The Rotation
                   algorithm is the fastest of the three, but is
                   relatively weak. I-VEA is the most secure but it adds
                   the maximum computational overhead. The XOR algorithm
                   is a good compromise between the two},
  bdsk-url-1     = {http://dx.doi.org/10.1109/CHINACOM.2008.4685262},
  citeulike-article-id = {4030242},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/CHINACOM.2008.4685262},
  keywords       = {-source-ieee-hw-comp-2007-},
  month          = {Aug.},
  posted-at      = {2009-02-10 15:43:40},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/CHINACOM.2008.4685262},
  year           = 2008
}

@Article{4579197,
  Author         = {Nandi, A. V. and Banakar, R. M. },
  Title          = {Hardware modeling and implementation of modified SPIHT
                   algorithm for compression of images},
  Journal        = {Industrial and Information Systems, 2007. ICIIS 2007.
                   International Conference on},
  Pages          = {329--334},
  abstract       = {We present a throughput-efficient FPGA implementation
                   of the Set Partitioning in Hierarchical Trees (SPIHT)
                   algorithm for compression of images. The SPIHT uses
                   inherent redundancy among wavelet coefficients and
                   suited for both grey and color images. The SPIHT
                   algorithm uses dynamic data structure which hinders
                   hardware realization. In our FPGA implementation we
                   have modified basic SPIHT in two ways, one by using
                   static (fixed) mappings which represent significant
                   information and the other by interchanging the sorting
                   and refinement passes. A hardware realization is done
                   in a Xilinx Vertex FPGA device. Significant throughput
                   and compression ratio are obtained.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/ICIINFS.2007.4579197},
  citeulike-article-id = {4030252},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/ICIINFS.2007.4579197},
  keywords       = {-source-ieee-hw-comp, fpga, image-coding},
  month          = {Aug.},
  posted-at      = {2009-02-10 15:43:41},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/ICIINFS.2007.4579197},
  year           = 2007
}

@Article{Lin00a,
  Author         = {Lin, Kun J. and Wu, Cheng W. },
  Title          = {A low-power CAM design for LZ data compression},
  Journal        = {Computers, IEEE Transactions on},
  Volume         = {49},
  Number         = {10},
  Pages          = {1139--1145},
  abstract       = {Low-power and high-performance data compressors play
                   an increasingly important role in the portable mobile
                   computing and wireless communication markets. Among
                   lossless data compression algorithms for hardware
                   implementation, LZ77 is one of the most widely used.
                   For real-time communication, some hardware LZ
                   compressors/decompressors have been proposed in the
                   past. Content addressable memory (CAM) is widely
                   considered as the most efficient architecture for
                   pattern matching required by the LZ77 compression
                   process. In this paper, we propose a low-power
                   CAM-based LZ77 data compressor. By shutting down the
                   power for unnecessary comparisons between the CAM words
                   and the input symbol, the proposed CAM architecture
                   consumes much lower power than the conventional ones
                   without noticeable performance penalty. Moreover, using
                   the proposed conditional comparison mechanism and the
                   novel CAM cell with the NAND-type matching logic, on
                   average we have close to two orders of improvement on
                   power consumption, i.e., a reduction of more than 98
                   percent for 8-bit words. Speed is sacrificed if we use
                   the NAND-type matching logic, but the NAND-type logic
                   and the NOR-type logic can be combined to provide the
                   best solution that balances power and delay. Our
                   approach also can be applied to general-purpose CAMs
                   which use the valid bits, so far as the proposed design
                   techniques are adopted},
  bdsk-url-1     = {http://dx.doi.org/10.1109/12.888055},
  citeulike-article-id = {4030295},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/12.888055},
  keywords       = {-source-ieee-hw-lz, hw, lz; ian_todo},
  month          = {Oct},
  posted-at      = {2009-02-10 15:43:44},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/12.888055},
  year           = 2000
}

@Article{4148767,
  Author         = {Milenkovic, M. and Milenkovic, A. and Burtscher, M. },
  Title          = {Algorithms and Hardware Structures for Unobtrusive
                   Real-Time Compression of Instruction and Data Address
                   Traces},
  Journal        = {Data Compression Conference, 2007. DCC '07},
  Pages          = {283--292},
  abstract       = {Instruction and data address traces are widely used by
                   computer designers for quantitative evaluations of new
                   architectures and workload characterization, as well as
                   by software developers for program optimization,
                   performance tuning, and debugging. Such traces are
                   typically very large and need to be compressed to
                   reduce the storage, processing, and communication
                   bandwidth requirements. However, preexisting
                   general-purpose and trace-specific compression
                   algorithms are designed for software implementation and
                   are not suitable for runtime compression. Compressing
                   program execution traces at runtime in hardware can
                   deliver insights into the behavior of the system under
                   test without any negative interference with normal
                   program execution. Traditional debugging tools, on the
                   other hand, have to stop the program frequently to
                   examine the state of the processor. Moreover, software
                   developers often do not have access to the entire
                   history of computation that led to an erroneous state.
                   In addition, stepping through a program is a tedious
                   task and may interact with other system components in
                   such a way that the original errors disappear, thus
                   preventing any useful insight. The need for unobtrusive
                   tracing is further underscored by the development of
                   computer systems that feature multiple processing cores
                   on a single chip. In this paper, we introduce a set of
                   algorithms for compressing instruction and data address
                   traces that can easily be implemented in an on-chip
                   trace compression module and describe the corresponding
                   hardware structures. The proposed algorithms are
                   analytically and experimentally evaluated. Our results
                   show that very small hardware structures suffice to
                   achieve a compression ratio similar to that of a
                   software implementation of gzip while being orders of
                   magnitude faster. A hardware structure with slightly
                   over 2 KB of state achieves a compression ratio of
                   125.9 for instruction address traces, whereas gzip
                   achieves a compression ratio of 87.4. For data addr- -
                   ess traces, a hardware structure with 5 KB of state
                   achieves a compression ratio of 6.1, compared to 6.8
                   achieved by gzip},
  bdsk-url-1     = {http://dx.doi.org/10.1109/DCC.2007.10},
  citeulike-article-id = {4030166},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/DCC.2007.10},
  keywords       = {-source-ieee-hw-comp-2007-},
  month          = {March},
  posted-at      = {2009-02-10 15:43:36},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/DCC.2007.10},
  year           = 2007
}

@Article{4291915,
  Author         = {Lu, Liang and Mccanny, J. V. and Sezer, S. },
  Title          = {Systolic Array Based Architecture for Variable
                   Block-Size Motion Estimation},
  Journal        = {Adaptive Hardware and Systems, 2007. AHS 2007. Second
                   NASA/ESA Conference on},
  Pages          = {160--168},
  abstract       = {A new systolic array based architecture is introduced
                   for variable block-size motion estimation (VBSME). This
                   comprises a 2D array structure for SAD value
                   computation combined with a parallel adder tree and a
                   SAD comparator unit. Parallel raster scan is applied
                   leading to high processor utilization. The exhibits
                   much simpler control and lower hardware cost compared
                   with previous circuits and can handle flexible search
                   ranges without any increase in silicon area. Resulting
                   computational rates are suitable for high end video
                   processing applications. This architecture can also be
                   simply configured as other video compression standards
                   usage. Silicon design studies, based on a 0.13 mum CMOS
                   technology, indicate that circuits based on this
                   approach are suitable for SDTV applications with search
                   ranges of up to 32times32 and for HDTV applications
                   with a search range of at least 16times16, with
                   increased ranges achievable using lower dimension
                   technologies.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/AHS.2007.103},
  citeulike-article-id = {4030167},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/AHS.2007.103},
  keywords       = {-source-ieee-hw-comp-2007-},
  month          = {Aug.},
  posted-at      = {2009-02-10 15:43:36},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/AHS.2007.103},
  year           = 2007
}

@Article{4512345,
  Author         = {Das, S. R. and Hossain, A. and Biswas, S. and Petriu,
                   E. M. and Assaf, M. H. and Jone, W. B. and Sahinoglu,
                   M. },
  Title          = {On a New Graph Theory Approach to Designing
                   Zero-Aliasing Space Compressors for Built-In
                   Self-Testing},
  Journal        = {Instrumentation and Measurement, IEEE Transactions on},
  Volume         = {57},
  Number         = {10},
  Pages          = {2146--2168},
  abstract       = {The realization of space-efficient support hardware
                   for built-in self-testing (BIST) is of great
                   significance in the design of present-day very large
                   scale integration (VLSI) circuits and systems,
                   particularly in the context of the recent paradigm
                   shift from system-on-board to system-on-chip (SOC). A
                   new approach in designing zero-aliasing
                   space-compaction hardware, specifically in relation to
                   embedded core-based SOC, is proposed in this paper for
                   single stuck-line faults, extending the well-known
                   concepts of conventional switching theory and of
                   incompatibility relation to generate the maximal
                   compatibility classes using graph theoretic concepts,
                   based on optimal generalized sequence mergeability, as
                   developed and applied by the authors in earlier works.
                   This is novel in the sense that zero-aliasing is
                   obtained without any modification of the original
                   module under test, while a maximal compaction is
                   achieved in almost all cases in reasonable time
                   utilizing some simple heuristics. The method is
                   illustrated with design details of space compactors for
                   the international symposium on circuits and systems
                   (ISCAS) 85 combinational and ISCAS 89 full-scan
                   sequential benchmark circuits using simulation programs
                   ATALANTA and FSIM, attesting to the usefulness of the
                   technique for its relative simplicity, resulting in low
                   area overhead, and full fault coverage for single
                   stuck-line faults, thus making it suitable in a VLSI
                   synthesis environment. With advances in computational
                   resources in the future, the heuristics applied in the
                   design algorithm may be further improved upon to
                   significantly lower the simulation CPU time and
                   storage.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/TIM.2007.910004},
  citeulike-article-id = {4030230},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/TIM.2007.910004},
  keywords       = {-source-ieee-hw-comp-2007-, embedded-systems},
  month          = {Oct.},
  posted-at      = {2009-02-10 15:43:40},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/TIM.2007.910004},
  year           = 2008
}

@Article{1514415,
  Author         = {Yanez, Nunez J. L. and Chouliaras, V. A. },
  Title          = {A configurable statistical lossless compression core
                   based on variable order Markov modeling and arithmetic
                   coding},
  Journal        = {Computers, IEEE Transactions on},
  Volume         = {54},
  Number         = {11},
  Pages          = {1345--1359},
  abstract       = {This paper presents a practical realization in
                   hardware of the concepts of variable order Markov
                   modeling using multisymbol alphabets and arithmetic
                   coding for lossless compression of universal data. This
                   type of statistical coding algorithm has long been
                   regarded as being able to deliver very high compression
                   ratios close to the information content of the source
                   data. However, their high computational complexity has
                   limited their practical application in embedded
                   environments such as in mobile computing and wireless
                   communications. In this paper, a hardware amenable
                   algorithm named PPMH and based on these principles has
                   been developed and its architecture and implementation
                   detailed. This novel lossless compression core offers
                   innovative solutions to the computational issues in
                   both stages of modeling and coding and delivers high
                   compression efficiency and throughput. The
                   configurability features of the core allow efficient
                   use of the embedded SRAM present in modern FPGA
                   technologies where memory resources range from a few
                   kilobits to several megabits per device family. The
                   core has been targeted to the Altera Stratix FPGA
                   family and performance, coding efficiency, and
                   complexity measured for different memory
                   configurations.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/TC.2005.171},
  citeulike-article-id = {4030289},
  date-modified  = {2009-02-12 00:37:39 +0100},
  doi            = {http://dx.doi.org/10.1109/TC.2005.171},
  keywords       = {-source-ieee-hw-comp, embedded-systems, fpga; ian_todo},
  month          = {Nov.},
  posted-at      = {2009-02-10 15:43:44},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/TC.2005.171},
  year           = 2005
}

@Article{4439245,
  Author         = {Koch, D. and Beckhoff, C. and Teich, J. },
  Title          = {Bitstream Decompression for High Speed FPGA
                   Configuration from Slow Memories},
  Journal        = {Field-Programmable Technology, 2007. ICFPT 2007.
                   International Conference on},
  Pages          = {161--168},
  abstract       = {In this paper, we present hardware decompression
                   accelerators for bridging the gap between high speed
                   FPGA configuration interfaces and slow configuration
                   memories. We discuss different compression algorithms
                   suitable for a decompression on FPGAs as well as on
                   CPLDs with respect to the achievable compression ratio,
                   throughput, and hardware overhead. This leads to
                   various decompressor implementations with one capable
                   to decompress at high data rates of up to 400 megabytes
                   per second while only requiring slightly more than a
                   hundred look-up tables. Furthermore, we present a
                   sophisticated configuration bitstream benchmark.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/FPT.2007.4439245},
  citeulike-article-id = {4030170},
  date-modified  = {2009-02-12 00:40:18 +0100},
  doi            = {http://dx.doi.org/10.1109/FPT.2007.4439245},
  keywords       = {-source-ieee-hw-comp-2007-, fpga; ian_todo},
  month          = {Dec.},
  posted-at      = {2009-02-10 15:43:37},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/FPT.2007.4439245},
  year           = 2007
}

@Article{628904,
  Author         = {Tierno, J.A. and Kudva, P.},
  Title          = {Asynchronous transpose-matrix architectures},
  Journal        = {Computer Design: VLSI in Computers and Processors,
                   1997. ICCD '97. Proceedings., 1997 IEEE International
                   Conference on},
  Pages          = {423-428},
  abstract       = {The matrix transposition operation is a necessary step
                   in several image/video compression and decompression
                   algorithms, in particular the discrete cosine transform
                   (DCT) and its inverse (IDCT), and some distributed
                   arithmetic applications. These algorithms have to be
                   performed at high data-rates, and with a minimum of
                   power dissipation for portable applications. The
                   authors describe how the clocked solution is usually
                   implemented, and present two new asynchronous
                   architectures that perform matrix transposition. These
                   architectures, one based on two phase signaling, one
                   based on four phase signaling, have better
                   characteristics than the clocked solution in terms of
                   latency and power, at no cost in area or throughput.
                   They discuss the characteristics of these three
                   architectures and evaluate the relative advantages of
                   each one},
  bdsk-url-1     = {http://dx.doi.org/10.1109/ICCD.1997.628904},
  date-added     = {2009-02-11 01:49:20 +0100},
  date-modified  = {2009-02-11 01:50:13 +0100},
  doi            = {10.1109/ICCD.1997.628904},
  keywords       = {ian_todo},
  month          = {Oct},
  year           = 1997
}

@Article{4762415,
  Author         = {Zipf, Peter and Hinkelmann, Heiko and Shao, Hui and
                   Dogaru, Radu and Glesner, Manfred },
  Title          = {An area-efficient FPGA realisation of a codebook-based
                   image compression method},
  Journal        = {Field-Programmable Technology, 2008. FPT 2008.
                   International Conference on},
  Pages          = {349--352},
  abstract       = {We present a hardware implementation of an efficient
                   image compression method optimised for small FPGAs. The
                   compression method is based on a codebook of reference
                   patterns to support multiplication-free quantisation of
                   the image data. Based on specific features of a
                   low-cost FPGA architecture, a pipelined implementation
                   is developed and evaluated. The implemented hardware
                   benefits from the simple structure of the compression
                   method and is optimised for area and performance. The
                   realised hardware as well as the underlying compression
                   mechanism are described and the synthesis results for
                   different model variants are compared. The results show
                   that a high compression rate is possible at extremely
                   low hardware costs. Also, a high frame rate can be
                   obtained even on a low-cost FPGA.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/FPT.2008.4762415},
  citeulike-article-id = {4030274},
  date-modified  = {2009-02-10 22:30:30 +0100},
  doi            = {http://dx.doi.org/10.1109/FPT.2008.4762415},
  keywords       = {-source-ieee-hw-comp; ian_todo},
  month          = {Dec.},
  posted-at      = {2009-02-10 15:43:43},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/FPT.2008.4762415},
  year           = 2008
}

@Article{4211796,
  Author         = {Wang, Zhanglei and Chakrabarty, K. and Wang, Seongmoon
                   },
  Title          = {SoC Testing Using LFSR Reseeding, and Scan-Slice-
                   Based TAM Optimization and Test Scheduling},
  Journal        = {Design, Automation \& Test in Europe Conference \&
                   Exhibition, 2007. DATE '07},
  Pages          = {1--6},
  abstract       = {We present an SoC testing approach that integrates
                   test data compression, TAM/test wrapper design, and
                   test scheduling. An improved LFSR reseeding technique
                   is used as the compression engine. All cores on the SoC
                   share a single on-chip LFSR. At any clock cycle, one or
                   more cores can simultaneously receive data from the
                   LFSR. Seeds for the LFSR are computed from the core
                   bits from the test cubes for multiple cores. We also
                   propose a scan-slice-based scheduling algorithm that
                   tries to maximize the number of core bits the LFSR can
                   produce at each clock cycle, such that the overall test
                   application time is minimized. Experimental results for
                   both ISCAS circuits and industrial circuits show that
                   optimal test application time, which is determined by
                   the largest core, can be achieved. The proposed
                   approach has small hardware overhead and is easy to
                   deploy. Only one LFSR, one phase shifter, and a few
                   counters should be added to the SoC. The scheduling
                   algorithm is also scalable for large industrial
                   circuits. The CPU time for a large industrial design
                   ranges from 1 to 30 minutes},
  bdsk-url-1     = {http://dx.doi.org/10.1109/DATE.2007.364591},
  citeulike-article-id = {4030182},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/DATE.2007.364591},
  keywords       = {-source-ieee-hw-comp-2007-},
  month          = {April},
  posted-at      = {2009-02-10 15:43:37},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/DATE.2007.364591},
  year           = 2007
}

@Article{4439277,
  Author         = {Konda, M. and Nakayama, T. and Miyamoto, N. and Ohmi,
                   T. },
  Title          = {A Balanced Vector-Quantization Processor Eliminating
                   Redundant Calculation for Real-Time Motion Picture
                   Compression},
  Journal        = {Field-Programmable Technology, 2007. ICFPT 2007.
                   International Conference on},
  Pages          = {325--328},
  abstract       = {A balanced vector-quantization (VQ) processor has been
                   developed for real-time encoding of motion pictures
                   (640times480 pixels) by using FPGA. The VQ processor
                   employs a search algorithm for VQ encoding to reduce
                   computational complexity and hardware volume. And this
                   VQ processor employs a new architecture of distance
                   calculation unit. By adopting the pipeline composition
                   of each element, the number of distance calculation
                   units could be reduced compared with fully parallel
                   hardware architecture. Besides, in order to reduce
                   memory size of codebook, 2048 template vectors consist
                   of 512 basic template vectors, and the distance
                   calculation units using pipeline composition are
                   arranged in parallel for rotated template vectors. As a
                   result, real-time VQ processor on FPGA is balanced
                   architecture compared with the fully parallel
                   architecture.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/FPT.2007.4439277},
  citeulike-article-id = {4030191},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/FPT.2007.4439277},
  keywords       = {-source-ieee-hw-comp-2007-, fpga, image-coding},
  month          = {Dec.},
  posted-at      = {2009-02-10 15:43:38},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/FPT.2007.4439277},
  year           = 2007
}

@Article{Henriques90a,
  Author         = {Henriques, S. and Ranganathan, N. },
  Title          = {A parallel architecture for data compression},
  Journal        = {Parallel and Distributed Processing, 1990. Proceedings
                   of the Second IEEE Symposium on},
  Pages          = {260--266},
  abstract       = {The authors describe a parallel algorithm and
                   architecture for implementing the LZ technique for data
                   compression. Data compression is the reduction of
                   redundancy in data representation in order to decrease
                   storage and communication costs. The LZ-based
                   compression method is a very powerful technique and
                   gives very high compression efficiency for text as well
                   as image data. The proposed architecture is systolic
                   and uses the principles of pipelining and parallelism
                   in order to obtain high speed and throughput. The order
                   of complexity of the computations is reduced from n2 to
                   n. The data compression hardware can be integrated into
                   real time systems so that data can be compressed and
                   decompressed on-the-fly. The basic processor cell for
                   the systolic array is currently being implemented using
                   CMOS VLSI technology},
  bdsk-url-1     = {http://dx.doi.org/10.1109/SPDP.1990.143545},
  citeulike-article-id = {4030303},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/SPDP.1990.143545},
  keywords       = {-source-ieee-hw-lz, hw, lz, ian_todo},
  month          = {Dec},
  posted-at      = {2009-02-10 15:43:45},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/SPDP.1990.143545},
  year           = 1990
}

@Article{1327078,
  Author         = {Brooks, L. and Fife, K. },
  Title          = {Hardware efficient lossless image compression engine},
  Journal        = {Acoustics, Speech, and Signal Processing, 2004.
                   Proceedings. (ICASSP '04). IEEE International
                   Conference on},
  Volume         = {5},
  abstract       = {A complete in-stream lossless hardware image
                   compression engine is implemented with a channel
                   splitting and division-free arithmetic encoding
                   technique. The hardware is less than 7000 gates and
                   requires 0.3 mm2 of area in a 0.35 Î¼m process. An
                   average of 46 compression is achieved over a diverse
                   set of images.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/ICASSP.2004.1327078},
  citeulike-article-id = {4030251},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/ICASSP.2004.1327078},
  keywords       = {-source-ieee-hw-comp; ian_todo},
  month          = {May},
  posted-at      = {2009-02-10 15:43:41},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/ICASSP.2004.1327078},
  year           = 2004
}

@Article{964994,
  Author         = {Kuang, Shiann R. and Jou, Jer M. and Chen, Ren D. and
                   Shiau, Yeu H. },
  Title          = {Dynamic pipeline design of an adaptive binary
                   arithmetic coder},
  Journal        = {Circuits and Systems II: Analog and Digital Signal
                   Processing, IEEE Transactions on},
  Volume         = {48},
  Number         = {9},
  Pages          = {813--825},
  abstract       = {Arithmetic coding is an attractive technique for
                   lossless data compression but it tends to be slow. In
                   this paper, a dynamic pipelined very large scale
                   integration architecture with high performance for
                   on-line adaptive binary arithmetic coding is presented.
                   To obtain a high throughput pipelined architecture, we
                   first analyze the computation flow of the coding
                   algorithm and modify the operations whose data and/or
                   control dependencies cause the difficulties in
                   pipelining. Then, a novel technique called dynamic
                   pipelining is developed to pipeline the coding process
                   with variant (or run-time determined) pipeline
                   latencies (or data initialization intervals)
                   efficiently. As for data path design, a systematic
                   design methodology of high level synthesis and a
                   lower-area but faster fixed-width multiplier are
                   applied, which implement the architecture with a little
                   additional hardware. The dynamic pipelined architecture
                   has been designed and simulated in Verilog HDL, and its
                   layout has also been implemented with the 0.8-Î¼m SPDM
                   CMOS process and the ITRI-CCL cell library. Its
                   simulated compression speeds under working frequencies
                   of 25 and 50 MHz are about 6 and 12.5 Mb/s,
                   respectively. About two times the speedup with 30
                   hardware overhead relative to the original sequential
                   realisation is achieved},
  bdsk-url-1     = {http://dx.doi.org/10.1109/82.964994},
  citeulike-article-id = {4030260},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/82.964994},
  keywords       = {-source-ieee-hw-comp, ian_todo},
  month          = {Sep},
  posted-at      = {2009-02-10 15:43:42},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/82.964994},
  year           = 2001
}

@Article{4196121,
  Author         = {Yamani, Al A. A. and Prasanna, Devta N. and Gunda, A. },
  Title          = {Systematic Scan Reconfiguration},
  Journal        = {Design Automation Conference, 2007. ASP-DAC '07. Asia
                   and South Pacific},
  Pages          = {738--743},
  abstract       = {We present a new test data compression technique that
                   achieves 10times to 40times compression ratios without
                   requiring any information from the ATPG tool about the
                   unspecified bits. The technique is applied to both
                   single-stuck as well as transition fault test sets. The
                   technique allows aggressive parallelization of scan
                   chains leading to similar reduction in test time. It
                   also reduces tester pins requirements by similar
                   ratios. The technique is implemented using a hardware
                   overhead of a few gates per scan chain.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/ASPDAC.2007.358075},
  citeulike-article-id = {4030215},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/ASPDAC.2007.358075},
  keywords       = {-source-ieee-hw-comp-2007-},
  month          = {Jan.},
  posted-at      = {2009-02-10 15:43:39},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/ASPDAC.2007.358075},
  year           = 2007
}

@Misc{MS97,
  Author         = {{Michael Schindler}},
  Title          = {Method and apparatus for sorting data blocks},
  HowPublished   = {US Patent},
  Note           = {International Classification: G06F 700; G06F 1730},
  month          = {14 } # nov,
  number         = {6199064},
  url            = {http://www.google.com/patents?id=k5IGAAAAEBAJ&dq=6199064},
  year           = 1997
}

@Article{1218222,
  Author         = {Nunez, J. L. and Jones, S. },
  Title          = {Gbit/s lossless data compression hardware},
  Journal        = {Very Large Scale Integration (VLSI) Systems, IEEE
                   Transactions on},
  Volume         = {11},
  Number         = {3},
  Pages          = {499--510},
  abstract       = {This paper presents the X-MatchPRO high-speed lossless
                   data compression algorithm and its hardware
                   implementation, which enables data independent
                   throughputs of 1.6 Gbit/s compression and decompression
                   using contemporary low-cost reprogrammable
                   field-programmable gate array technology. A full-duplex
                   implementation is presented that allows a combined
                   compression and decompression performance of 3.2
                   Gbit/s. The features of the compression algorithm and
                   architecture that have enabled the high throughputs are
                   described in detail. A comparison between this device
                   and other commercially available data compressors is
                   made in terms of technology, compression ratio, and
                   throughput. X-MatchPRO is a fully synchronous design
                   proven in silicon specially targeted to improve the
                   performance of Gbit/s storage and communication
                   applications.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/TVLSI.2003.812288},
  citeulike-article-id = {4030286},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/TVLSI.2003.812288},
  keywords       = {-source-ieee-hw-comp; ian_todo},
  month          = {June},
  posted-at      = {2009-02-10 15:43:44},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/TVLSI.2003.812288},
  year           = 2003
}

@Article{4392005,
  Author         = {Shi, Lei and Pang, Jun and Hua, Siliang and Zhang,
                   Tiejun and Hou, Chaohuan },
  Title          = {Implementation and analysis of configurable Real Time
                   Address Trace Compressor for embedded microprocessors},
  Journal        = {Communications and Information Technologies, 2007.
                   ISCIT '07. International Symposium on},
  Pages          = {163--167},
  abstract       = {Real-Time Address Trace Compression (RTATC) is a very
                   useful method for debugging or analyzing software
                   programs running on a processor-based system. Address
                   trace compression means that the instruction addresses,
                   which are produced in the instruction-fetch stage of
                   the microprocessor, are compressed and out putted for
                   later reconstruction and analysis. This paper presents
                   a kind of RTATC method which includes three phases:
                   branch filtering, address encoding and address
                   compressing. A synthesizable RTL code for this method
                   is constructed and integrated with a DSP\&CPU processor
                   to analyze the compressing effect and evaluate the
                   hardware cost. The results show that our hardware is
                   capable of real-time compression and achieving a very
                   high compression ratio.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/ISCIT.2007.4392005},
  citeulike-article-id = {4030168},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/ISCIT.2007.4392005},
  keywords       = {-source-ieee-hw-comp-2007-, embedded-systems},
  month          = {Oct.},
  posted-at      = {2009-02-10 15:43:36},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/ISCIT.2007.4392005},
  year           = 2007
}

@Article{904003,
  Author         = {Chitre, P.},
  Title          = {ATM and Internet via satellite},
  Journal        = {MILCOM 2000. 21st Century Military Communications
                   Conference Proceedings},
  Volume         = {2},
  Pages          = {624-628 vol.2},
  abstract       = {The twenty-first century military communications will
                   depend critically on satellite systems for their global
                   reach. The two major networking technologies underlying
                   the global information infrastructure (GII) will be
                   asynchronous transfer mode (ATM) and Internet (IP). The
                   seamless operation of ATM and Internet over the hybrid
                   (satellite and terrestrial) GII will be essential for
                   the unhampered access, dissemination and exchange of
                   information. However, there are major challenges to
                   achieve seamless operation for ATM and Internet via
                   satellite systems. Specific issues related to
                   asynchronous transfer mode (ATM) and internet standards
                   to function efficiently and provide high-quality
                   service over satellite links and networks are
                   discussed. Standardization activities for terrestrial
                   and satellite interoperability of ATM and Internet
                   Protocols (IP) are described. New technologies
                   developed to address these issues are presented,
                   including dynamic adaptive coding, data compression,
                   Transmission Control Protocol (TCP) Proxy, bandwidth on
                   demand, and traffic management. These technologies are
                   currently incorporated in COMSAT Laboratories products
                   such as COMSAT Link Accelerators for ATM and IP and the
                   LINKWAYTM 2000},
  bdsk-url-1     = {http://dx.doi.org/10.1109/MILCOM.2000.904003},
  date-added     = {2009-02-11 02:30:27 +0100},
  date-modified  = {2009-02-11 02:30:50 +0100},
  doi            = {10.1109/MILCOM.2000.904003},
  keywords       = {ian_todo},
  year           = 2000
}

@Article{4429984,
  Author         = {Osorio, R. R. and Bruguera, J. D. },
  Title          = {Entropy Coding on a Programmable Processor Array for
                   Multimedia SoC},
  Journal        = {Application -specific Systems, Architectures and
                   Processors, 2007. ASAP. IEEE International Conf. on},
  Pages          = {222--227},
  abstract       = {Entropy encoding and decoding is a crucial part of any
                   multimedia system that can be highly demanding in terms
                   of computing power. Hardware implementation of typical
                   compression and decompression algorithms is cumbersome,
                   while conventional software implementations are slow
                   due to bit-level operations, data dependencies and
                   conditional branching. Several solutions have been
                   proposed along the years, ranging from hardware
                   accelerators for high-end systems to careful
                   implementations in VLIW processors and instruction-set
                   extensions, both hardwired and reconfigurable.
                   Multimedia systems must often implement several
                   encoders and decoders for different formats. Hence, a
                   programmable solution is mandatory. However,
                   programmable processors may be challenged by
                   highly-complex algorithms. In this work, a highly
                   efficient and low cost alternative is presented based
                   on an array processor. The dataflow of several entropy
                   coding algorithms has been studied, leading to the
                   choice of an efficient programming model, processor
                   layout and interconnection system. Results are
                   presented for JPEG and H.264 image and video coding
                   standards.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/ASAP.2007.4429984},
  citeulike-article-id = {4030222},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/ASAP.2007.4429984},
  keywords       = {-source-ieee-hw-comp-2007-},
  month          = {July},
  posted-at      = {2009-02-10 15:43:39},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/ASAP.2007.4429984},
  year           = 2007
}

@Article{893543,
  Author         = {Aspar, Z. and Mohd Yusof, Z. and Suleiman, I.},
  Title          = {Parallel Huffman decoder with an optimized look up
                   table option on FPGA},
  Journal        = {TENCON 2000. Proceedings},
  Volume         = {1},
  Number         = {},
  Pages          = {73-76 vol.1},
  abstract       = {Compression is very important for systems with limited
                   channel bandwidth and/or limited storage size. One of
                   the main components in image/video compression is
                   variable length coding (VLC). This paper discusses one
                   of the most popular VLC technique known as Huffman
                   coding. A real time hardware parallel Huffman decoder
                   has been successfully designed and implemented using
                   50,000 gate FPGA (FLEX10K20 from Altera). The
                   parallelism is exploited in the design to achieve the
                   high frame rate such as in JPEG and MPEG
                   implementation. Using a parallel technique, a codeword
                   is guaranteed to be processed within a single clock
                   cycle. The codeword to be processed is matched with the
                   one stored in a look up table (LUT). A LUT is needed
                   during the coding and decoding process. In order to
                   save memory cost, an optimized LUT is suggested. This
                   paper does not intend to complete an optimized
                   operating speed design, but instead only concentrates
                   on producing a workable real-time decoder design},
  comment        = {Leading 0 and Leading 1 detector um den LUT-Speicher
                   klein zu halten, indem die Entries nach f\"uhrenden 0en
                   und 1en aufgespalten werden. Sehr interessant.},
  doi            = {10.1109/TENCON.2000.893543},
  file           = {file:///home/roman/workspace/zipchip/papers/Aspar04.pdf},
  issn           = {},
  keywords       = {roman\_info},
  month          = {},
  url            = {http://dx.doi.org/10.1109/TENCON.2000.893543},
  year           = 2000
}

@Article{JB01,
  Author         = {J. Badier, P. Busson, A. Karar},
  Title          = {Reduction of {ECAL} data volume using lossless data
                   compression techniques},
  Journal        = {Nucl. Instrum. Methods Phys. Res.},
  Volume         = {463},
  Pages          = {361-374},
  comment        = {Vergleich verschiedener Kompressionsalgorithmen??? Mal
                   schauen},
  file           = {file:///home/roman/workspace/zipchip/papers/Badier01.pdf},
  keywords       = {roman\_todo},
  year           = 2001
}

@Article{205736,
  Author         = {Parhami, B. and Lai, H. F. },
  Title          = {Alternate memory compression schemes for modular
                   multiplication},
  Journal        = {Signal Processing, IEEE Transactions on},
  Volume         = {41},
  Number         = {3},
  Pages          = {1378--1385},
  abstract       = {A memory compression scheme which reduces the size of
                   the lookup tables for modular multiplication by using a
                   new symmetry property is presented. The compression
                   ratio for a modulus p is equal to 4 and implies a 75\%
                   savings except if p is even and small. Although this
                   compression ratio has been achieved before, the present
                   scheme has the advantage of simpler peripheral
                   hardware. A further benefit is that it lends itself to
                   additional reduction of table size by a factor of about
                   two, for a total savings of 87\%. This additional
                   reduction requires two stages of table lookup or more
                   complicated addressing circuits. This modification,
                   which achieves table compression by a factor of eight,
                   is quite attractive in applications where long
                   sequences of multiplications are performed. It is shown
                   that by using a multiplication algorithm based on
                   squaring, a compression ratio of roughly p/2 is
                   achievable with moderate hardware complexity, and two
                   lookup steps},
  bdsk-url-1     = {http://dx.doi.org/10.1109/78.205736},
  citeulike-article-id = {4030283},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/78.205736},
  keywords       = {-source-ieee-hw-comp; ian_todo},
  month          = {Mar},
  posted-at      = {2009-02-10 15:43:43},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/78.205736},
  year           = 1993
}

@Article{4147663,
  Author         = {Alameldeen, A. R. and Wood, D. A. },
  Title          = {Interactions Between Compression and Prefetching in
                   Chip Multiprocessors},
  Journal        = {High Performance Computer Architecture, 2007. HPCA
                   2007. IEEE 13th International Symposium on},
  Pages          = {228--239},
  abstract       = {In chip multiprocessors (CMPs), multiple cores compete
                   for shared resources such as on-chip caches and
                   off-chip pin bandwidth. Stride-based hardware
                   prefetching increases demand for these resources,
                   causing contention that can degrade performance (up to
                   35\% for one of our benchmarks). In this paper, we
                   first show that cache and link (off-chip interconnect)
                   compression can increase the effective cache capacity
                   (thereby reducing off-chip misses) and increase the
                   effective off-chip bandwidth (reducing contention). On
                   an 8-processor CMP with no prefetching, compression
                   improves performance by up to 18\% for commercial
                   workloads. Second, we propose a simple adaptive
                   prefetching mechanism that uses cache compressions
                   extra tags to detect useless and harmful prefetches.
                   Furthermore, in the central result of this paper, we
                   show that compression and prefetching interact in a
                   strong positive way, resulting in combined performance
                   improvement of 10-51\% for seven of our eight workloads},
  bdsk-url-1     = {http://dx.doi.org/10.1109/HPCA.2007.346200},
  citeulike-article-id = {4030194},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/HPCA.2007.346200},
  keywords       = {-source-ieee-hw-comp-2007-},
  month          = {Feb.},
  posted-at      = {2009-02-10 15:43:38},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/HPCA.2007.346200},
  year           = 2007
}

@Article{Li01a,
  Author         = {Li, Zhiyuan and Hauck, S. },
  Title          = {Configuration Compression for Virtex FPGAs},
  Journal        = {Field-Programmable Custom Computing Machines, 2001.
                   FCCM '01. The 9th Annual IEEE Symposium on},
  Pages          = {147--159},
  abstract       = {Although run-time reconfigurable systems have been
                   shown to achieve very high performance, the speedups
                   over traditional microprocessor systems are limited by
                   the cost of configuration of the hardware. Current
                   reconfigurable systems suffer from a significant
                   overhead due to the time it takes to reconfigure their
                   hardware. In order to deal with this overhead, and
                   increase the compute power of reconfigurable systems,
                   it is important to develop hardware and software
                   systems to reduce or eliminate this delay. In this
                   paper, we explore the idea of configuration compression
                   and develop algorithms for reconfigurable systems.
                   These algorithms, targeted to Xilinx Virtex series
                   FPGAs with minimum modification of hardware, can
                   significantly reduce the amount of data needed to
                   transfer during configuration. In this work we have
                   extensively researched the current compression
                   techniques, including the Huffman coding, the
                   Arithmetic coding and LZ coding. We have also developed
                   different algorithms targeting different hardware
                   structures. Our readback algorithm allows certain
                   frames to be reused as a dictionary and sufficiently
                   utilize the regularities within the configuration
                   bitstream. In addition, we have developed frame
                   reordering techniques that better uses the regularities
                   by shuffling the sequence of the configuration. We have
                   also developed the wildcard approach that can be used
                   for true partial reconfiguration. The simulation
                   results demonstrate that a factor of 4 compression
                   ratio can be achieved.},
  citeulike-article-id = {4030305},
  date-modified  = {2009-02-12 01:23:39 +0100},
  keywords       = {-source-ieee-hw-lz, hw, lz; ian_todo},
  posted-at      = {2009-02-10 15:43:45},
  priority       = {2},
  year           = 2001
}

@Article{4437998,
  Author         = {Mccoy, D. and Bergmann, J. and Seefeld, S. },
  Title          = {Sourcery VSIPL++ on the Cell Broadband Engine: A Fused
                   Fast Convolution Example},
  Journal        = {DoD High Performance Computing Modernization Program
                   Users Group Conference, 2007},
  Pages          = {283--288},
  abstract       = {The importance of obtaining significant performance in
                   reasonable amounts of time is emphasized by the speed
                   at which hardware platforms evolve as compared to large
                   signal- and image- processing software applications.
                   Tools like Sourcery VSIPL++ increase productivity while
                   still enabling system designers to extend the software
                   life cycle by leveraging the potential of a portable,
                   standards- compliant, math-intensive library. This
                   paper illustrates this by showing a pulse-compression
                   (fast convolution) example, written in just a few lines
                   of C++, achieves good performance on the Cell/B.E. as
                   well as several other different processors.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/HPCMP-UGC.2007.66},
  citeulike-article-id = {4030220},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/HPCMP-UGC.2007.66},
  keywords       = {-source-ieee-hw-comp-2007-},
  month          = {June},
  posted-at      = {2009-02-10 15:43:39},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/HPCMP-UGC.2007.66},
  year           = 2007
}

@Article{Huang08a,
  Author         = {Huang, Shizhen and Zheng, Tianyi },
  Title          = {Hardware design for accelerating PNG decode},
  Journal        = {Electron Devices and Solid-State Circuits, 2008. EDSSC
                   2008. IEEE International Conference on},
  Pages          = {1--4},
  abstract       = {This paper discusses a hardware accelerated
                   implementation for PNG image decoding within LZ77 and
                   Huffman compression algorithm without any distortion,
                   while compare with software decoding, parallel
                   processing is the most significant feature of high
                   efficiency. This design utilize cooperating of software
                   and hardware to solve the problem of controlling
                   hardware decoder, extend Deflate algorithm of Huffman
                   tree to achieve fast look up, so it can improve decode
                   efficiency with lower power consume.},
  annote         = {hw/sw l{\"o}sung f{\"u}r PNG theorie zu deflate lz77
                   und huffman sehr grobe hw-implementierung - hab ich
                   nicht verstanden - parallelisierung schlechter artikel},
  bdsk-url-1     = {http://dx.doi.org/10.1109/EDSSC.2008.4760652},
  citeulike-article-id = {4030310},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/EDSSC.2008.4760652},
  keywords       = {hw, lz, ian_note},
  month          = {Dec.},
  posted-at      = {2009-02-10 15:43:45},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/EDSSC.2008.4760652},
  year           = 2008
}

@Article{4264327,
  Author         = {Kavousianos, X. and Kalligeros, E. and Nikolos, D. },
  Title          = {Optimal Selective Huffman Coding for Test-Data
                   Compression},
  Journal        = {Computers, IEEE Transactions on},
  Volume         = {56},
  Number         = {8},
  Pages          = {1146--1152},
  abstract       = {Selective Huffman coding has recently been proposed
                   for efficient test- data compression with low hardware
                   overhead. In this paper, we show that the already
                   proposed encoding scheme is not optimal and we present
                   a new one, proving that it is optimal. Moreover, we
                   compare the two encodings theoretically and we derive a
                   set of conditions which show that, in practical cases,
                   the proposed encoding always offers better compression.
                   In terms of hardware overhead, the new scheme is at
                   least as low-demanding as the old one. The increased
                   compression efficiency, the resulting test-time
                   savings, and the low hardware overhead of the proposed
                   method are also verified experimentally.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/TC.2007.1057},
  citeulike-article-id = {4030206},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/TC.2007.1057},
  file           = {file:///home/roman/workspace/zipchip/papers/Kavousianos071.pdf},
  keywords       = {-source-ieee-hw-comp-2007-; ian\_todo; roman\_todo},
  month          = {Aug.},
  posted-at      = {2009-02-10 15:43:38},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/TC.2007.1057},
  year           = 2007
}

@Article{Chen98a,
  Author         = {Chen, Jin M. and Wei, Che H. },
  Title          = {A novel VLSI design for Ziv-Lempel data compression},
  Journal        = {Circuits and Systems, 1998. IEEE APCCAS 1998. The 1998
                   IEEE Asia-Pacific Conference on},
  Pages          = {739--742},
  abstract       = {In this paper, we present a simple real-time parallel
                   architecture for CMOS VLSI implementation of Ziv-Lempel
                   (LZ type) data compression system. This encoding system
                   employs a linear systolic array to find concurrently
                   the matches between each input data character and its
                   corresponding dictionary. A new encoding architecture
                   is proposed to improve the encoding speed and reduce
                   the hardware complexity for the encoding cells. The
                   access time of memory is reduced to save its power
                   consumption for high-speed applications. The encoder
                   encodes one character( more than 8 bits) per encoding
                   cycle. The clock rate by Verilog simulator can be
                   constrained below 12 ns by 0.6 Î¼m CMOS technology
                   process},
  bdsk-url-1     = {http://dx.doi.org/10.1109/APCCAS.1998.743927},
  citeulike-article-id = {4030308},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/APCCAS.1998.743927},
  keywords       = {-source-ieee-hw-lz, hw, lz, ian_todo},
  month          = {Nov},
  posted-at      = {2009-02-10 15:43:45},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/APCCAS.1998.743927},
  year           = 1998
}

@Article{4429246,
  Author         = {Rahmatullah, M. M. and Khan, S. A. and Jamal, H. },
  Title          = {Carrier Class High Density VoIP Media Gateway using
                   Hardware Software Distributed Architecture},
  Journal        = {Consumer Electronics, IEEE Transactions on},
  Volume         = {53},
  Number         = {4},
  Pages          = {1513--1520},
  abstract       = {This paper presents a system on chip (SOC) based
                   high-density carrier class Voice over Internet Protocol
                   (VoIP) media gateway. The design exploits three levels
                   of parallelism namely HW/SWpartitioning,
                   multiprocessing and instruction level parallelism. The
                   system facilities the carrier class service providers
                   to transform channels on PSTN network to IP network,
                   enabling the consumers to enjoy several value added
                   services, such as voice, data, fax and video all
                   through a uniform network at lower cost. The SoC
                   implements a high-density media gateway switch for
                   carrier class VoIP applications. The solution is
                   synthesized to operate at 266 MHz handling up to OC-3
                   data rate port density equivalent to 2016 simultaneous
                   calls on a single chip and it can be cascaded to
                   support OC-12 and OC-48 rate port density on single
                   board in different configurations. On each channel the
                   SoC supports up to 128 ms tail length G.168 compliant
                   Line Echo Cancellation (LEC), voice activity detection
                   (VAD), comfort noise generation (CNG) and discontinuous
                   transmission (DTX), dual tone multi frequency (DTMF)
                   detection and generation, variety of G.xxx voice
                   codecs, V. 17 Fax and V.34 data modem standards and
                   MPEG4 video compression/decompression algorithms. The
                   solution is compared with the existing architectures
                   and commercial products; it offers 3x to 10x cost,
                   size, power and port density performance improvements
                   with IP and PSTN internetworking support. The media
                   gateway system based on the presented architecture has
                   been used to develop IP-based automatic call
                   distribution (ACD) by etel Pvt. Ltd and installed in
                   Telephone Inquiry Exchange of Pakistan Telecom and its
                   variants are being tested by Buraq Telecom for
                   installation in their service network.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/TCE.2007.4429246},
  citeulike-article-id = {4030207},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/TCE.2007.4429246},
  keywords       = {-source-ieee-hw-comp-2007-},
  month          = {Nov.},
  posted-at      = {2009-02-10 15:43:38},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/TCE.2007.4429246},
  year           = 2007
}

@Article{160075,
  Author         = {Jones, S. },
  Title          = {100 Mbit/s adaptive data compressor design using
                   selectively shiftable content-addressable memory},
  Journal        = {Circuits, Devices and Systems, IEE Proceedings G},
  Volume         = {139},
  Number         = {4},
  Pages          = {498--502},
  abstract       = {A hardware architecture for an adaptive lossless data
                   compressor is described. The architecture is suitable
                   for implementation on a single ASIC. The architecture
                   results from an investigation aimed at developing novel
                   compression algorithms that can utilise the fine-grain
                   parallel processing capabilities of VLSI integrable
                   structures and hence, achieve high performance. The
                   efficiency of different hardware structures are
                   assessed for text, image and machine code data
                   compression through simulation. Suitable candidate
                   designs based around a shifting content-addressable
                   memory (CAM) array are identified. A design for one
                   such option is developed using a commercial CAD
                   package. Despite using modest 2 Î¼m CMOS technology,
                   compressed data is produced at a minimum rate of 100
                   Mbit/s. Details of the design are presented},
  citeulike-article-id = {4030291},
  date-modified  = {2009-02-12 01:23:39 +0100},
  keywords       = {-source-ieee-hw-comp; ian_todo},
  month          = {Aug},
  posted-at      = {2009-02-10 15:43:44},
  priority       = {2},
  year           = 1992
}

@Article{4654014,
  Author         = {Mohseni, R. and Sheikhi, A. and Shirazi, M. A. M. },
  Title          = {Compression of Multicarrier Phase-Coded radar signals
                   with low sampling rate},
  Journal        = {Radar, 2008 International Conference on},
  Pages          = {718--721},
  abstract       = {Multicarrier phase-coded (MCPC) signals have been
                   recently introduced to achieve high range resolution in
                   radar systems. As in single carrier phase coded radars,
                   the conventional method for compression of these
                   signals is based on using matched filter or direct
                   computation of the autocorrelation function. In the
                   first part of this paper, we introduce a new method
                   based on fast Fourier transform (FFT) that is
                   mathematically equivalent to the matched filtering and
                   has lower computational complexity compared to the
                   traditional approach. Then based on this approach,
                   another method is proposed that needs lower sampling
                   rate to compress MCPC signals, which leads to the lower
                   hardware and computational complexity requirements.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/RADAR.2008.4654014},
  citeulike-article-id = {4030244},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/RADAR.2008.4654014},
  keywords       = {-source-ieee-hw-comp-2007-},
  month          = {Sept.},
  posted-at      = {2009-02-10 15:43:40},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/RADAR.2008.4654014},
  year           = 2008
}

@Article{Abd-El-ghany07a,
  Author         = {Abd and Salama, A. E. and Khalil, A. H. },
  Title          = {Design and Implementation of FPGA-based Systolic Array
                   for LZ Data Compression},
  Journal        = {Circuits and Systems, 2007. ISCAS 2007. IEEE
                   International Symposium on},
  Pages          = {3691--3695},
  abstract       = {Hardware implementation of data compression algorithms
                   is receiving increasing attention due to exponentially
                   expanding network traffic and digital data storage
                   usage. Among lossless data compression algorithms for
                   hardware implementation, Lempel-Ziv algorithm is one of
                   the most widely used. The main objective of this paper
                   is to enhance the efficiency of systolic-array approach
                   for implementation of Lempel-Ziv algorithm. The
                   proposed implementation is area and speed efficient.
                   The compression rate is increased by more than 40\% and
                   the design area is decreased by more than 30\%. The
                   effect of the selected buffer's size on the compression
                   ratio is analyzed. An FPGA implementation of the
                   proposed design is carried out. It verifies that data
                   can be compressed and decompressed on-the-fly.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/ISCAS.2007.378644},
  citeulike-article-id = {4030298},
  date-modified  = {2009-02-12 00:41:47 +0100},
  doi            = {http://dx.doi.org/10.1109/ISCAS.2007.378644},
  keywords       = {-source-ieee-hw-lz, fpga, hw, lz; ian_todo},
  month          = {May},
  posted-at      = {2009-02-10 15:43:44},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/ISCAS.2007.378644},
  year           = 2007
}

@Article{4282082,
  Author         = {Mahapatra, S. and Singh, K. },
  Title          = {An FPGA-Based Implementation of Multi-Alphabet
                   Arithmetic Coding},
  Journal        = {Circuits and Systems I: Regular Papers, IEEE
                   Transactions on},
  Volume         = {54},
  Number         = {8},
  Pages          = {1678--1686},
  abstract       = {A fully parallel implementation of the multi-alphabet
                   arithmetic-coding algorithm, an integral part of many
                   lossless data compression systems, had so far eluded
                   the research community. Although schemes were in
                   existence for performing the encoding operation in
                   parallel, the data dependencies involved in the
                   decoding phase prevented its parallel execution. This
                   paper presents a scheme for the parallel-pipelined
                   implementation of both the phases of the
                   arithmetic-coding algorithm for multisymbol alphabets
                   in high-speed programmable hardware. The compression
                   performance of the proposed scheme has been evaluated
                   and compared with an existing sequential implementation
                   in terms of average compression ratio as well as the
                   estimated execution time for the Canterbury Corpus test
                   set of files. The proposed scheme facilitates hardware
                   realization of both coder and decoder modules by
                   reducing the storage capacity necessary for maintaining
                   the modeling information. The design has been
                   synthesized for Xilinx field-programmable gate arrays
                   and the synthesis results obtained are encouraging,
                   paving the way for further research in this direction.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/TCSI.2007.902527},
  citeulike-article-id = {4030258},
  date-modified  = {2009-02-12 00:38:17 +0100},
  doi            = {http://dx.doi.org/10.1109/TCSI.2007.902527},
  keywords       = {-source-ieee-hw-comp, fpga; ian_todo},
  month          = {Aug.},
  posted-at      = {2009-02-10 15:43:42},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/TCSI.2007.902527},
  year           = 2007
}

@Article{4203085,
  Author         = {Ko, Ming Y. and Zissulescu, C. and Puthenpurayil, S.
                   and Bhattacharyya, S. S. and Kienhuis, B. and
                   Deprettere, E. F. },
  Title          = {Parameterized Looped Schedules for Compact
                   Representation of Execution Sequences in DSP Hardware
                   and Software Implementation},
  Journal        = {Signal Processing, IEEE Transactions on},
  Volume         = {55},
  Number         = {6},
  Pages          = {3126--3138},
  abstract       = {In this paper, we present a technique for compact
                   representation of execution sequences in terms of
                   efficient looping constructs. Here, by a looping
                   construct, we mean a compact way of specifying a finite
                   repetition of a set of execution primitives. Such
                   compaction, which can be viewed as a form of
                   hierarchical run-length encoding (RLE), has application
                   in many very large scale integration (VLSI) signal
                   processing contexts, including efficient control
                   generation for Kahn processes on field-programmable
                   gate arrays (FPGAs), and software synthesis for static
                   dataflow models of computation. In this paper, we
                   significantly generalize previous models for loop-based
                   code compaction of digital signal processing (DSP)
                   programs to yield a configurable code compression
                   methodology that exhibits a broad range of achievable
                   tradeoffs. Specifically, we formally develop and apply
                   to DSP hardware and software synthesis a
                   parameterizable loop scheduling approach with compact
                   format, dynamic reconfigurability, and low-overhead
                   decompression},
  bdsk-url-1     = {http://dx.doi.org/10.1109/TSP.2007.893964},
  citeulike-article-id = {4030204},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/TSP.2007.893964},
  keywords       = {-source-ieee-hw-comp-2007-, fpga},
  month          = {June},
  posted-at      = {2009-02-10 15:43:38},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/TSP.2007.893964},
  year           = 2007
}

@Article{4084757,
  Author         = {Borgio, Simone and Bosisio, Davide and Ferrandi,
                   Fabrizio and Monchiero, Matteo and Santambrogio, Marco
                   D. and Sciuto, Donatella and Tumeo, Antonino },
  Title          = {Hardware DWT accelerator for MultiProcessor
                   System-on-Chip on FPGA},
  Journal        = {Embedded Computer Systems: Architectures, Modeling and
                   Simulation, 2006. IC-SAMOS 2006. International
                   Conference on},
  Pages          = {107--114},
  abstract       = {High performance multimedia applications are typical
                   targets of today embedded systems. These applications,
                   complex both in terms of execution flow and amount of
                   elaborated data, can be well addressed by
                   multiprocessor systems on-chip (MPSoCs). MPSoCs are
                   composed of simple processors and memories tightly
                   interconnected with fast communication channels and
                   customized IP cores for the most demanding functions
                   can be implemented and attached to these systems to
                   enhance performance even more. Reconfigurable devices
                   like FPGA, can act as a target, even programmed at
                   runtime, for the custom IP cores, or as a prototyping
                   platform for the whole system. Image compression like
                   JPEG2000, can benefit very much from this approach and
                   this type of architectures. This paper shows how the
                   most demanding task of the JPEG2000 compression
                   algorithm, the two-dimensional discrete wavelet
                   transform, can be hardware accelerated and implemented
                   in a multiprocessor system-on-chip prototyping platform
                   on field programmable gate array (FPGA), CerberO.
                   Architectures with different number of processors and
                   hardware accelerators, shared among the processors or
                   dedicated, have been implemented. To validate the
                   approach, we show some experimental results on the
                   platform with the hardware and the software
                   implementation of the transformation},
  bdsk-url-1     = {http://dx.doi.org/10.1109/ICSAMOS.2006.300816},
  citeulike-article-id = {4030268},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/ICSAMOS.2006.300816},
  keywords       = {-source-ieee-hw-comp, embedded-systems, fpga,
                   image-coding},
  month          = {July},
  posted-at      = {2009-02-10 15:43:42},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/ICSAMOS.2006.300816},
  year           = 2006
}

@Article{4051119,
  Author         = {Huffman, D.A.},
  Title          = {A Method for the Construction of Minimum-Redundancy
                   Codes},
  Journal        = {Proceedings of the IRE},
  Volume         = {40},
  Number         = {9},
  Pages          = {1098-1101},
  abstract       = {An optimum method of coding an ensemble of messages
                   consisting of a finite number of members is developed.
                   A minimum-redundancy code is one constructed in such a
                   way that the average number of coding digits per
                   message is minimized.},
  doi            = {10.1109/JRPROC.1952.273898},
  file           = {file:///home/roman/workspace/zipchip/papers/Huffman52.pdf},
  issn           = {0096-8390},
  month          = {Sept.},
  url            = {http://dx.doi.org/10.1109/JRPROC.1952.273898},
  year           = 1952
}

@Article{4258439,
  Author         = {Hossain, A. and Das, S. R. and Nayak, A. R. and
                   Petriu, E. M. and Biswas, S. and Sahinoglu, M. },
  Title          = {Further Studies on Zero-Aliasing Space Compression
                   Based on Graph Theory},
  Journal        = {Instrumentation and Measurement Technology Conference
                   Proceedings, 2007. IMTC 2007. IEEE},
  Pages          = {1--6},
  abstract       = {The design of space-efficient support hardware for
                   built-in self-testing (BIST) is of great significance
                   in the realization of present day very large scale
                   integration (VLSI) circuits and systems, particularly
                   in the context of design paradigm shift from
                   system-onboard to system-on-chip (SOC). This paper
                   revisits the problem of designing zero-aliasing space
                   compression hardware in relation to embedded
                   cores-based SOC for single stuck-line faults in
                   particular, extending the well-known concepts of
                   switching theory, and of incompatibility relation to
                   generate maximal compatibility classes (MCCs) utilizing
                   new graph theory concepts, based on optimal generalized
                   sequence mergeability as developed by the authors in
                   earlier works. The paper briefly presents the
                   mathematical basis of selection criteria for merger of
                   an optimal number of outputs of the module under test
                   (MUT) for realizing maximal compaction ratio in the
                   design, along with some experimental results on ISCAS
                   89 full-scan sequential benchmark circuits, with
                   simulation programs ATALANTA and FSIM.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/IMTC.2007.379373},
  citeulike-article-id = {4030169},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/IMTC.2007.379373},
  keywords       = {-source-ieee-hw-comp-2007-, ian_todo},
  month          = {May},
  posted-at      = {2009-02-10 15:43:36},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/IMTC.2007.379373},
  year           = 2007
}

@Article{614737,
  Author         = {Ming-Bo Lin},
  Title          = {A Parallel VLSI Architecture For The LZW Data
                   Compression Algorithm},
  Journal        = {VLSI Technology, Systems, and Applications, 1997.
                   Proceedings of Technical Papers. 1997 International
                   Symposium on},
  Volume         = {},
  Number         = {},
  Pages          = {98-101},
  abstract       = {},
  doi            = {},
  file           = {file:///home/roman/workspace/zipchip/papers/Lin97.pdf},
  issn           = {},
  keywords       = {roman\_todo},
  month          = {Jun},
  year           = 1997
}

@Article{4287290,
  Author         = {Kamran, M. and Qureshi, S. A. and Feng, S. and Sattar,
                   A. },
  Title          = {Task Partitioning-An Efficient Scalable Pipelined
                   Digital Design Scheme},
  Journal        = {Electrical Engineering, 2007. ICEE '07. International
                   Conference on},
  Pages          = {1--5},
  abstract       = {Digital circuits are designed after careful
                   investigation of implementation constraints and
                   limitations. Implemented circuits, whether realized on
                   FPGA or an ASIC is developed, it is made sure that
                   resulting design should be optimized with respect to
                   processing speed and area occupied. Much informative
                   work has been already proposed and effective results
                   have obtained in the research field of processing speed
                   and area optimization. In this paper, the concept of
                   hierarchical concurrent flow graph (HCFG) is utilized
                   to present proposed coarse grained layered scalable
                   concurrent image compression (LSCIC) precoder design
                   with pipelined scheme. This design causes all modules
                   to operate concurrently for fast and minimum data loss
                   operation. This scheme will not only highlight the task
                   partitioning procedure to operate all modules in
                   parallel but also gives rise to the concept of
                   pipelining with reasonable number of stages so that
                   system remains optimized. Moreover, practical solutions
                   acquired by simulating tools are presented in this
                   paper with appropriate substantiation. This paper also
                   addresses the issue of selected FPGA resource
                   utilization depending upon the complexity of operation
                   and hardware components placed in corresponding module.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/ICEE.2007.4287290},
  citeulike-article-id = {4030225},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/ICEE.2007.4287290},
  keywords       = {-source-ieee-hw-comp-2007-, fpga, image-coding},
  month          = {April},
  posted-at      = {2009-02-10 15:43:39},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/ICEE.2007.4287290},
  year           = 2007
}

@Article{1437664,
  Author         = {Reaz, M. B. I. and Yasin, Mohd F. and Sulairnan, M. S.
                   and Tho, K. T. and Yeow, K. H. },
  Title          = {Hardware prototyping of boolean function
                   classification schemes for lossless data compression},
  Journal        = {Computational Cybernetics, 2004. ICCC 2004. Second
                   IEEE International Conference on},
  Pages          = {47--51},
  abstract       = {In this paper, we present the realization of Boolean
                   function classification schemes on Altera FLEX10K FPGA
                   device for lossless data compression. The compression
                   algorithm is performed by incorporating Boolean
                   function classification into Huffman coding. This
                   allows for more efficient compression because the data
                   has been categorized and simplified before the encoding
                   is done. The design is followed by the timing analysts
                   and circuit synthesis for the validation, functionality
                   and performance of the designated circuit which
                   supports the practicality, advantages and effectiveness
                   of the proposed hardware realization for the
                   applications. The average compression ratio is 25\% to
                   37.5\% from numerous testing with various text inputs
                   with a maximum clock frequency of 27.9 MHz},
  bdsk-url-1     = {http://dx.doi.org/10.1109/ICCCYB.2004.1437664},
  citeulike-article-id = {4030269},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/ICCCYB.2004.1437664},
  keywords       = {-source-ieee-hw-comp, fpga; ian_todo},
  posted-at      = {2009-02-10 15:43:42},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/ICCCYB.2004.1437664},
  year           = 2004
}

@Article{VZG06,
  Author         = {Virgilio Zuniga Grajeda, Claudia Feregrino Uribe, Rene
                   Cumplido Parra},
  Title          = {Parallel {H}ardware/{S}oftware {A}rchitecture for the
                   {BWT} and {LZ}77 {L}ossless {D}ata {C}ompression
                   {A}lgorithms},
  Journal        = {Computacion y Sistemas},
  Volume         = {10},
  Number         = {2},
  Pages          = {172-188},
  comment        = {Implementierung eines Weavesorters f\"ur LZ und BWT
                   inkl. Ressourcenverbrauch und Performancemessung },
  keywords       = {roman\_info},
  url            = {http://redalyc.uaemex.mx/pdf/615/61500206.pdf},
  year           = 2006
}

@Article{4417945,
  Author         = {Clements, A. },
  Title          = {Work in progress --- The role of hardware and
                   architecture in the new computer sciences},
  Journal        = {Frontiers in education conference - global
                   engineering: knowledge without borders, opportunities
                   without passports, 2007. FIE '07. 37th annual},
  abstract       = {Because many computer science students see themselves
                   as future practitioners rather than computer designers,
                   their interest in computer architecture and hardware
                   has been declining over the years. This trend has
                   accelerated with the growth of multimedia, web design
                   and computer games. In this work in progress we argue
                   that computer architecture is still a necessary
                   component of the curriculum. However, if it is to
                   survive, it must change to suit today's courses.
                   Slightly modifying the computer architecture curriculum
                   is no longer an option; the curriculum must be
                   transformed into a tool that serves the interests of
                   students studying the new computing subjects. The focus
                   must move from traditional topics such as number
                   conversion and logic design to topics like data
                   compression, mobile computer technology, ubiquitous
                   computing and even steganography. The proposed course
                   is not intended to replace all conventional computer
                   architecture courses; it is targeted at students on
                   multimedia-oriented courses.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/FIE.2007.4417945},
  citeulike-article-id = {4030173},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/FIE.2007.4417945},
  keywords       = {-source-ieee-hw-comp-2007-; ian_todo},
  month          = {Oct.},
  posted-at      = {2009-02-10 15:43:37},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/FIE.2007.4417945},
  year           = 2007
}

@Article{Lin04,
  Author         = {Jia-Yu Lin and Ying Liu and Ke-Chu Yi},
  Title          = {Balance of 0, 1 bits for Huffman and reversible
                   variable-length coding},
  Journal        = {Communications, IEEE Transactions on},
  Volume         = {52},
  Number         = {3},
  Pages          = { 359-361},
  abstract       = { This letter proposes a novel algorithm to obtain a
                   suboptimal solution for the balance of bit distribution
                   after Huffman coding. The algorithm is simple, and can
                   be embedded in the conventional Huffman coding process.
                   In addition, the letter also discusses the bit-balance
                   problem for reversible variable-length codes (RVLCs)
                   based on Huffman coding. Analytical and experimental
                   results suggest that the new algorithm is very useful
                   in improving the 0/1 balance property for Huffman codes
                   and RVLCs.},
  doi            = {10.1109/TCOMM.2004.823568},
  issn           = {0090-6778},
  keywords       = { Huffman codes, minimisation, statistical
                   distributions, variable length codes Huffman coding,
                   bidirectionally decodable stream, bit distribution
                   balancing, bit probability difference, bit-balance
                   problem, reversible variable-length coding},
  month          = {March},
  url            = {http://dx.doi.org/10.1109/TCOMM.2004.823568},
  year           = 2004
}

@Article{4266956,
  Author         = {Gunter, C. and Rothermel, A. },
  Title          = {Quantizer and Entropy Effects on EBCOT Based
                   Compression},
  Journal        = {Consumer Electronics, IEEE Transactions on},
  Volume         = {53},
  Number         = {2},
  Pages          = {661--666},
  abstract       = {Detailed process analysis and different optimization
                   strategies are necessary in order to make wavelet based
                   image processing algorithms usable for portable
                   multimedia devices. Besides optimizing the hardware
                   structures for better performance, the overall
                   processing load has to be minimized as proposed in this
                   paper. Analyzing the image content adoptively for all
                   decomposition levels and subbands, signal entropy can
                   be reduced to obtain shorter code words after the
                   entropy coder. We propose a reduction of the data
                   accuracy in the quantizer for high compression rates,
                   saving up to 28\% processing power without compromising
                   the image quality.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/TCE.2007.381743},
  citeulike-article-id = {4030203},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/TCE.2007.381743},
  keywords       = {-source-ieee-hw-comp-2007-, image-coding},
  month          = {May},
  posted-at      = {2009-02-10 15:43:38},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/TCE.2007.381743},
  year           = 2007
}

@Article{4127425,
  Author         = {Pal, S. K. },
  Title          = {Fast, Reliable \& Secure Digital Communication Using
                   Hadamard Matrices},
  Journal        = {Computing: Theory and Applications, 2007. ICCTA '07.
                   International Conference on},
  Pages          = {526--532},
  abstract       = {The symmetric, orthogonal and self-inverting
                   properties of Hadamard matrices make them useful for a
                   number of present day applications. In addition, the
                   binary nature of elements of the basis vectors helps in
                   the design of efficient algorithms for coding,
                   compression and security of digital media together with
                   their compact hardware realization. This paper presents
                   schemes for providing fast, reliable and secure
                   communications over resource constrained wireless
                   networks using Hadamard matrices. Error-correction and
                   bandwidth expansion capabilities are demonstrated for
                   unobtrusive communication of text and speech. Fast
                   encryption of speech signal using permutations on
                   Hadamard matrices is demonstrated. Techniques for
                   extending the key-space for improving the security of
                   such schemes are also reported},
  bdsk-url-1     = {http://dx.doi.org/10.1109/ICCTA.2007.61},
  citeulike-article-id = {4030180},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/ICCTA.2007.61},
  keywords       = {-source-ieee-hw-comp-2007-; ian_todo},
  month          = {March},
  posted-at      = {2009-02-10 15:43:37},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/ICCTA.2007.61},
  year           = 2007
}

@Article{4384057,
  Author         = {Wander, E. and Vaslin, R. and Gogniat, G. and Diguet,
                   J. P. },
  Title          = {A Code Compression Method to Cope with Security
                   Hardware Overheads},
  Journal        = {Computer Architecture and High Performance Computing,
                   2007. SBAC-PAD 2007. 19th International Symposium on},
  Pages          = {185--192},
  abstract       = {Code Compression has been used to alleviate the memory
                   requirements as well as to improve performance and/or
                   minimize energy consumption. On the other hand,
                   implementing security primitives on Embedded Systems is
                   always costly in terms of area and performance. In this
                   paper we present a code compression method, the IBC-EI
                   (instruction based compression with encryption and
                   integrity checking), tailored to provide integrity
                   checking and encryption to secure processor-memory
                   transactions. The principle is to keep the code
                   compressed and ciphered in the memory, thus reducing
                   the memory footprint and providing more information per
                   memory access. For the Leon processor and a set of
                   benchmarks from the Mediabench and MiBench suites the
                   habitual overheads due to security trend to zero in
                   comparison to a system without security neither
                   compression.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/SBAC-PAD.2007.40},
  citeulike-article-id = {4030192},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/SBAC-PAD.2007.40},
  keywords       = {-source-ieee-hw-comp-2007-, embedded-systems; ian_todo},
  month          = {Oct.},
  posted-at      = {2009-02-10 15:43:38},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/SBAC-PAD.2007.40},
  year           = 2007
}

@Article{4318807,
  Author         = {Yongming, Yang and Jungang, Liu and Jianmin, Wei },
  Title          = {LADT Arithmetic Improved and Hardware Implemented for
                   FPGA -Based ECG Data Compression},
  Journal        = {Industrial Electronics and Applications, 2007. ICIEA
                   2007. 2nd IEEE Conference on},
  Pages          = {2230--2234},
  abstract       = {To resolve the problem of complex operation of the
                   linear approximation distance threshold (LADT)
                   algorithm, some improved researches were made to the
                   algorithm by taking the characteristic of
                   electrocardiograph (ECG) signal and the advantages of
                   field programmable gate arrays (FPGA) into account. The
                   paper puts forward a real time compression method with
                   much less mathematical operation and much easier
                   real-time implementation. With the artificial and
                   experimental verification, it turned to be that the
                   arithmetic has the advantages of low distortion and in
                   real time. In the application of improved fast LADT
                   algorithm, the paper designed the hardware system of
                   ECG data compression based on FPGA and the program of
                   the algorithm based on very high speed integrated
                   circuit hardware description language (VHDL). And the
                   experiment results of this system turned to be good
                   performance. The approach can be easily extended to
                   other similar applications.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/ICIEA.2007.4318807},
  citeulike-article-id = {4030272},
  date-modified  = {2009-02-12 00:41:35 +0100},
  doi            = {http://dx.doi.org/10.1109/ICIEA.2007.4318807},
  keywords       = {-source-ieee-hw-comp, fpga; ian_todo},
  month          = {May},
  posted-at      = {2009-02-10 15:43:43},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/ICIEA.2007.4318807},
  year           = 2007
}

@Article{Makhani94a,
  Author         = {Makhani, A. },
  Title          = {Single-chip lossless data compression solutions},
  Journal        = {WESCON/94. 'Idea/Microelectronics'. Conference Record},
  Pages          = {410--413},
  abstract       = {Today's lossless data compression single chip
                   solutions offer something for everybody. Architectural
                   flexibility, algorithm performance, industry standards,
                   price and future direction differentiate one solution
                   from the other. These differences position one above
                   the other in certain applications. This paper contrasts
                   recently introduced lossless data compression products
                   from Advanced Hardware Architectures with those of STAC
                   and IBM. Device examples used for this paper are
                   respectively: AHA3210, STAC9731 and ALDC1-5S},
  bdsk-url-1     = {http://dx.doi.org/10.1109/WESCON.1994.403563},
  citeulike-article-id = {4026354},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/WESCON.1994.403563},
  keywords       = {hw, lz; ian_todo},
  month          = {Sep},
  posted-at      = {2009-02-09 19:53:16},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/WESCON.1994.403563},
  year           = 1994
}

@InProceedings{Horibe91,
  Author         = {Horibe, Y.},
  Title          = {Balance Properties Of Optimal Binary Trees},
  Volume         = {},
  Number         = {},
  Pages          = {110-110},
  abstract       = {},
  doi            = {},
  issn           = {},
  journal        = {Information Theory, 1991 (papers in summary form only
                   received), Proceedings. 1991 IEEE International
                   Symposium on (Cat. No.91CH3003-1)},
  month          = {Jun},
  url            = {http://ieeexplore.ieee.org/xpl/RecentCon.jsp?punumber=557},
  year           = 1991
}

@Article{4387537,
  Author         = {Wang, Jiun P. and Kuang, Shiann R. },
  Title          = {Area-Efficient Signed Fixed-Width Multipliers with
                   Low-Error Compensation Circuit},
  Journal        = {Signal Processing Systems, 2007 IEEE Workshop on},
  Pages          = {157--162},
  abstract       = {In this paper, a framework of designing a low-error
                   signed fixed-width multiplier that receives two n-bits
                   operands and generates an n-bits product is proposed.
                   The proposed error compensation circuit not only leads
                   signed fixed-width multipliers to very low maximum
                   error, mean error and mean-square error but also can be
                   easily constructed with a simple logic gate. Moreover,
                   the proposed signed fixed-width multiplier is also
                   applied to the inverse discrete cosine transform
                   computation in JPEG image compression. Experimental
                   results demonstrate that the proposed circuit not only
                   improves the accurate performance but also
                   significantly reduces the hardware complexity and power
                   consumption when compared with the previous published
                   compensation circuit.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/SIPS.2007.4387537},
  citeulike-article-id = {4030165},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/SIPS.2007.4387537},
  keywords       = {-source-ieee-hw-comp-2007-},
  month          = {Oct.},
  posted-at      = {2009-02-10 15:43:36},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/SIPS.2007.4387537},
  year           = 2007
}

@Article{703836,
  Author         = {Shiann-Rong Kuang and Jer-Min Jou and Yuh-Lin Chen},
  Title          = {The design of an adaptive on-line binary
                   arithmetic-coding chip},
  Journal        = {Circuits and Systems I: Fundamental Theory and
                   Applications, IEEE Transactions on},
  Volume         = {45},
  Number         = {7},
  Pages          = {693-706},
  abstract       = {In this paper, we present a very large scale
                   integration (VLSI) design of the adaptive binary
                   arithmetic coding for lossless data compression and
                   decompression. The main modules of it consist of an
                   adaptive probability estimation modeler (APEM), an
                   arithmetic operation unit (AOU), and a normalization
                   unit (NU), A new bit-stuffing technique, which
                   simultaneously solves both the carry-over and
                   source-termination problems efficiently, is proposed
                   and designed in an NU. The APEM estimates the
                   conditional probabilities of input symbols efficiently
                   using a table lookup approach with 1.28-kbytes memory.
                   A new formula which efficiently reflects the change of
                   symbols' occurring probability is proposed, and a
                   complete binary tree is used to set up the values in
                   the probability table of an APEM. In an AOU, a
                   simplified parallel multiplier, which requires
                   approximately half of the area of a standard parallel
                   multiplier while maintaining a good compression ratio,
                   is proposed. Owing to these novel designs, the designed
                   chip can compress any type of data with an efficient
                   compression ratio, An asynchronous interface circuit
                   with an 8-b first-in first-out (FIFO) buffer for
                   input/output (UO) communication of the chip is also
                   designed. Thus, both UO and compression operations in
                   the chip can be done simultaneously. Moreover, the
                   concept of design for testability is used and a scan
                   path is implemented in the chip. A prototype 0.8-Î¼m
                   chip has been designed and fabricated in a reasonable
                   die size. This chip can yield a processing rate of 3
                   Mb/s with a clock rate of 25 MHz},
  bdsk-url-1     = {http://dx.doi.org/10.1109/81.703836},
  date-added     = {2009-02-11 01:38:15 +0100},
  date-modified  = {2009-02-11 01:48:52 +0100},
  doi            = {10.1109/81.703836},
  issn           = {1057-7122},
  keywords       = {ian_todo},
  month          = {Jul},
  year           = 1998
}

@Article{Ranganathan91b,
  Author         = {Ranganathan, N. and Henriques, S. },
  Title          = {A systolic architecture for LZ based decompression},
  Journal        = {Data Compression Conference, 1991. DCC '91.},
  abstract       = {Summary form only given. A parallel architecture is
                   proposed for decompression of data compressed using the
                   Lempel-Ziv technique. This paper reports ongoing work
                   towards realizing high speed VLSI hardware for the
                   decompression process. The semi-systolic architecture
                   aims at reducing the interconnect area and global
                   communication, thereby increasing the clock speed. The
                   algorithm has been mapped onto a special purpose VLSI
                   architecture, which requires three global control
                   signals. The hardware can yield a decompression rate of
                   one symbol per clock cycle. The clock speed is improved
                   by minimizing propagation delays},
  bdsk-url-1     = {http://dx.doi.org/10.1109/DCC.1991.213311},
  citeulike-article-id = {4030309},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/DCC.1991.213311},
  keywords       = {-source-ieee-hw-lz, hw, lz, ian_todo},
  month          = {Apr},
  posted-at      = {2009-02-10 15:43:45},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/DCC.1991.213311},
  year           = 1991
}

@Article{4631875,
  Author         = {Chew, Li W. and Ang, Li M. and Seng, Kah P. },
  Title          = {Survey of image compression algorithms in wireless
                   sensor networks},
  Journal        = {Information Technology, 2008. ITSim 2008.
                   International Symposium on},
  Volume         = {4},
  Pages          = {1--9},
  abstract       = {The implementation of image processing engines in
                   visual sensor nodes has been a major concern in the
                   development of wireless multimedia sensor networks in a
                   hardware constrained environment. In this paper, a
                   review on eight popular image compression algorithms is
                   presented. After conducting a comprehensive evaluation,
                   it is found that Set-Partitioning in Hierarchical Trees
                   (SPIHT) wavelet-based image compression is the most
                   suitable hardware implemented image compression
                   algorithm in wireless sensor networks due to its high
                   compression efficiency and its simplicity in coding
                   procedures.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/ITSIM.2008.4631875},
  citeulike-article-id = {4030197},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/ITSIM.2008.4631875},
  keywords       = {-source-ieee-hw-comp-2007-},
  month          = {Aug.},
  posted-at      = {2009-02-10 15:43:38},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/ITSIM.2008.4631875},
  year           = 2008
}

@Article{Mencer06a,
  Author         = {Mencer, O. },
  Title          = {ASC: a stream compiler for computing with FPGAs},
  Journal        = {Computer-Aided Design of Integrated Circuits and
                   Systems, IEEE Transactions on},
  Volume         = {25},
  Number         = {9},
  Pages          = {1603--1617},
  abstract       = {A stream compiler (ASC) for computing with field
                   programmable gate arrays (FPGAs) emerges from the
                   ambition to bridge the hardware-design productivity gap
                   where the number of available transistors grows more
                   rapidly than the productivity of very large scale
                   integration (VLSI) and FPGA computer-aided-design (CAD)
                   tools. ASC addresses this problem with a softwarelike
                   programming interface to hardware design (FPGAs) while
                   keeping the performance of hand-designed circuits at
                   the same time. ASC improves productivity by letting the
                   programmer optimize the implementation on the algorithm
                   level, the architecture level, the arithmetic level,
                   and the gate level, all within the same C++ program.
                   The increased productivity of ASC is applied to the
                   hardware acceleration of a wide range of applications.
                   Traditionally, hardware accelerators are tediously
                   handcrafted to achieve top performance. ASC simplifies
                   design-space exploration of hardware accelerators by
                   transforming the hardware-design task into a
                   software-design process, using only "GNU compiler
                   collection (GCC)" and "make" to obtain a hardware
                   netlist. From experience, the hardware-design
                   productivity and ease of use are close to pure software
                   development. This paper presents results and case
                   studies with optimizations that are: 1) on the gate
                   level-Kasumi and International Data Encryption
                   Algorithm (IDEA) encryptions; 2) on the arithmetic
                   level-redundant addition and multiplication function
                   evaluation for two-dimensional (2-D) rotation; and 3)
                   on the architecture level-Wavelet and Lempel-Ziv
                   (LZ)-like compression},
  bdsk-url-1     = {http://dx.doi.org/10.1109/TCAD.2005.857377},
  citeulike-article-id = {4030299},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/TCAD.2005.857377},
  keywords       = {-source-ieee-hw-lz, fpga, hw, lz, ian_todo},
  month          = {Sept.},
  posted-at      = {2009-02-10 15:43:44},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/TCAD.2005.857377},
  year           = 2006
}

@Article{17694,
  Author         = {Copeland, J.A.},
  Title          = {Data compression technique for PC communications},
  Journal        = {Selected Areas in Communications, IEEE Journal on},
  Volume         = {7},
  Number         = {2},
  Pages          = {246-248},
  abstract       = {An adaptive technique is described for compressing
                   data that are flowing through a communications channel,
                   being transferred to and from a personal computer using
                   a modem to connect to a remote computer or database
                   service. The objective is to reduce the transmitted
                   data to close to five bits for each character of input
                   data in typical ASCII text files with an algorithm that
                   is simple enough that an eight-bit microprocessor, such
                   as a Z-80, can handle a 19200-b/s asynchronous channel
                   and still be able to handle, in addition, X.25/LAPB
                   error-control protocol functions. This allows the data
                   to be transmitted from modem to modem at 9600 b/s with
                   synchronous transmission using the fast-turnaround
                   ping-pong protocol developed for the Hayes V-series
                   9600 modems, with the user connected to the modem at
                   19200 b/s. The technique described meets those
                   objectives for most text files and for most Lotus
                   worksheet files as well. Less effective compression is
                   realized when more random data are transmitted, such as
                   executable binary files; however, the compression ratio
                   is always close to the theoretical limit given by
                   information theory for character-by-character
                   compression. As an example of the effectiveness of this
                   technique, it reduced the ASCII text file of a draft of
                   this paper from 10928 to 6583 bytes, or to 60% of the
                   original size. This corresponds to 4.8 bits per
                   character},
  bdsk-url-1     = {http://dx.doi.org/10.1109/49.17694},
  date-added     = {2009-02-11 01:38:15 +0100},
  date-modified  = {2009-02-11 01:48:56 +0100},
  doi            = {10.1109/49.17694},
  issn           = {0733-8716},
  keywords       = {ian_todo},
  month          = {Feb},
  year           = 1989
}

@Article{143529,
  Author         = {Zito-Wolf, R.J.},
  Title          = {A broadcast/reduce architecture for high-speed data
                   compression},
  Journal        = {Parallel and Distributed Processing, 1990. Proceedings
                   of the Second IEEE Symposium on},
  Volume         = {},
  Number         = {},
  Pages          = {174-181},
  abstract       = {The author presents a parallel architecture for
                   high-speed data compression based on textual
                   substitution using a sliding window. The architecture
                   combines a systolic array with trees for data broadcast
                   and reduction. Compression involves two steps. First, a
                   match generator computes in parallel the maximal
                   matches available at each position of the input. The
                   generator uses a systolic array to hold the dictionary,
                   a pipelined broadcast tree to deliver each input
                   character simultaneously to every array cell, and a
                   reduction tree to identify the largest available match
                   each cycle. From this information a second process
                   selects a match sequence exactly covering the input.
                   Decoding mirrors encoding. The tree interconnect
                   provides through-delay proportional to the log of the
                   dictionary size, and pipelining reduces the effective
                   per-character processing time to a single system cycle.
                   A system data rate of 300 Mbit/sec is easily
                   attainable. The author discusses layout issues arising
                   from the tree interconnect},
  doi            = {10.1109/SPDP.1990.143529},
  file           = {file:///home/roman/workspace/zipchip/papers/Zito90.pdf},
  issn           = {},
  keywords       = {roman_todo},
  month          = {Dec},
  year           = 1990
}

@Article{5688,
  Author         = {Bentley,, Jon Louis and Sleator,, Daniel D. and
                   Tarjan,, Robert E. and Wei,, Victor K.},
  Title          = {A locally adaptive data compression scheme},
  Journal        = {Commun. ACM},
  Volume         = {29},
  Number         = {4},
  Pages          = {320--330},
  abstract       = {A data compression scheme that exploits locality of
                   reference, such as occurs when words are used
                   frequently over short intervals and then fall into long
                   periods of disuse, is described. The scheme is based on
                   a simple heuristic for self-organizing sequential
                   search and on variable-length encodings of integers. We
                   prove that it never performs much worse than Huffman
                   coding and can perform substantially better;
                   experiments on real files show that its performance is
                   usually quite close to that of Huffman coding. Our
                   scheme has many implementation advantages: it is
                   simple, allows fast encoding and decoding, and requires
                   only one pass over the data to be compressed (static
                   Huffman coding takes two passes).},
  address        = {New York, NY, USA},
  doi            = {http://doi.acm.org/10.1145/5684.5688},
  file           = {file:///home/roman/workspace/zipchip/papers/bentley86.pdf},
  issn           = {0001-0782},
  keywords       = {roman\_todo},
  publisher      = {ACM},
  year           = 1986
}

@Article{4629952,
  Author         = {Stefan, R. and Cotofana, S. D. },
  Title          = {Bitstream compression techniques for Virtex 4 FPGAs},
  Journal        = {Field Programmable Logic and Applications, 2008. FPL
                   2008. International Conference on},
  Pages          = {323--328},
  abstract       = {This paper examines the opportunity of using
                   compression for accelerating the (re)configuration of
                   FPGA devices, focusing on the choice of compression
                   algorithms, and their hardware implementation cost. As
                   our purpose is the acceleration of the configuration
                   process, estimating the decoder speed also plays a
                   major role in our study. We evaluate a wide range of
                   well-established compression algorithms and we also
                   propose two methods specifically developed for
                   compressing FPGA configuration bitstreams, one based on
                   a static dictionary and the other on arithmetic coding.
                   For the arithmetic coding we propose a statistical
                   model that takes advantage of the particularities of
                   the configuration bitstreams of the Virtex 4 FPGA
                   family. We evaluate the efficiency of the proposed
                   methods along with state of the art compression
                   algorithms on a number of benchmark circuits, some
                   selected from the available open source implementations
                   and some synthetically generated. Our evaluations
                   indicate that using modest resources we can achieve
                   parity and even exceed comercial software in terms of
                   compression ratio, and outperform all other traditional
                   algorithms. All our implemented decompressors are shown
                   to use less than 1.5 of the slices available on the
                   FPGA device.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/FPL.2008.4629952},
  citeulike-article-id = {4030236},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/FPL.2008.4629952},
  keywords       = {-source-ieee-hw-comp-2007-; ian_todo},
  month          = {Sept.},
  posted-at      = {2009-02-10 15:43:40},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/FPL.2008.4629952},
  year           = 2008
}

@Article{4343811,
  Author         = {Vittanala, L. R. and Chaudhuri, M. },
  Title          = {Integrating Memory Compression and Decompression with
                   Coherence Protocols in Distributed Shared Memory
                   Multiprocessors},
  Journal        = {Parallel Processing, 2007. ICPP 2007. International
                   Conference on},
  Pages          = {4},
  abstract       = {Ever-increasing memory footprint of applications and
                   increasing mainstream popularity of shared memory
                   parallel computing motivate us to explore memory
                   compression potential in distributed shared memory
                   (DSM) multiprocessors. This paper for the first time
                   integrates on-the-fly cache block
                   compression/decompression algorithms in the cache
                   coherence protocols by leveraging the directory
                   structure already present in these scalable machines.
                   Our proposal is unique in the sense that instead of
                   employing custom compression/decompression hardware, we
                   use a simple on-die protocol processing core in
                   dual-core nodes for running our directory-based
                   coherence protocol suitably extended with
                   compression/decompression algorithms. We design a
                   low-overhead compression scheme based on frequent
                   patterns and zero runs present in the evicted dirty L2
                   cache blocks. Our compression algorithm examines the
                   first eight bytes of an evicted dirty L2 block arriving
                   at the home memory controller and speculates which
                   compression scheme to invoke for the rest of the block.
                   Our customized algorithm for handling completely zero
                   cache blocks helps hide a significant amount of memory
                   access latency. Our simulation-based experiments on a
                   16-node DSM multiprocessor with seven scientific
                   computing applications show that our best design
                   achieves, on average, 16\% to 73\% storage saving per
                   evicted dirty L2 cache block for four out of the seven
                   applications at the expense of at most 15\% increased
                   parallel execution time.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/ICPP.2007.51},
  citeulike-article-id = {4030183},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/ICPP.2007.51},
  keywords       = {-source-ieee-hw-comp-2007-; ian_todo},
  month          = {Sept.},
  posted-at      = {2009-02-10 15:43:37},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/ICPP.2007.51},
  year           = 2007
}

@Article{4393466,
  Author         = {Wang, Chengyou and Hou, Zhengxin and Yang, Aiping },
  Title          = {Image Coding Algorithm Based on All Phase Walsh
                   Biorthogonal Transform},
  Journal        = {Microwave, Antenna, Propagation and EMC Technologies
                   for Wireless Communications, 2007 International
                   Symposium on},
  Pages          = {1118--1123},
  abstract       = {This paper proposes new concepts of the all phase
                   Walsh biorthogonal transform (APWBT) and the dual
                   biorthogonal basis vectors based on the Walsh-Hadamard
                   transform. The matrices of APWBT are deduced. They can
                   be used in image compression instead of the discrete
                   cosine transform (DCT). Compared with DCT in JPEG image
                   compression algorithm, the PSNRs of the reconstructed
                   images using the transform are the same as DCT's at the
                   same bit rates, but the advantage is that the
                   quantization is very simple. The transform coefficients
                   can be quantized uniformly. Therefore the computational
                   complexity is reduced and it is much easier to realize
                   with the hardware.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/MAPE.2007.4393466},
  citeulike-article-id = {4030164},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/MAPE.2007.4393466},
  keywords       = {-source-ieee-hw-comp-2007-, image-coding},
  month          = {Aug.},
  posted-at      = {2009-02-10 15:43:36},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/MAPE.2007.4393466},
  year           = 2007
}

@Article{4115287,
  Author         = {Morgenshtein, A. and Kolodny, A. and Ginosar, R.},
  Title          = {Asynchronous Bit-stream Compression (ABC)},
  Journal        = {Electrical and Electronics Engineers in Israel, 2006
                   IEEE 24th Convention of},
  Pages          = {241-244},
  abstract       = {Asynchronous signaling is used for high-speed data
                   communication in large Systems-on-Chip in. The
                   bandwidth limitations of serial link dictate a need for
                   real-time compression techniques. In this paper we
                   propose a new technique of Asynchronous Bit-stream
                   Compression (ABC), based on Level Encoded Dual-Rail
                   protocol. The ABC method is based on transitions added
                   to LEDR protocol which allow simple identification of
                   the compression code and ease its separate treatment in
                   the receiver. This compression allows a significant
                   saving in the transmission time and power without
                   losing data. The concept of ABC is described in this
                   paper together with the proposed architecture of its
                   hardware components. Simulations results are presented
                   for several data patterns with various differentiation
                   rates. Application of ABC results in reduction of
                   transmission time by 9% to 54% depending on type of
                   source data.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/EEEI.2006.321063},
  date-added     = {2009-02-11 01:38:15 +0100},
  date-modified  = {2009-02-11 01:48:41 +0100},
  doi            = {10.1109/EEEI.2006.321063},
  keywords       = {ian_todo},
  month          = {Nov. },
  year           = 2006
}

@Article{Papadopoulos08a,
  Author         = {Papadopoulos, Konstantinos and Papaefstathiou, Ioannis
                   },
  Title          = {Titan-R: A Reconfigurable Hardware Implementation of a
                   High-Speed Compressor},
  Journal        = {Field-Programmable Custom Computing Machines, 2008.
                   FCCM '08. 16th International Symposium on},
  Pages          = {216--225},
  abstract       = {Data compression techniques can alleviate low
                   bandwidth problems in multigigabit networks and are
                   especially useful when combined with encryption. This
                   paper presents a reconfigurable hardware compressor
                   core, the Titan-R, which can compress data streams at
                   8.5 Gb/sec making it the fastest reconfigurable such
                   device ever proposed. Its compression algorithm is a
                   variation of the most widely used and efficient such
                   scheme the Lempel-Ziv (LZ) algorithm that uses part of
                   the previous input stream as the dictionary. In order
                   to support this high network throughput the
                   Titan-Rutilizes a very fine-grained pipeline and takes
                   advantages of the high-bandwidth provided by the
                   distributed on-chip RAMs of the state-of-the-art FPGAs.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/FCCM.2008.14},
  citeulike-article-id = {4030302},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/FCCM.2008.14},
  file           = {file:///home/roman/workspace/zipchip/papers/Papadopoulos08.pdf},
  keywords       = {-source-ieee-hw-lz, hw, lz; ian_todo},
  month          = {April},
  posted-at      = {2009-02-10 15:43:45},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/FCCM.2008.14},
  year           = 2008
}

@Article{4697174,
  Author         = {Qin, Qiming and Sui, Xinxin and Jin, Chuan and Sun,
                   Yumei },
  Title          = {Design and implementation of aerial remote sensing
                   data compression system based on DSP},
  Journal        = {Signal Processing, 2008. ICSP 2008. 9th International
                   Conference on},
  Pages          = {478--481},
  abstract       = {Based on the characteristic and requirement of aerial
                   remote sensing system, the schemes of whole, software
                   and hardware design of compression system is put
                   forward. This paper brings forward the high compression
                   performance algorithm, modified CDF97(MCDF97) lifting
                   scheme combined with set partitioning in hierarchical
                   trees (SPIHT) coding, and realizes it in the Visual C++
                   environment. On this basis, SPM176430ER1000 coprocessor
                   of RTD corporation is adopted as hardware platform. The
                   program is transplanted, debugged and optimized in the
                   code composer studio (CCS) environment, then burned
                   into FLASH of corporation. In the flying experiment,
                   aerial remote sensing data compression system performed
                   well. At 5:1 compression ratio (CR), the average
                   compression speed of every image is 30 to 50 ms, while
                   average decompression speed is 60 ms. Decompression
                   images are displayed in real time in the process of
                   flying, which proved the correction and robustness of
                   the compression system.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/ICOSP.2008.4697174},
  citeulike-article-id = {4030200},
  date-modified  = {2009-02-10 22:30:30 +0100},
  doi            = {http://dx.doi.org/10.1109/ICOSP.2008.4697174},
  keywords       = {-source-ieee-hw-comp-2007-; ian_todo},
  month          = {Oct.},
  posted-at      = {2009-02-10 15:43:38},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/ICOSP.2008.4697174},
  year           = 2008
}

@Article{4378353,
  Author         = {Wang, Seongmoon and Balakrishnan, K. J. and Wei,
                   Wenlong },
  Title          = {X-Block: An Efficient LFSR Reseeding-Based Method to
                   Block Unknowns for Temporal Compactors},
  Journal        = {Computers, IEEE Transactions on},
  Volume         = {57},
  Number         = {7},
  Pages          = {978--989},
  abstract       = {This paper presents an efficient method to block
                   unknown values for temporal compactors. The control
                   signals for the blocking logic are generated by a
                   linear feedback shift register (LFSR). Control
                   patterns, which describe values required at the control
                   signals of the blocking logic, are compressed by LFSR
                   reseeding. The size of the control LFSR, which is
                   determined by the number of specified bits in the most
                   specified control pattern, is minimized by propagating
                   only one fault effect for each fault and targeting the
                   faults that are uniquely detected by each test pattern.
                   The linear solver to find seeds of the LFSR
                   intelligently chooses a solution such that the impact
                   on test quality is minimal. Very high compression (over
                   230X) is achieved for benchmark and industrial circuits
                   by the proposed method. Experimental results show that
                   the sizes of control data for the proposed method are
                   smaller than prior work and the runtime of the proposed
                   method is several orders of magnitude smaller than that
                   of prior work. Hardware overhead is very low.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/TC.2007.70833},
  citeulike-article-id = {4030231},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/TC.2007.70833},
  keywords       = {-source-ieee-hw-comp-2007-},
  month          = {July},
  posted-at      = {2009-02-10 15:43:40},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/TC.2007.70833},
  year           = 2008
}

@Article{4470156,
  Author         = {Yamani, Al A. and Prasanna, Devto N. and Gunda, A. },
  Title          = {Comparative study of centralised and distributed
                   compatibility-based test data compression},
  Journal        = {Computers \& Digital Techniques, IET},
  Volume         = {2},
  Number         = {2},
  Pages          = {108--117},
  abstract       = {Analysis of the tradeoff between hardware overhead,
                   runtime and test data volume is presented when
                   implementing systematic scan reconfiguration using
                   centralised and distributed architectures of the
                   segmented addressable scan, which is an
                   Illinois-scan-based architecture. The results show that
                   the centralised scheme offers better data volume
                   compression, similar automatic test pattern generation
                   (ATPG) runtime results and lower hardware overhead. The
                   cost with the centralised scheme is in the routing
                   congestion.},
  bdsk-url-1     = {http://dx.doi.org/10.1049/iet-cdt:20070037},
  citeulike-article-id = {4030290},
  date-modified  = {2009-02-10 22:30:30 +0100},
  doi            = {http://dx.doi.org/10.1049/iet-cdt:20070037},
  keywords       = {-source-ieee-hw-comp; ian_todo},
  month          = {March},
  posted-at      = {2009-02-10 15:43:44},
  priority       = {2},
  url            = {http://dx.doi.org/10.1049/iet-cdt:20070037},
  year           = 2008
}

@Article{4276411,
  Author         = {Haggett, S. and Knowles, G. and Bignell, G. },
  Title          = {Tokenisation of Class Files for an Embedded Java
                   Processor},
  Journal        = {Computer and Information Science, 2007. ICIS 2007. 6th
                   IEEE/ACIS International Conference on},
  Pages          = {375--381},
  abstract       = {We present a new approach to extend the tokenisation
                   scheme used in Java card to allow for both
                   invokevirtual and invokeinterface calls to be
                   dispatched using the same virtual method table. An
                   algorithm is described for token assignment to identify
                   methods that can use the same token, even in the
                   presence of interfaces. As a consequence much of the
                   string data in the class files is no longer required at
                   runtime, resulting in compression of the class files by
                   a factor of two. This is applied to the Java 2 micro
                   edition (J2ME) platform and simplifies the method
                   dispatch process to the point where it can be
                   implemented directly in hardware.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/ICIS.2007.181},
  citeulike-article-id = {4030211},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/ICIS.2007.181},
  keywords       = {-source-ieee-hw-comp-2007-, embedded-systems},
  month          = {July},
  posted-at      = {2009-02-10 15:43:38},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/ICIS.2007.181},
  year           = 2007
}

@Article{785700,
  Author         = {Papaefstathiou, I.},
  Title          = {Compressing ATM streams on-line},
  Journal        = {Data Compression Conference, 1999. Proceedings. DCC
                   '99},
  Pages          = {543-},
  abstract       = {Summary form only given. Asynchronous transfer mode
                   (ATM) is one of the state-of-the art network protocols
                   nowadays. A main characteristic of an ATM network is
                   that its switches should be fairly simple and
                   inexpensive. As a result, a significant part of the
                   network cost is in the cost of the links. So by
                   increasing the network traffic we can send over a given
                   link, we will certainly increase the effectiveness of
                   the whole network. A way of increasing this traffic is
                   to send compressed ATM cells. This idea, although it
                   seems very simple, is a new one and we have proved that
                   it is very important. Our compression scheme is
                   particularly useful in LAN since it will increase the
                   bandwidth of a typical ATM LAN by at least 250%. This
                   increase comes at a minimum additional cost (the cost
                   of a pair of our inexpensive hardware chips per link)
                   and with no bad side effects. This scheme is also
                   useful in WAN. The effective bandwidth of such a
                   network can be increased by about 50% at a minimum
                   additional cost and without any side effects, again.
                   This increase is particularly important in the case of
                   WAN since the cost of the link bandwidth in such a
                   network dominates by far the cost of the whole network.
                   But the kind of networks that would be perfect for
                   applying this compression scheme to, will be the LAN
                   interconnection networks over WAN. In these networks
                   the increase in the effective bandwidth of the
                   expensive WAN links will be equal to that of a typical
                   LAN (e.g., at least 250%), since the traffic sent over
                   these WAN links will be mainly LAN traffic},
  bdsk-url-1     = {http://dx.doi.org/10.1109/DCC.1999.785700},
  date-added     = {2009-02-11 02:33:32 +0100},
  date-modified  = {2009-02-11 02:33:38 +0100},
  doi            = {10.1109/DCC.1999.785700},
  keywords       = {ian_todo},
  month          = {Mar},
  year           = 1999
}

@Article{4382127,
  Author         = {Kato, M. and Lo},
  Title          = {Power Consumption Reduction in Java-enabled,
                   Battery-powered Handheld Devices through Memory
                   Compression},
  Journal        = {Consumer Electronics, 2007. ISCE 2007. IEEE
                   International Symposium on},
  Pages          = {1--6},
  abstract       = {Mobile/wireless services have been developed using
                   Java technology on Java-enabled, battery-powered
                   handheld devices. These services such as games, web
                   browser, audio/video players, are easily accessible,
                   but consume a huge amount of power. The handheld
                   devices typically last for 5 to 6 hours in running the
                   Java applications. A new technique for low power
                   consumption is to be developed. Using runtime power
                   control in Java-enabled, battery-powered handheld
                   devices. The technique improves in-memory compression
                   to minimize memory consumption and a memory bank
                   partitioning technique to cluster memory banks into
                   active banks and inactive banks are powered off
                   dynamically. The runtime power control is integrated
                   into a Java runtime environment (JREPC) using a
                   hardware/software codesign technique. The JREPC is
                   studied and examined with benchmark applications (web,
                   email, audio/video, game, M- commerce). The in-memory
                   compression reduces more than 50 \% of memory
                   consumption. Only half of the memory may be activated.
                   The time overhead due to the in-memory compression is
                   negligible using a hardware de/compressor. We believe
                   that the in-memory compression with the memory bank
                   partitioning technique is a key to achieving low power
                   consumption in the Java-enabled, battery-powered
                   handheld devices.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/ISCE.2007.4382127},
  citeulike-article-id = {4030171},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/ISCE.2007.4382127},
  keywords       = {-source-ieee-hw-comp-2007-; ian_todo},
  month          = {June},
  posted-at      = {2009-02-10 15:43:37},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/ISCE.2007.4382127},
  year           = 2007
}

@Article{4483282,
  Author         = {Chen, Xi and Yang, Lei and Lekatsas, H. and Dick, R.
                   P. and Shang, Li },
  Title          = {Design and Implementation of a High-Performance
                   Microprocessor Cache Compression Algorithm},
  Journal        = {Data Compression Conference, 2008. DCC 2008},
  Pages          = {43--52},
  abstract       = {Abstract Researchers have proposed using hardware data
                   compression units within the memory hierarchies of
                   microprocessors in order to improve performance, energy
                   efficiency, and functionality. However, most past work,
                   and in particular work on cache compression, has made
                   unsubstantiated assumptions about the performance,
                   power consumption, and area overheads of the required
                   compression hardware. We present a lossless compression
                   algorithm that has been designed for on-line memory
                   hierarchy compression, and cache compression in
                   particular. We reduced our algorithm to a register
                   transfer level hardware implementation, permitting
                   performance, power consumption, and area estimation.
                   The results of experiments comparing our work to
                   previous work are presented.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/DCC.2008.90},
  citeulike-article-id = {4030253},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/DCC.2008.90},
  keywords       = {-source-ieee-hw-comp; ian_todo},
  month          = {March},
  posted-at      = {2009-02-10 15:43:41},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/DCC.2008.90},
  year           = 2008
}

@Article{4668638,
  Author         = {Aslam, N. and Milward, M. J. and Erdogan, A. T. and
                   Arslan, T. },
  Title          = {Code Compression and Decompression for Coarse-Grain
                   Reconfigurable Architectures},
  Journal        = {Very Large Scale Integration (VLSI) Systems, IEEE
                   Transactions on},
  Volume         = {16},
  Number         = {12},
  Pages          = {1596--1608},
  abstract       = {This paper presents a code compression and on-the-fly
                   decompression scheme suitable for coarse-grain
                   reconfigurable technologies. These systems pose further
                   challenges by having an order of magnitude higher
                   memory requirement due to much wider instruction words
                   than typical VLIW/TTA architectures. Current
                   compression schemes are evaluated. A highly efficient
                   and novel dictionary-based lossless compression
                   technique is implemented and compared against a
                   previous implementation for a reconfigurable system.
                   This paper looks at several conflicting design
                   parameters, such as the compression ratio, silicon
                   area, latency, and power consumption. Compression
                   ratios in the range of 0.32 to 0.44 are recorded with
                   the proposed scheme for a given set of test programs.
                   With these test programs, a 60\% overall silicon area
                   saving is achieved, even after the decompressor
                   hardware overhead is taken into account. The proposed
                   technique may be applied to any architecture which
                   exhibits common characteristics to the example
                   reconfigurable architecture targeted in this paper.},
  annote         = {code (de)compression - dazu gute biblio ... f{\"u}r
                   reconfigurable architecture (geiler schei{\ss}!)
                   vergleicht statistische und w{\"o}rterbuchmethoden},
  bdsk-url-1     = {http://dx.doi.org/10.1109/TVLSI.2008.2001562},
  citeulike-article-id = {4030229},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/TVLSI.2008.2001562},
  keywords       = {-source-ieee-hw-comp-2007-; ian_note},
  month          = {Dec.},
  posted-at      = {2009-02-10 15:43:40},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/TVLSI.2008.2001562},
  year           = 2008
}

@Article{4450582,
  Author         = {Nandi, A. V. and Banakar, R. M. },
  Title          = {Throughput Efficient Parallel Implementation of SPIHT
                   Algorithm},
  Journal        = {VLSI Design, 2008. VLSID 2008. 21st International
                   Conference on},
  Pages          = {718--725},
  abstract       = {We present a throughput efficient FPGA implementation
                   of the 'Set Partitioning in Hierarchical Trees' (SPIHT)
                   algorithm for compression of images. The SPIHT uses
                   inherent redundancy among wavelet coefficients and
                   suited for both gray and color images. The SPIHT
                   algorithm uses dynamic data structures which hinders
                   hardware realization. In our FPGA implementation we
                   have modified basic SPIHT in two ways, one by using
                   static (fixed) mappings which represent significant
                   information and the other by interchanging the sorting
                   and refinement passes. A hardware realization is done
                   in a Xilinx XC2S30 device. Significant compression
                   ratio and throughput is obtained for a sample image of
                   size 128 times 128 pixels.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/VLSI.2008.48},
  citeulike-article-id = {4030202},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/VLSI.2008.48},
  keywords       = {-source-ieee-hw-comp-2007-, fpga, image-coding},
  month          = {Jan.},
  posted-at      = {2009-02-10 15:43:38},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/VLSI.2008.48},
  year           = 2008
}

@Article{4584303,
  Author         = {Wu, Xiaofeng and Vladimirova, T. },
  Title          = {Hardware-in-Loop Simulation of a Satellite Sensor
                   Network for Distributed Space Applications},
  Journal        = {Adaptive Hardware and Systems, 2008. AHS '08. NASA/ESA
                   Conference on},
  Pages          = {424--431},
  abstract       = {In this paper a hardware-in-loop simulator is
                   presented to demonstrate the satellite sensor network
                   concept developed at the Surrey Space Centre under the
                   ESPACENET project. The simulator includes software that
                   emulates satellite orbit dynamics in Low Earth Orbit
                   and picosatellite sensor nodes. The picosatellite
                   currently under development is based on the CubeSat
                   platform. The main payload will be an FPGA board that
                   implements intellectual property cores like the LEON3
                   processor, a media access controller for intersatellite
                   links, image compression, encryption, etc.. The
                   payloads of the individual satellites are connected
                   together via IEEE 802.11 intersatellite links to
                   demonstrate a distributed computing platform for future
                   space missions.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/AHS.2008.55},
  citeulike-article-id = {4030239},
  date-modified  = {2009-02-12 00:39:11 +0100},
  doi            = {http://dx.doi.org/10.1109/AHS.2008.55},
  keywords       = {-source-ieee-hw-comp-2007-, fpga; ian_todo},
  month          = {June},
  posted-at      = {2009-02-10 15:43:40},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/AHS.2008.55},
  year           = 2008
}

@Article{4749067,
  Author         = {Koyrakh, L. A. },
  Title          = {Data compression for implantable medical devices},
  Journal        = {Computers in Cardiology, 2008},
  Pages          = {417--420},
  abstract       = {Introduction: Implantable devices have limited memory,
                   computationaland battery power resources, while
                   collecting, processing and transmitting out information
                   from potentially many sensors. These limitations
                   require that information within the devices be
                   efficiently compressed. Such data compression presents
                   a challenging task, as it must provide high fidelity of
                   the waveform reproduction and high compression ratios
                   on limited size data frames. Also, it must efficiently
                   run on ultra low power hardware, and allow flexible
                   configuration, based on the type of data to be
                   compressed. Methods: The new compression algorithm was
                   implemented as a bit accurate Matlab simulation,
                   consisting of the following major steps: 1. Integer
                   wavelet transform. 2. Quantization coupled with
                   filtration. Two selectable quantization schemes could
                   be utilized based on the signal properties: linear and
                   dead-zone. Data filtration is performed on bit
                   boundaries, which simplifies hardware implementation.
                   The filtration thresholds are made different in
                   different wavelet sub-bands, controlled by a single
                   parameter. 3. Original adaptive data encoding. Our
                   approach only requires basic logical operations such as
                   bit counters and shifts, and is highly optimized for
                   implementation in implantable device hardware. For high
                   reliability each compressed data frame contains all
                   information needed for decompression. Results: The
                   algorithm was applied to data from the PhysioNet ECG
                   compression test database. 40 ECG frames of 1024
                   samples from 4 patients were sampled at 250 Hz, 12 bit
                   resolution, and compressed with distortions below 8\%.
                   Compression ratios were 9.3\$\\pm\$2.5, consistently
                   exceeding 85\% of the theoretical limit based on bit
                   entropy for each individual data frame. Conclusions:
                   The compression algorithm is efficient on data
                   collected by implantable devices, and could be used in
                   various applications in both microprocessor and ASIC
                   implementations, helping to reduce memory requirements
                   and the batte- - ry energy spent on the information
                   transmission to and from the implantable device.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/CIC.2008.4749067},
  citeulike-article-id = {4030264},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/CIC.2008.4749067},
  keywords       = {-source-ieee-hw-comp},
  month          = {Sept.},
  posted-at      = {2009-02-10 15:43:42},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/CIC.2008.4749067},
  year           = 2008
}

@Article{4258441,
  Author         = {Das, S. R. and Mukherjee, S. and Petriu, E. M. and
                   Assaf, M. H. and Hossain, A. },
  Title          = {Space Compaction for Embedded Cores-Based
                   System-on-Chips (SOCs) Using Fault Graded Output Merger},
  Journal        = {Instrumentation and Measurement Technology Conference
                   Proceedings, 2007. IMTC 2007. IEEE},
  Pages          = {1--5},
  abstract       = {The design of space-efficient support hardware for
                   built-in self-testing (BIST) is of crucial importance
                   in the design and manufacture of complex system-on-chip
                   (SOC) circuits. This paper reports on a new space
                   compression technique that facilitates designing such
                   circuits using pseudorandom and compact test sets, with
                   the primary objective of minimizing the storage
                   requirements for the circuit under test (CUT) while
                   maintaining the fault coverage information. The
                   compaction technique utilizes the concept of fault
                   grading of output lines based on strong and weak
                   compatibility information of output pairs. The proposed
                   technique guarantees simple design with 100\% fault
                   coverage for single stuck-line faults, low CPU
                   simulation time, and acceptable area overhead.
                   Simulation runs on ISCAS 85 combinational benchmark
                   circuits with FSIM and ATALANTA programs confirm the
                   usefulness of the suggested approach.},
  bdsk-url-1     = {http://dx.doi.org/10.1109/IMTC.2007.379374},
  citeulike-article-id = {4030195},
  date-modified  = {2009-02-12 01:23:39 +0100},
  doi            = {http://dx.doi.org/10.1109/IMTC.2007.379374},
  keywords       = {-source-ieee-hw-comp-2007-; ian_todo},
  month          = {May},
  posted-at      = {2009-02-10 15:43:38},
  priority       = {2},
  url            = {http://dx.doi.org/10.1109/IMTC.2007.379374},
  year           = 2007
}

